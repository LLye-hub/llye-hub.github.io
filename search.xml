<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>mysql和hiveSQL的语法差别</title>
      <link href="/posts/e912f2da.html"/>
      <url>/posts/e912f2da.html</url>
      
        <content type="html"><![CDATA[<p>最近在牛客网上刷sql题，但编程语言居然只支持mysql，一些函数用法上与平时工作使用的hiveSQL有较大差别，所以在这篇博客中整理一下两种语法的函数使用差异</p><p><a href>mysql内置函数</a></p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF?spm=a2c4g.11186623.0.0.3c267254Ka3fUh#LanguageManualUDF-get_json_object">hive内置函数</a></p><h1><span id="日期-时间函数">日期、时间函数</span></h1><table><thead><tr><th>函数用途</th><th>mysql函数</th><th>mysql用法</th><th>hive函数</th><th>hiveSQL用法</th></tr></thead><tbody><tr><td>日期、时间格式化</td><td>date_format</td><td>date_format(‘2008-08-08 22:23:01’, ‘%Y%m%d%H%i%s’)</td><td>date_format</td><td>date_format(‘2008-08-08 22:23:01’, ‘yyyyMMddHHmmss’)</td></tr><tr><td>日期、时间加</td><td>date_add</td><td>date_add(‘2008-08-08 22:23:01’,interval 1 day&#x2F;hour&#x2F;minute&#x2F;second&#x2F;microsecond&#x2F;week&#x2F;month&#x2F;quarter&#x2F;year)，返回dateTime格式</td><td>date_add</td><td>date_add(‘2008-08-08 22:23:01’,1)，只加days，返回date格式</td></tr><tr><td>日期、时间减</td><td>date_sub</td><td>date_sub(‘2008-08-08 22:23:01’,interval 1 day&#x2F;hour&#x2F;minute&#x2F;second&#x2F;microsecond&#x2F;week&#x2F;month&#x2F;quarter&#x2F;year)，返回dateTime格式</td><td>date_sub</td><td>date_sub(‘2008-08-08 22:23:01’,1)，只加days，返回date格式</td></tr><tr><td>日期相差</td><td>datediff</td><td>datediff(‘2008-08-08 22:22:00’,’2008-08-07 22:23:00’)</td><td>datediff</td><td>datediff(‘2008-08-08 22:22:00’,’2008-08-07 22:23:00’)</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>排序算法</title>
      <link href="/posts/735e5788.html"/>
      <url>/posts/735e5788.html</url>
      
        <content type="html"><![CDATA[<p>整理一些数据结构中常用的排序算法原理和java实现</p><h1><span id="快速排序">快速排序</span></h1><h2><span id="原理">原理</span></h2><p>在数组中找到一个基准值<code>t</code>，将小于<code>t</code>的值放它前面，大于<code>t</code>的值放它后面，再以此方法对子数组递归进行快速排序</p><h2><span id="java代码">java代码</span></h2>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>解题思路之动态规划</title>
      <link href="/posts/d6cdfd6a.html"/>
      <url>/posts/d6cdfd6a.html</url>
      
        <content type="html"><![CDATA[<h1><span id="什么是动态规划">什么是动态规划</span></h1><p>动态规划，英文：Dynamic Programming，简称DP。<br>简单理解，动态规划的每一个状态都能由上一个状态推导而来</p><h1><span id="解题步骤">解题步骤</span></h1><p>以斐波那契数列为例，动态规划问题可以拆解为五步曲：</p><p>1、确定dp数组和下标含义：第n个斐波那契数是<code>dp[n]</code></p><p>2、确定递推公式（也可叫状态转移方程）：<code>dp[n] = dp[n-1] + dp[n-2]</code></p><p>3、dp数组初始化：<code>dp[0] = 0; dp[1] = 1</code></p><p>4、确定遍历顺序：从前到后遍历，<code>dp[n]</code>依赖<code>dp[n-1]</code>和<code>dp[n-2]</code></p><p>5、举例推导dp数组：当<code>n=10</code>时，dp数组应该为：<code>0 1 1 2 3 5 8 13 21 34 55</code></p><h1><span id="参考资料">参考资料</span></h1><p><a href="https://programmercarl.com/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92">代码随想录之动态规划</a></p>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL优化之数据倾斜</title>
      <link href="/posts/faab1ad7.html"/>
      <url>/posts/faab1ad7.html</url>
      
        <content type="html"><![CDATA[<h1><span id="前言">前言</span></h1><p>在Spark作业优化场景中，最常见且比较棘手的就是数据倾斜问题。个人认为，具备数据倾斜调优能力对从事数仓开发人员是必备的基本要求。当然，数据倾斜的场景是比较复杂的，针对不同的数据倾斜有不同的处理方案。</p><h1><span id="如何辨别和定位数据倾斜">如何辨别和定位数据倾斜</span></h1><p>从Spark作业的执行计划看，若出现某个task任务比其他task任务执行耗时极其久，比如：某个stage有100个task，其中99个task在1min左右就执行成功，但是有1个task却执行了1个小时甚至更久，这种情况显然是出现了数据倾斜。</p><p>数据倾斜问题仅出现在shuffle过程，一些会触发shuffle的算子：distinct、groupByKey、reduceByKey、aggregateByKey、countByKey、join、cogroup、repartition等。<br>对应提交的SparkSQL中可能有distinct、count(distinct)、group by、partition by、join等关键词。</p><h1><span id="常见的数据倾斜场景及解决方案">常见的数据倾斜场景及解决方案</span></h1><h1><span id="碰到的数据倾斜案例">碰到的数据倾斜案例</span></h1><h2><span id="窗口分组数据倾斜">窗口分组数据倾斜</span></h2><p><strong>倾斜场景</strong><br>业务上有一张消息记录表msg_records，sql要求是取下一次回复消息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> msg_tmp <span class="keyword">as</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span>  id                  <span class="comment">-- 唯一键，消息id</span></span><br><span class="line">           ,from_chat_id        <span class="comment">-- 消息发送者id</span></span><br><span class="line">           ,to_chat_id          <span class="comment">-- 消息接受者id</span></span><br><span class="line">           ,msg_time            <span class="comment">-- 消息时间</span></span><br><span class="line">    <span class="keyword">from</span> msg_records</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span>  id</span><br><span class="line">       ,msg_time</span><br><span class="line">       ,<span class="built_in">first_value</span>(if(type <span class="operator">=</span> <span class="string">&#x27;reply&#x27;</span>,id,<span class="keyword">null</span>),<span class="literal">true</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> from_chat_id,to_chat_id <span class="keyword">order</span> <span class="keyword">by</span> msg_time,id <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> following <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> reply_msg_id_n1t <span class="comment">-- 取下一次回复消息</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span>  id</span><br><span class="line">           ,from_chat_id</span><br><span class="line">           ,to_chat_id</span><br><span class="line">           ,msg_time</span><br><span class="line">           ,<span class="string">&#x27;send&#x27;</span> <span class="keyword">as</span> type</span><br><span class="line">    <span class="keyword">from</span> msg_tmp</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="comment">-- 调转，取返回消息</span></span><br><span class="line">    <span class="keyword">select</span>  id</span><br><span class="line">           ,to_chat_id   <span class="keyword">as</span> from_chat_id</span><br><span class="line">           ,from_chat_id <span class="keyword">as</span> to_chat_id</span><br><span class="line">           ,msg_time</span><br><span class="line">           ,<span class="string">&#x27;reply&#x27;</span>      <span class="keyword">as</span> type</span><br><span class="line">    <span class="keyword">from</span> msg_tmp</span><br><span class="line">) t1</span><br></pre></td></tr></table></figure><p><strong>sql执行分析</strong><br>有一个task执行耗时1h<br><img src="https://i.328888.xyz/2023/02/10/R2j3w.png" alt="R2j3w.png"></p><p><strong>数据倾斜分析</strong><br>根据窗口函数的分组<code>from_chat_id + to_chat_id</code>分析，数据量出现严重倾斜，表总数据量1亿多，其中，分组<code>from_chat_id=12 and to_chat_id=81867</code>的数据量有30w，其他分组数据量至多3w。</p><p>另外，分组<code>from_chat_id=12 and to_chat_id=81867</code>的数据在业务上可定义为脏数据，且first_value()函数计算出的值全为null。</p><p>经过测试验证发现，没有 <strong>rows between语句</strong> 或是 <strong>过滤倾斜数据</strong> 时，SQL执行很快</p><p>综上分析，再对照spark执行计划基本可以定位倾斜原因为<strong>窗口数据倾斜和rows between计算耗时</strong></p><p><strong>解决方案</strong><br>结合业务知识，在sql逻辑中过滤<code>from_chat_id=12 and to_chat_id=81867</code>的数据</p><p>最终，任务执行耗时从<code>1h</code>优化至<code>10min</code></p><h1><span id="参考资料">参考资料</span></h1><p><a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">美团技术团队：Spark性能优化指南——高级篇</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之生成连续数字</title>
      <link href="/posts/3ce3d37f.html"/>
      <url>/posts/3ce3d37f.html</url>
      
        <content type="html"><![CDATA[<h1><span id="sql要求">sql要求</span></h1><p>生成100以内的全部整数</p><h1><span id="涉及udtf函数">涉及udtf函数</span></h1><p><strong>posexplode</strong>(ARRAY&lt;T&gt; a) <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF?spm=a2c4g.11186623.0.0.3c267254Ka3fUh#LanguageManualUDF-posexplode(array)">官方说明</a><br><strong>Return</strong>: Returns a row-set with two columns (pos int,val T), one row for each element from the array.<br><strong>Description</strong>: posexplode() is similar to explode but instead of just returning the elements of the array it returns the element as well as its position in the original array.</p><p><strong>用法示例</strong>：<br>有如下一张表myTable</p><table><thead><tr><th align="center">(array&lt;int&gt;)myCol</th></tr></thead><tbody><tr><td align="center">[100,200,300]</td></tr><tr><td align="center">[400,500,600]</td></tr></tbody></table><p>执行hive sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 造数据</span></span><br><span class="line"><span class="keyword">with</span> myTable <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">array</span>(<span class="number">100</span>,<span class="number">200</span>,<span class="number">300</span>) <span class="keyword">as</span> myCol</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span> </span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">array</span>(<span class="number">300</span>,<span class="number">400</span>,<span class="number">500</span>) <span class="keyword">as</span> myCol</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 查询sql</span></span><br><span class="line"><span class="keyword">SELECT</span> posexplode(myCol) <span class="keyword">AS</span> (pos, val) <span class="keyword">FROM</span> myTable</span><br></pre></td></tr></table></figure><p>得到结果为：</p><table><thead><tr><th align="center">(int)pos</th><th align="center">(int)val</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">100</td></tr><tr><td align="center">1</td><td align="center">200</td></tr><tr><td align="center">2</td><td align="center">300</td></tr><tr><td align="center">0</td><td align="center">400</td></tr><tr><td align="center">1</td><td align="center">500</td></tr><tr><td align="center">2</td><td align="center">600</td></tr></tbody></table><h1><span id="sql实现">sql实现</span></h1><p>借助posexplode返回的pos即可实现</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> posexplode(split(space(<span class="number">99</span>), <span class="string">&#x27; &#x27;</span>)) <span class="keyword">as</span> (pos, val)</span><br><span class="line"><span class="comment">-- 返回的pos字段即为[0,99]区间的100个整数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 或者下面这种写法</span></span><br><span class="line"><span class="keyword">select</span> posexplode(split(repeat(<span class="string">&#x27;,&#x27;</span>,<span class="number">99</span>), <span class="string">&#x27;,&#x27;</span>)) <span class="keyword">as</span> (pos, val)</span><br></pre></td></tr></table></figure><h1><span id="实例场景">实例场景</span></h1><h2><span id="数据重复扩容10倍">数据重复扩容10倍</span></h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 造数据</span></span><br><span class="line"><span class="keyword">with</span> myTable <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="string">&#x27;张三&#x27;</span> <span class="keyword">as</span> name</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span> </span><br><span class="line">    <span class="keyword">select</span> <span class="string">&#x27;李四&#x27;</span> <span class="keyword">as</span> name</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 将myTable的每行数据重复复制为5行</span></span><br><span class="line"><span class="keyword">SELECT</span> name</span><br><span class="line">       ,posexplode(split(space(<span class="number">4</span>), <span class="string">&#x27; &#x27;</span>)) <span class="keyword">AS</span> (pos, val) </span><br><span class="line"><span class="keyword">FROM</span> myTable</span><br></pre></td></tr></table></figure><p>得到结果为：</p><table><thead><tr><th align="center">name</th><th align="center">pos</th><th align="center">val</th></tr></thead><tbody><tr><td align="center">张三</td><td align="center">0</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">1</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">2</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">3</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">4</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">0</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">1</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">2</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">3</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">4</td><td align="center"></td></tr></tbody></table><h2><span id="生成指定范围内的连续日期">生成指定范围内的连续日期</span></h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> subquery <span class="keyword">as</span>  (</span><br><span class="line">    <span class="keyword">select</span> split(space(datediff(<span class="string">&#x27;2023-1-31&#x27;</span>,<span class="string">&#x27;2022-11-30&#x27;</span>)), <span class="string">&#x27; &#x27;</span>)  <span class="keyword">as</span> x</span><br><span class="line">) </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    date_add(<span class="string">&#x27;2022-11-30&#x27;</span>, pos) <span class="keyword">as</span> new_date</span><br><span class="line"><span class="keyword">from</span>  </span><br><span class="line">    subquery t</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> </span><br><span class="line">    posexplode(x) pe <span class="keyword">as</span> pos, val</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL高级语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL之conf参数</title>
      <link href="/posts/5e220c44.html"/>
      <url>/posts/5e220c44.html</url>
      
        <content type="html"><![CDATA[<h1><span id="官方说明传送门"></span></h1><h1><span id="资源参数">资源参数</span></h1><h2><span id="num-executors">num-executors</span></h2><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h2><span id="executor-memory">executor-memory</span></h2><ul><li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1&#x2F;3~1&#x2F;2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><p>##executor-cores</p><ul><li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1&#x2F;3~1&#x2F;2左右比较合适，也是避免影响其他同学的作业运行。</li></ul><h2><span id="driver-memory">driver-memory</span></h2><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h2><span id="sparkdefaultparallelism">spark.default.parallelism</span></h2><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li><li>参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h2><span id="sparkstoragememoryfraction">spark.storage.memoryFraction</span></h2><ul><li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h2><span id="sparkshufflememoryfraction">spark.shuffle.memoryFraction</span></h2><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h1><span id="广播相关">广播相关</span></h1><h2><span id="sparksqlbroadcasttimeout">spark.sql.broadcastTimeout</span></h2><h2><span id="sparkkryoserializerbuffermaxx3d128m">spark.kryoserializer.buffer.max&#x3D;128M</span></h2><h2><span id="sparksqlshufflepartitionsx3d1000">spark.sql.shuffle.partitions&#x3D;1000</span></h2><h2><span id="sparksqlorccompressioncodecx3dzlib">spark.sql.orc.compression.codec&#x3D;zlib</span></h2><h2><span id="sparksqlfilesmaxpartitionbytesx3d65536">spark.sql.files.maxPartitionBytes&#x3D;65536</span></h2>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之set参数</title>
      <link href="/posts/4547a6e2.html"/>
      <url>/posts/4547a6e2.html</url>
      
        <content type="html"><![CDATA[<h1><span id="官方传送门"></span></h1><h2><span id="hivemergemapfiles">hive.merge.mapfiles</span></h2><p>Default Value: true<br>map-only任务结束时合并小文件</p><h2><span id="hivemergemapredfiles">hive.merge.mapredfiles</span></h2><p>Default Value: true<br>map-reduce任务结束时合并小文件</p><h2><span id="hiveoptimizectematerializethreshold">hive.optimize.cte.materialize.threshold</span></h2><p>默认情况下是-1（关闭）；当开启（大于0），比如设置为2，则如果with..as语句被引用2次及以上时，会把with..as语句生成的table物化，从而做到with..as语句只执行一次，来提高效率</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL命令之alter partition</title>
      <link href="/posts/8a94c1da.html"/>
      <url>/posts/8a94c1da.html</url>
      
        <content type="html"><![CDATA[<p>msck repair table</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition</a></p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql alter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell命令之set-e</title>
      <link href="/posts/ba81765c.html"/>
      <url>/posts/ba81765c.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop基本命令</title>
      <link href="/posts/b24f0feb.html"/>
      <url>/posts/b24f0feb.html</url>
      
        <content type="html"><![CDATA[<p>hadoop fs -cp<br>hadoop fs -rm -r<br>hadoop distcp -overwrite -delete -p<br>hadoop fs -mkdir -p</p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive动态分区</title>
      <link href="/posts/44d3528f.html"/>
      <url>/posts/44d3528f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态分区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL无法处理hive表中的空ORC文件</title>
      <link href="/posts/1f69e18b.html"/>
      <url>/posts/1f69e18b.html</url>
      
        <content type="html"><![CDATA[<h1><span id="碰到了什么问题">碰到了什么问题</span></h1><p>起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo<br><img src="https://i.328888.xyz/2022/12/19/AfHSH.png" alt="AfHSH.png"><br><img src="https://i.328888.xyz/2022/12/19/AfbiQ.png" alt="AfbiQ.png"><br>之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客</p><h1><span id="问题分析">问题分析</span></h1><h2><span id="定位问题">定位问题</span></h2><p>根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段。</p><h2><span id="查看hdfs文件">查看HDFS文件</span></h2><p>查看表存储路径下的文件，发现有1个空文件<br><img src="https://i.328888.xyz/2022/12/19/AfjbE.png" alt="AfjbE.png"></p><h2><span id="为什么会有空文件">为什么会有空文件</span></h2><p>1、sparkSQL建表<br>2、表写入数据时，sql最后做了distribute by操作，产生了空文件</p><p>sparksql读取空文件的时候，因为表是orc格式的，导致sparkSQL解析orc文件出错。但是用hive却可以正常读取。</p><h2><span id="网上搜罗的解决办法">网上搜罗的解决办法</span></h2><p>问题原因基本清晰了，就是读取空文件导致的报错，如果非得用SparkSQL执行查询语句，这里提供几种解决方案：</p><h4><span id="1-修改表存储格式为parquet">1、修改表存储格式为parquet</span></h4><p>这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐</p><h4><span id="2-参数设置set-hiveexecorcsplitstrategyetl">2、参数设置：<code>set hive.exec.orc.split.strategy=ETL</code></span></h4><p>既然已经定位到是空文件读取的问题，那就从文件读取层面解决。</p><p>自建集群<code>Spark</code>源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java</span></span><br><span class="line"><span class="keyword">switch</span>(context.splitStrategyKind) &#123;</span><br><span class="line">    <span class="keyword">case</span> BI:</span><br><span class="line">    <span class="comment">// BI strategy requested through config</span></span><br><span class="line">    splitStrategy = <span class="keyword">new</span> <span class="title class_">BISplitStrategy</span>(context, fs, dir, children, isOriginal,</span><br><span class="line">        deltas, covered);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> ETL:</span><br><span class="line">    <span class="comment">// ETL strategy requested through config</span></span><br><span class="line">    splitStrategy = <span class="keyword">new</span> <span class="title class_">ETLSplitStrategy</span>(context, fs, dir, children, isOriginal,</span><br><span class="line">        deltas, covered);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    <span class="comment">// HYBRID strategy</span></span><br><span class="line">    <span class="keyword">if</span> (avgFileSize &gt; context.maxSize) &#123;</span><br><span class="line">        splitStrategy = <span class="keyword">new</span> <span class="title class_">ETLSplitStrategy</span>(context, fs, dir, children, isOriginal, deltas,</span><br><span class="line">            covered);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        splitStrategy = <span class="keyword">new</span> <span class="title class_">BISplitStrategy</span>(context, fs, dir, children, isOriginal, deltas,</span><br><span class="line">            covered);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ./repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf.class</span></span><br><span class="line">HIVE_ORC_SPLIT_STRATEGY(<span class="string">&quot;hive.exec.orc.split.strategy&quot;</span>, <span class="string">&quot;HYBRID&quot;</span>, <span class="keyword">new</span> <span class="title class_">StringSet</span>(<span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;HYBRID&quot;</span>, <span class="string">&quot;BI&quot;</span>, <span class="string">&quot;ETL&quot;</span>&#125;), <span class="string">&quot;This is not a user level config. BI strategy is used when the requirement is to spend less time in split generation as opposed to query execution (split generation does not read or cache file footers). ETL strategy is used when spending little more time in split generation is acceptable (split generation reads and caches file footers). HYBRID chooses between the above strategies based on heuristics.&quot;</span>)      </span><br><span class="line">  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。</p><ul><li>BI策略以文件为粒度进行split划分</li><li>ETL策略会将文件进行切分，多个stripe组成一个split</li><li>HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。</li></ul><p>ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;OrcSplit&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  List&lt;OrcSplit&gt; splits = Lists.newArrayList();</span><br><span class="line">  <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">    String[] hosts = SHIMS</span><br><span class="line">        .getLocationsWithOffset(fs, fileStatus) <span class="comment">// 对空文件会返回一个空的TreeMap</span></span><br><span class="line">        .firstEntry()  <span class="comment">// null</span></span><br><span class="line">        .getValue()    <span class="comment">// NPE</span></span><br><span class="line">        .getHosts();</span><br><span class="line">    <span class="type">OrcSplit</span> <span class="variable">orcSplit</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OrcSplit</span>(fileStatus.getPath(), <span class="number">0</span>, fileStatus.getLen(), hosts,</span><br><span class="line">                                     <span class="literal">null</span>, isOriginal, <span class="literal">true</span>, deltas, -<span class="number">1</span>);</span><br><span class="line">    splits.add(orcSplit);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// add uncovered ACID delta splits</span></span><br><span class="line">  splits.addAll(<span class="built_in">super</span>.getSplits());</span><br><span class="line">  <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;SplitInfo&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    List&lt;SplitInfo&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">for</span> (FileStatus file : files) &#123;</span><br><span class="line">    <span class="type">FileInfo</span> <span class="variable">info</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (context.cacheStripeDetails) &#123;</span><br><span class="line">        info = verifyCachedFileInfo(file);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ignore files of 0 length（此处对空文件做了过滤）</span></span><br><span class="line">    <span class="keyword">if</span> (file.getLen() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        result.add(<span class="keyword">new</span> <span class="title class_">SplitInfo</span>(context, fs, file, info, isOriginal, deltas, <span class="literal">true</span>, dir, covered));</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 本质上是一个BUG，<code>Spark2.4</code>版本中解决了这个问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;OrcSplit&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    List&lt;OrcSplit&gt; splits = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">    String[] hosts = SHIMS.getLocationsWithOffset(fs, fileStatus).firstEntry().getValue()</span><br><span class="line">        .getHosts();</span><br><span class="line">    <span class="type">OrcSplit</span> <span class="variable">orcSplit</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OrcSplit</span>(fileStatus.getPath(), <span class="number">0</span>, fileStatus.getLen(), hosts,</span><br><span class="line">        <span class="literal">null</span>, isOriginal, <span class="literal">true</span>, deltas, -<span class="number">1</span>);</span><br><span class="line">    splits.add(orcSplit);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add uncovered ACID delta splits</span></span><br><span class="line">    splits.addAll(<span class="built_in">super</span>.getSplits());</span><br><span class="line">    <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>了解了spark读取orc文件策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.orc.split.strategy<span class="operator">=</span>ETL</span><br></pre></td></tr></table></figure><p>经测试无效。原因分析：<br>1、参数未生效<br>2、hdfs文件有两个，大小为49B和7.45G，文件的平均大小肯定是大于256M的，所以按默认HYBRID策略规则应本就是采取的ETL策略split ORC文件</p><h4><span id="3-参数设置sparksqlhiveconvertmetastoreorctrue">3、参数设置：<code>spark.sql.hive.convertMetastoreOrc=true</code></span></h4><p>关于参数的<a href="https://spark.apache.org/docs/2.3.3/sql-programming-guide.html#orc-files">官方介绍</a></p><blockquote><p>Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true.</p></blockquote><p>经测试有效。若仍报错，可尝试搭配spark.sql.orc.impl&#x3D;native使用。</p><h1><span id="补充知识">补充知识</span></h1><p><img src="https://i.328888.xyz/2022/12/19/Af23F.jpeg" alt="Af23F.jpeg"><br>hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。<br>另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。</p><h1><span id="参考资料">参考资料</span></h1><p><a href="https://www.freesion.com/article/8054484645/">SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTION</a><br><a href="https://blog.csdn.net/weixin_45240507/article/details/124689323?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-124689323-blog-100524131.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-124689323-blog-100524131.pc_relevant_default&utm_relevant_index=7">SparkSQL读取ORC表时遇到空文件</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ORC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>get_json_object在sql中的高级用法</title>
      <link href="/posts/5f45fcd7.html"/>
      <url>/posts/5f45fcd7.html</url>
      
        <content type="html"><![CDATA[<h2><span id="语法介绍">语法介绍</span></h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get_json_object(String json_string, String path)</span><br><span class="line"><span class="comment">-- return string</span></span><br></pre></td></tr></table></figure><p>get_json_object函数是用来根据指定路径提取json字符串中的json对象，并返回json对象的json字符串</p><h2><span id="现有困惑">现有困惑</span></h2><p>关于这个函数最常见的用法就是<code>get_json_object(&#39;&#123;&quot;a&quot;:&quot;b&quot;&#125;&#39;, &#39;$.a&#39;)</code>，返回结果<code>b</code><br>但<code>$.a</code>这种path写法仅适用于简单的多层嵌套json字符串解析，碰到嵌套层有json数组时就难以解析了<br>比如，要提取下面这段json中的所有<code>weight</code>对象的值</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"> <span class="attr">&quot;store&quot;</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;fruit&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span><span class="number">8</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;apple&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span><span class="number">9</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;pear&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span>  <span class="comment">//json数组</span></span><br><span class="line">         <span class="attr">&quot;bicycle&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;price&quot;</span><span class="punctuation">:</span><span class="number">19.95</span><span class="punctuation">,</span><span class="attr">&quot;color&quot;</span><span class="punctuation">:</span><span class="string">&quot;red&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span><span class="punctuation">,</span> </span><br><span class="line"> <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span><span class="string">&quot;amy@only_for_json_udf_test.net&quot;</span><span class="punctuation">,</span> </span><br><span class="line"> <span class="attr">&quot;owner&quot;</span><span class="punctuation">:</span><span class="string">&quot;amy&quot;</span> </span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>通过<code>$.store.fruit.weight</code>路径是无法提取的，<code>$.store.fruit[0].weight</code>这种写法仅能获取json数组中第一个json字符串中<code>weight</code>对象的值，也总不能用<code>[0]、[1]、[2]……</code>的方式无穷尽取值吧</p><p>到这里思维就限制住了，遇到这种情况时，以前的方式是通过正则表达式处理<br>具体实现如下：<br>首先将<code>item_properties</code>按指定分隔符split为array数组，再利用explode函数将array数组的元素逐行输出，最终得到的<code>item_propertie</code>即为单个json字符串，可根据<code>$.</code>提取指定json对象的值，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- item_properties = [&#123;&quot;id&quot;:42,&quot;name&quot;:&quot;包装&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:43,&quot;name&quot;:&quot;种类&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:44,&quot;name&quot;:&quot;规格&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:63,&quot;name&quot;:&quot;保质期(天)&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:100,&quot;name&quot;:&quot;适用年龄&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:101,&quot;name&quot;:&quot;储存条件&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> get_json_object(item_propertie,<span class="string">&#x27;$.id&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> table_a</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(regexp_replace(substr(item_properties,<span class="number">2</span>,length(item_properties)<span class="number">-2</span>),<span class="string">&#x27;\\&#125;\\,\\&#123;&#x27;</span>,<span class="string">&#x27;\\&#125;\\|\\|\\&#123;&#x27;</span>),<span class="string">&#x27;\\|\\|&#x27;</span>)) tmp <span class="keyword">as</span> item_propertie</span><br></pre></td></tr></table></figure><p>但上面这种处理方式存在bug，将json数据split为array数组时，必须保证指定分隔符不出现在单个json字符串中，比如上述case中是用<code>&#125;,&#123;</code>替换为<code>&#125;||&#123;</code>，再以<code>||</code>作为分隔符split，如若在单个json字符串中也出现了<code>&#125;,&#123;</code>或是<code>||</code>就会导致解析失败</p><h2><span id="怎么高级了">怎么高级了</span></h2><p>突然有一天在翻看<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF?spm=a2c4g.11186623.0.0.3c267254Ka3fUh#LanguageManualUDF-get_json_object">hive官方文档</a>时发现path支持的通配符<code>*</code><br><a href="https://imgloc.com/i/2Pw2H"><img src="https://i.328888.xyz/2023/01/17/2Pw2H.png" alt="2Pw2H.png"></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ : 表示根节点</span><br><span class="line">. : 表示子节点</span><br><span class="line">[] : [number]表示数组下标，从0开始</span><br><span class="line">* : []的通配符，返回整个数组</span><br></pre></td></tr></table></figure><p>所以，一开始的问题应该按如下解法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- jsonArray = &#123;&quot;store&quot;:</span></span><br><span class="line"><span class="comment">--                     &#123;</span></span><br><span class="line"><span class="comment">--                     &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],</span></span><br><span class="line"><span class="comment">--                     &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;</span></span><br><span class="line"><span class="comment">--                     &#125;, </span></span><br><span class="line"><span class="comment">--             &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, </span></span><br><span class="line"><span class="comment">--             &quot;owner&quot;:&quot;amy&quot;&#125;</span></span><br><span class="line"><span class="keyword">select</span> get_json_object(jsonArray, <span class="string">&#x27;$.store.fruit[*].weight&#x27;</span>);</span><br><span class="line"><span class="comment">-- return [8,9]</span></span><br></pre></td></tr></table></figure><p>笔者个人认为，高级之处在于写法极其清爽，按照以前用正则表达式的处理方法，需要多道处理才能得到结果<code>[8,9]</code>，而且其中还有隐性风险，但是现在<code>$.store.fruit[*].weight</code>这种极简语法既避免了风险，又清晰易理解</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL高级语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资料汇总</title>
      <link href="/posts/76d5a95a.html"/>
      <url>/posts/76d5a95a.html</url>
      
        <content type="html"><![CDATA[<h1><span id="sparksql">SparkSQL</span></h1><p><a href="https://cloud.tencent.com/developer/article/1348071">Spark SQL Limit 介绍及优化</a></p><p><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html">Spark性能优化指南——基础篇</a></p><p><a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">Spark性能优化指南——高级篇</a></p><h1><span id="大数据笔记">大数据笔记</span></h1><p><a href="https://github.com/heibaiying/BigData-Notes">大数据入门指南</a></p><h1><span id="数据结构与算法">数据结构与算法</span></h1><p><a href="https://programmercarl.com/other/algo_pdf.html">代码随想录</a></p>]]></content>
      
      
      <categories>
          
          <category> 资料 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
