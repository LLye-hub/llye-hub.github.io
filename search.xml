<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>hexo+GitHub自建博客遇到的问题</title>
      <link href="/posts/69a95f3b.html"/>
      <url>/posts/69a95f3b.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_45149481/article/details/116794535">hexo文章目录点击不跳转，html没有生成href</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之理解explain参数</title>
      <link href="/posts/2369b6cf.html"/>
      <url>/posts/2369b6cf.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain">hive官方文档说明</a></p><p><strong>关键字</strong> <code>EXPLAIN</code></p><p><strong>语法：</strong> <code>EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION|VECTORIZATION|ANALYZE] query</code>（<code>hive2.3.9</code>版本支持）  </p><ul><li>EXPLAIN EXTENDED是EXPLAIN的扩展，能展示更加详细的信息。除了EXPLAIN打印出的内容，还包括每个标的HDFS读取路径，每个HIVE表的配置信息等，可以查看出表是否被全表扫描。</li><li>EXPLAIN DEPENDENCY用于描述一段sql需要的数据来源，输出的是一个json格式的数据，里面包含一下两个部分的内容，<br>input_partitions：描述一段sql依赖的数据来源表分区，里面存储的分区名的列表，格式如下：<br>{“partitionname”:“库名@表名 @分区列&#x3D;分区列的值”} ，如果整段sql包含的所有表都是是非分区表，则显示为空。<br>input_table：描述一段sql依赖的数据来源表，里面存储的是HIVE表名的列表格式如下：<br>{“tablename”:“库名@表名 ”，“tabletype”：表的类型（外部表&#x2F;内部表）}</li><li>EXPLAIN ANALYZE是EXPLAIN的扩展。EXPLAIN的信息中标示的是预估扫描行数，EXPLAIN ANALYZE但展示信息中也标示了实际扫描行数，格式为：<code>Num rows: (estimated row count)/(actual row count)</code></li></ul><p><strong>执行计划包含三部分内容</strong></p><ul><li>查询sql的抽象语法树</li><li>所有stage之间的依赖关系</li><li>每个stage的具体描述</li></ul><p><strong>参数解释</strong></p><ul><li><p>MapReduce: 表示当前任务执行所用的计算机引擎是MapReduce  </p></li><li><p>Map Operator Tree: 表示当前描述的Map阶段执行的操作信息。</p><ul><li>TableScan: 表示对关键字alias声明的结果集。这里代指表明。</li><li>Statistics: 表示对当前阶段的统计信息。例如数据行数和数据量，这两个都是预估值。</li><li>Filter Operator: 表示在之前操作（TableScan）的结果集上进行数据的过滤。</li><li>Predicate: 表示Filter Operator进行过滤时候使用的谓词，例如 pt_dt &#x3D; 2020-11-11</li><li>Select Operator: 表示在之前的结果集上对列进行投影，即列筛选。<ul><li>Expressions: 表示需要投影的列，即筛选的列。</li><li>OutputColNames: 表示输出的列名。</li><li>Group By Operator: 表示在之前的结果集上分组聚合。<ul><li>keys: 表示分组的列。</li><li>mode[aggregations]: 表示分组聚合使用的算法.例如 count（）。</li><li>Reduce Output Operator: 表示当前描述的是对之前的结果聚合后的输出信息，这里表示Map端聚合后的信息。<ul><li>key expressions&#x2F;value expressions: MapReduce计算引擎，在Map阶段和Reduce阶段输出的都是键-值对的形式，这里key expression和 key expression 和 value expression 分别描述的就是Map阶段输出的键（key） 和值（value）所用的数据列。key expression指代的就是聚合列。 value expression 指代的就是 聚合的函数。</li><li>sort order：表示输出是否进行排序，每个<code>+</code>表示正序排序的一列，每个<code>-</code>表示倒序排序的一列。</li><li>Map-Reduce partition columns: 表示Map阶段输出到Reduce阶段的分区列。在HIVE-SQL中可以用distribute by 指代分区的列。</li></ul></li></ul></li></ul></li></ul></li><li><p>Reduce Operator Tree: 表示当前描述的Reduce阶段执行的操作信息。<br>  Reduce 阶段关键字和Map阶段的含义一样，不同的如下：</p></li><li><p>compressed: 在 File output operator中这个关键词表示文件输出的结果是否进行压缩，FALSE表示不进行输出压缩。</p></li><li><p>table: 表示正在操作的表。</p></li><li><p>input format&#x2F;out putformat: 分别表示文件输入和输出的文件类型。</p></li><li><p>serde:表示读取表数据的序列化和反序列化的方式。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hiveSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之groupBy语句增强语法grouping sets/CUBE/rollup</title>
      <link href="/posts/67bc5d15.html"/>
      <url>/posts/67bc5d15.html</url>
      
        <content type="html"><![CDATA[<p>本文详细整理了关于group by子句的增强聚合语法grouping sets&#x2F;CUBE&#x2F;rollup的具体用法，语法的<a href="https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup">hive官方介绍文档</a> 。</p><h1 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h1><p>首先声明使用的hive版本为 <code>2.3.9</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> travel_data;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> travel_data(</span><br><span class="line">      province string,</span><br><span class="line">      city string,</span><br><span class="line">      attraction string,</span><br><span class="line">      star_level <span class="type">int</span>,</span><br><span class="line">      Price <span class="keyword">double</span>,</span><br><span class="line">      sales <span class="type">int</span>,</span><br><span class="line">      sale_date string</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> travel_data</span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;河南省&#x27;</span>,<span class="string">&#x27;郑州市&#x27;</span>,<span class="string">&#x27;方特&#x27;</span>,<span class="number">4</span>,<span class="number">312.22</span>,<span class="number">15789</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;河南省&#x27;</span>,<span class="string">&#x27;郑州市&#x27;</span>,<span class="string">&#x27;二七广场&#x27;</span>,<span class="number">4</span>,<span class="number">0</span>,<span class="number">5942</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;河南省&#x27;</span>,<span class="string">&#x27;郑州市&#x27;</span>,<span class="string">&#x27;河南省博物馆&#x27;</span>,<span class="number">4</span>,<span class="number">1.22</span>,<span class="number">943</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;河南省&#x27;</span>,<span class="string">&#x27;洛阳市&#x27;</span>,<span class="string">&#x27;白云山&#x27;</span>,<span class="number">4</span>,<span class="number">324.44</span>,<span class="number">16843</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;河南省&#x27;</span>,<span class="string">&#x27;洛阳市&#x27;</span>,<span class="string">&#x27;白马寺&#x27;</span>,<span class="number">4</span>,<span class="number">23.45</span>,<span class="number">2567</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;河南省&#x27;</span>,<span class="string">&#x27;洛阳市&#x27;</span>,<span class="string">&#x27;龙门石窟&#x27;</span>,<span class="number">4</span>,<span class="number">45</span>,<span class="number">15784</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;广东省&#x27;</span>,<span class="string">&#x27;深圳市&#x27;</span>,<span class="string">&#x27;东部华侨城&#x27;</span>,<span class="number">4</span>,<span class="number">86</span>,<span class="number">9523</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;广东省&#x27;</span>,<span class="string">&#x27;深圳市&#x27;</span>,<span class="string">&#x27;欢乐谷&#x27;</span>,<span class="number">4</span>,<span class="number">54</span>,<span class="number">2573</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;广东省&#x27;</span>,<span class="string">&#x27;深圳市&#x27;</span>,<span class="string">&#x27;世界之窗&#x27;</span>,<span class="number">4</span>,<span class="number">34</span>,<span class="number">5644</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;广东省&#x27;</span>,<span class="string">&#x27;广州市&#x27;</span>,<span class="string">&#x27;长隆&#x27;</span>,<span class="number">4</span>,<span class="number">46</span>,<span class="number">25673</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;广东省&#x27;</span>,<span class="string">&#x27;广州市&#x27;</span>,<span class="string">&#x27;广州塔&#x27;</span>,<span class="number">4</span>,<span class="number">35</span>,<span class="number">9735</span>,<span class="string">&#x27;2019-02-03&#x27;</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><h1 id="grouping-set语句"><a href="#grouping-set语句" class="headerlink" title="grouping set语句"></a>grouping set语句</h1><p>官方说明</p><blockquote><p>The GROUPING SETS clause in GROUP BY allows us to specify more than one GROUP BY option in the same record set. All GROUPING SET clauses can be logically expressed in terms of several GROUP BY queries connected by UNION. Table-1 shows several such equivalent statements. This is helpful in forming the idea of the GROUPING SETS clause. A blank set ( ) in the GROUPING SETS clause calculates the overall aggregate.</p></blockquote><p>grouping set子句可以实现对同一个数据集指定多个group by条件，适合多维聚合场景下使用。其执行效果等同于对多个group by查询进行union all操作。</p><p><code>SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b) )</code>等同下面语句</p><blockquote><p>SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b</p></blockquote><p><code>SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a, b), a, b, ( ) )</code>等同下面语句</p><blockquote><p>SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b<br>UNION<br>SELECT a, null, SUM( c ) FROM tab1 GROUP BY a, null<br>UNION<br>SELECT null, b, SUM( c ) FROM tab1 GROUP BY null, b<br>UNION<br>SELECT null, null, SUM( c ) FROM tab1</p></blockquote><p><strong>语法</strong></p><ol><li>grouping sets子句必须跟在group by语句后，且出现在grouping sets的字段必须出现在group by语句中，但是出现在group by中字段不一定要出现在grouping sets语句中</li><li>出现在group by中但是没有在grouping sets中的字段将会被赋值为null</li><li>grouping__id字段可以区分不同的聚合粒度，表示当前行数据数据哪个分组集合</li><li>grouping函数可以处理空值，grouping()接受一个列名作为参数，如果结果对应行使用了参数列做聚合，返回0，此时意味着NULL来自输入数据；否则返回1，此时意味着NULL是grouping sets的占位符。</li></ol><p><strong>测试sql：从省&amp;市聚合维度统计销售数量</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    td.province,</span><br><span class="line">    td.city,</span><br><span class="line">    IF(<span class="keyword">grouping</span>(td.city) <span class="operator">=</span> <span class="number">0</span>,td.city,<span class="string">&#x27;城市&#x27;</span>) <span class="keyword">as</span> city2, <span class="comment">-- 进行空值判断，替换输出更有实际意义的值</span></span><br><span class="line">    <span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">    grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    travel_data td</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    td.province,</span><br><span class="line">    td.city</span><br><span class="line">    <span class="keyword">grouping</span> SETS (td.province,td.city)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> grouping__id</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>查询结果：</p><p><img src="https://i.328888.xyz/2023/05/05/iTpX0x.png" alt="iTpX0x.png"></p><ul><li>第一列按照province</li><li>第二列按照city</li><li>第三列按照city分组，并对空值进行替换</li><li>第四列按照province或city分组，进行统计计算</li><li>第五列grouping__id表示当前行数据属于哪个分组，1表示province，2表示city</li></ul><p><strong>测试sql：从省&amp;市、省&amp;日期、省三个聚合维度统计销售数量</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">td.province ,</span><br><span class="line">td.city,</span><br><span class="line"><span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">td.sale_date ,</span><br><span class="line">grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">travel_data td</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">td.province ,</span><br><span class="line">td.city,</span><br><span class="line">td.sale_date</span><br><span class="line"><span class="keyword">grouping</span> SETS (</span><br><span class="line">(td.province , td.city)</span><br><span class="line">,(td.province, td.sale_date)</span><br><span class="line">,td.province </span><br><span class="line">)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">grouping__id</span><br></pre></td></tr></table></figure><h1 id="cube语句"><a href="#CUBE语句" class="headerlink" title="CUBE语句"></a>CUBE语句</h1><p>CUBE函数跟group by语句一起使用，可以对group by的所有字段进行组合再进行聚合计算。</p><p><code>group by a,b,c with CUBE</code>执行效果等同于 <code>group by a, b, c grouping sets ( (a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ( ))</code></p><p><strong>测试sql</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    province ,</span><br><span class="line">    city,</span><br><span class="line">    <span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">    grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    travel_data</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    province,city</span><br><span class="line"><span class="keyword">with</span> <span class="keyword">CUBE</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    grouping__id</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 或者下面这种写法</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    province ,</span><br><span class="line">    city,</span><br><span class="line">    <span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">    grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    travel_data</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    <span class="keyword">CUBE</span>(province,city)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    grouping__id</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>查询结果：</p><p><img src="https://i.328888.xyz/2023/05/06/iatksp.png" alt="iatksp.png"></p><p>从上面的结果数据可以看到，对聚合字段 <code>(province,city)</code>使用CUBE函数后，返回结果有4种聚合维度：<code>(province,city)</code>、<code>(province)</code>、<code>(city)</code>、<code>()</code></p><h1 id="rollup语句"><a href="#rollup语句" class="headerlink" title="rollup语句"></a>rollup语句</h1><p>rollup是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合，可以实现上钻和下钻的效果</p><p><code>group by a,b,c with rollup</code>假设层次结构是 “a “向下钻到 “b “向下钻到 “c”，执行效果等同于 <code>group by a, b, c grouping sets ( (a, b, c), (a, b), (a), ( ))</code></p><p><strong>测试sql</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    province ,</span><br><span class="line">    city,</span><br><span class="line">    sale_date ,</span><br><span class="line">    <span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">    grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    travel_data</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    province,city,sale_date</span><br><span class="line"><span class="keyword">with</span> <span class="keyword">rollup</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    grouping__id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 或者下面这种写法</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">province ,</span><br><span class="line">city,</span><br><span class="line">sale_date ,</span><br><span class="line"><span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">travel_data </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line"><span class="keyword">rollup</span>(province,city,sale_date)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">grouping__id</span><br></pre></td></tr></table></figure><p>查询结果：</p><p><img src="https://i.328888.xyz/2023/05/06/iakvoL.png" alt="iakvoL.png"></p><p>从上面的结果数据可以看到，对聚合字段 <code>(province,city,sale_date)</code>使用rollup函数后，返回结果有4种聚合维度：<code>(province,city,sale_date)</code>、<code>(province,city)</code>、<code>(province)</code>、<code>()</code></p><h1 id="grouping__id计算方法"><a href="#grouping-id计算方法" class="headerlink" title="grouping__id计算方法"></a>grouping__id计算方法</h1><p>从rollup函数的例子可以看到，grouping__id的数值并不是连续的，下面总结下grouping__id计算方法</p><ol><li>按group by语句的字段顺序（不理解网上有说法是按字段倒序排序）。所以这里要注意groupby字段顺序变化是会影响grouping__id计算结果的。</li><li>对于每个字段，若出现在了当前粒度中，则该字段位置赋值为0，否则为1。</li><li>这样就形成了一个二进制数，将这个二进制数转为十进制，即为当前粒度对应的 grouping__id。</li></ol><p>以统计粒度 <code>group by province,city,sale_date</code>为例，</p><ul><li>字段顺序为:province,city,sale_date</li><li>所有聚合维度对应的二进制数为：</li></ul><table><thead><tr><th align="center">grouping sets</th><th align="center">按字段顺序赋值二进制数</th><th align="center">转换为十进制的grouping__id</th></tr></thead><tbody><tr><td align="center">province,city,sale_date</td><td align="center">000</td><td align="center">0</td></tr><tr><td align="center">province,city</td><td align="center">001</td><td align="center">1</td></tr><tr><td align="center">province,sale_date</td><td align="center">010</td><td align="center">2</td></tr><tr><td align="center">province</td><td align="center">011</td><td align="center">3</td></tr><tr><td align="center">city,sale_date</td><td align="center">100</td><td align="center">4</td></tr><tr><td align="center">city</td><td align="center">101</td><td align="center">5</td></tr><tr><td align="center">sale_date</td><td align="center">110</td><td align="center">6</td></tr><tr><td align="center">无</td><td align="center">111</td><td align="center">7</td></tr></tbody></table><p><strong>测试sql：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">td.province ,</span><br><span class="line">city,</span><br><span class="line">td.sale_date ,</span><br><span class="line"><span class="built_in">sum</span>(sales) <span class="keyword">as</span> sales,</span><br><span class="line">grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">travel_data td</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line"><span class="keyword">CUBE</span>(td.province,td.city,td.sale_date)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">grouping__id</span><br></pre></td></tr></table></figure><p>查询结果：</p><p><img src="https://i.328888.xyz/2023/05/05/iT55Jb.png" alt="iT55Jb.png"></p><p>从上面的结果可以看到，grouping__id的数值与计算规则得出来的一致。</p><h1 id="grouping-sets和union-all性能对比"><a href="#grouping-sets和union-all性能对比" class="headerlink" title="grouping sets和union all性能对比"></a>grouping sets和union all性能对比</h1><p><strong>实现逻辑</strong><br>如果说 <code>union all</code>是先聚合再联合，那么 <code>grouping sets</code>就是先联合再聚合。<code>grouping sets</code>根据 <code>N</code>个分组对每条数据进行计算，不在当前分组的字段置为null，将数据量扩展成原来的 <code>N</code>倍，再按 <code>group by</code>的字段做聚合计算。</p><p><code>group by province,city grouping sets ((province,city),province,())</code>计算效果图如下：</p><p><img src="https://i.328888.xyz/2023/05/06/iacyut.png" alt="iacyut.png"></p><p><code>…… group by province,city union all …… group by province</code>计算效果图如下：</p><p><img src="https://i.328888.xyz/2023/05/06/iaRuvE.png" alt="iaRuvE.png"></p><p>下面通过执行计划分析两种方式的差异。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">select</span></span><br><span class="line">    province,</span><br><span class="line">    city,</span><br><span class="line">    grouping__id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    travel_data </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    province,</span><br><span class="line">    city</span><br><span class="line">    <span class="keyword">grouping</span> SETS ((province,city),province,())</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> grouping__id</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>hive执行计划：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">Explain                                                                                                      |</span><br><span class="line">-------------------------------------------------------------------------------------------------------------+</span><br><span class="line">STAGE DEPENDENCIES:                                                                                          |</span><br><span class="line">  Stage-1 is a root stage                                                                                    |</span><br><span class="line">  Stage-2 depends on stages: Stage-1                                                                         |</span><br><span class="line">  Stage-0 depends on stages: Stage-2                                                                         |</span><br><span class="line">                                                                                                             |</span><br><span class="line">STAGE PLANS:                                                                                                 |</span><br><span class="line">  Stage: Stage-1                                                                                             |</span><br><span class="line">    Map Reduce                                                                                               |</span><br><span class="line">      Map Operator Tree:                                                                                     |</span><br><span class="line">          TableScan                                                                                          |</span><br><span class="line">            alias: travel_data                                                                               |</span><br><span class="line">            Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE                 |</span><br><span class="line">            Select Operator                                                                                  |</span><br><span class="line">              expressions: province (type: string), city (type: string)                                      |</span><br><span class="line">              outputColumnNames: _col0, _col1                                                                |</span><br><span class="line">              Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE               |</span><br><span class="line">              Group By Operator                                                                              |</span><br><span class="line">                keys: _col0 (type: string), _col1 (type: string), 0 (type: int)                              |</span><br><span class="line">                mode: hash                                                                                   |</span><br><span class="line">                outputColumnNames: _col0, _col1, _col2                                                       |</span><br><span class="line">                Statistics: Num rows: 33 Data size: 1791 Basic stats: COMPLETE Column stats: NONE            |  这里读取数据后按grouping sets的3个分组维度，将数据由11条扩充为33条</span><br><span class="line">                Reduce Output Operator                                                                       |</span><br><span class="line">                 ……</span><br><span class="line">      Reduce Operator Tree:                                                                                  |</span><br><span class="line">        Group By Operator                                                                                    |</span><br><span class="line">          keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: int)                    |</span><br><span class="line">          mode: mergepartial                                                                                 |</span><br><span class="line">          outputColumnNames: _col0, _col1, _col2                                                             |</span><br><span class="line">          Statistics: Num rows: 16 Data size: 868 Basic stats: COMPLETE Column stats: NONE                   |</span><br><span class="line">          Select Operator                                                                                    |</span><br><span class="line">            expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int)                       |</span><br><span class="line">            outputColumnNames: _col0, _col1, _col2                                                           |</span><br><span class="line">            Statistics: Num rows: 16 Data size: 868 Basic stats: COMPLETE Column stats: NONE                 |</span><br><span class="line">            ……</span><br><span class="line">                                                                                                             |</span><br><span class="line">  Stage: Stage-2                                                                                             |</span><br><span class="line">    Map Reduce                                                                                               |</span><br><span class="line">      Map Operator Tree:                                                                                     |</span><br><span class="line">          TableScan                                                                                          |</span><br><span class="line">            Reduce Output Operator                                                                           |</span><br><span class="line">              key expressions: _col2 (type: int)                                                             |</span><br><span class="line">              sort order: +                                                                                  |</span><br><span class="line">              Statistics: Num rows: 16 Data size: 868 Basic stats: COMPLETE Column stats: NONE               |</span><br><span class="line">              value expressions: _col0 (type: string), _col1 (type: string)                                  |</span><br><span class="line">      Reduce Operator Tree:                                                                                  |</span><br><span class="line">        Select Operator                                                                                      |</span><br><span class="line">          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string), KEY.reducesinkkey0 (type: int)|  这里KEY.reducesinkkey0即为grouping__id</span><br><span class="line">          outputColumnNames: _col0, _col1, _col2                                                             |</span><br><span class="line">          Statistics: Num rows: 16 Data size: 868 Basic stats: COMPLETE Column stats: NONE                   |</span><br><span class="line">          ……                                                                                        |</span><br><span class="line"> </span><br><span class="line">                                 |</span><br><span class="line">                                                                                                             |</span><br><span class="line">  Stage: Stage-0                                                                                             |</span><br><span class="line">    ……                                                                                     </span><br></pre></td></tr></table></figure><p>从上面的执行计划可以看到，<code>Stage-1</code>在读取数据时，在<code>map</code>阶段根据<code>grouping sets</code>有3个分组维度，将数据量扩充至原来的3倍，然后在<code>reduce</code>阶段做<code>group by province,city</code>操作。 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">explain</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">province,</span><br><span class="line">city</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">travel_data</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    province,city</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    province,</span><br><span class="line">    <span class="keyword">NULL</span> <span class="keyword">as</span> city</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">travel_data</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    province </span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>hive执行计划：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">Explain                                                                                           |</span><br><span class="line">--------------------------------------------------------------------------------------------------+</span><br><span class="line">STAGE DEPENDENCIES:                                                                               |</span><br><span class="line">  Stage-1 is a root stage                                                                         |</span><br><span class="line">  Stage-2 depends on stages: Stage-1, Stage-3                                                     |</span><br><span class="line">  Stage-3 is a root stage                                                                         |</span><br><span class="line">  Stage-0 depends on stages: Stage-2                                                              |</span><br><span class="line">                                                                                                  |</span><br><span class="line">STAGE PLANS:                                                                                      |</span><br><span class="line">  Stage: Stage-1                                                                                  |</span><br><span class="line">    Map Reduce                                                                                    |</span><br><span class="line">      Map Operator Tree:                                                                          |</span><br><span class="line">          TableScan                                                                               |</span><br><span class="line">            alias: travel_data                                                                    |</span><br><span class="line">            Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE      |</span><br><span class="line">            Select Operator                                                                       |</span><br><span class="line">              expressions: province (type: string), city (type: string)                           |</span><br><span class="line">              outputColumnNames: province, city                                                   |</span><br><span class="line">              Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE    |</span><br><span class="line">              Group By Operator                                                                   |</span><br><span class="line">                keys: province (type: string), city (type: string)                                |</span><br><span class="line">                mode: hash                                                                        |</span><br><span class="line">                outputColumnNames: _col0, _col1                                                   |</span><br><span class="line">                Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE  |  这里数据量没有变化</span><br><span class="line">                Reduce Output Operator                                                            |</span><br><span class="line">                  ……</span><br><span class="line">      Reduce Operator Tree:                                                                       |</span><br><span class="line">        Group By Operator                                                                         |</span><br><span class="line">          keys: KEY._col0 (type: string), KEY._col1 (type: string)                                |</span><br><span class="line">          mode: mergepartial                                                                      |</span><br><span class="line">          outputColumnNames: _col0, _col1                                                         |</span><br><span class="line">          Statistics: Num rows: 5 Data size: 271 Basic stats: COMPLETE Column stats: NONE         |</span><br><span class="line">          ……</span><br><span class="line">                                                                                                  |</span><br><span class="line">  Stage: Stage-2                                                                                  |</span><br><span class="line">    Map Reduce                                                                                    |</span><br><span class="line">      Map Operator Tree:                                                                          |</span><br><span class="line">          TableScan                                                                               |</span><br><span class="line">            Union                                                                                 |</span><br><span class="line">              Statistics: Num rows: 10 Data size: 542 Basic stats: COMPLETE Column stats: NONE    |</span><br><span class="line">              ……                  |</span><br><span class="line">          TableScan                                                                               |</span><br><span class="line">            Union                                                                                 |</span><br><span class="line">              Statistics: Num rows: 10 Data size: 542 Basic stats: COMPLETE Column stats: NONE    |</span><br><span class="line">              ……                 </span><br><span class="line">  Stage: Stage-3                                                                                  |</span><br><span class="line">    Map Reduce                                                                                    |</span><br><span class="line">      Map Operator Tree:                                                                          |</span><br><span class="line">          TableScan                                                                               |</span><br><span class="line">            alias: travel_data                                                                    |</span><br><span class="line">            Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE      |</span><br><span class="line">            Select Operator                                                                       |</span><br><span class="line">              expressions: province (type: string)                                                |</span><br><span class="line">              outputColumnNames: province                                                         |</span><br><span class="line">              Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE    |</span><br><span class="line">              Group By Operator                                                                   |</span><br><span class="line">                keys: province (type: string)                                                     |</span><br><span class="line">                mode: hash                                                                        |</span><br><span class="line">                outputColumnNames: _col0                                                          |</span><br><span class="line">                Statistics: Num rows: 11 Data size: 597 Basic stats: COMPLETE Column stats: NONE  |  这里数据量没有变化</span><br><span class="line">                Reduce Output Operator                                                            |</span><br><span class="line">                  ……</span><br><span class="line">      Reduce Operator Tree:                                                                       |</span><br><span class="line">        Group By Operator                                                                         |</span><br><span class="line">          keys: KEY._col0 (type: string)                                                          |</span><br><span class="line">          mode: mergepartial                                                                      |</span><br><span class="line">          outputColumnNames: _col0                                                                |</span><br><span class="line">          Statistics: Num rows: 5 Data size: 271 Basic stats: COMPLETE Column stats: NONE         |</span><br><span class="line">            ……               </span><br><span class="line">                                                                                                  |</span><br><span class="line">  Stage: Stage-0                                                                                  |</span><br><span class="line">    ……                       </span><br></pre></td></tr></table></figure><p>从上面的执行计划可以看到，<code>Stage-1</code>和<code>Stage-3</code>都是读取数据，再分别按照<code>group by province,city</code>和<code>group by province</code>做聚合操作，最后在<code>Stage-2</code>做<code>union</code>操作合并数据。<code>union all</code>这种写法对表<code>travel_data</code>重复读取两次，查询性能上比<code>grouping sets</code>写法要差些。在集群空闲的情况下，对两种写法的sql分别执行5次，得到如下结果：</p><blockquote><p>grouping sets写法执行5次的耗时:</p><blockquote><p>select province, city, grouping__id from travel_data group by province, city grouping SETS ((province,city),province) order by grouping__id ;</p></blockquote><p>Time taken: 54.807 seconds, Fetched: 6 row(s)<br>Time taken: 56.261 seconds, Fetched: 6 row(s)<br>Time taken: 52.671 seconds, Fetched: 6 row(s)<br>Time taken: 62.945 seconds, Fetched: 6 row(s)<br>Time taken: 57.337 seconds, Fetched: 6 row(s)</p></blockquote><blockquote><p>union all写法执行5次的耗时:</p><blockquote><p>select province,city from travel_data group by province,city union all select province, NULL as city from travel_data group by province;</p></blockquote><p>Time taken: 83.91 seconds, Fetched: 6 row(s)<br>Time taken: 94.466 seconds, Fetched: 6 row(s)<br>Time taken: 86.253 seconds, Fetched: 6 row(s)<br>Time taken: 75.509 seconds, Fetched: 6 row(s)<br>Time taken: 88.633 seconds, Fetched: 6 row(s)</p></blockquote><p>可以算出，<code>grouping sets</code>写法的平均耗时为56.8s，<code>union all</code>写法的平均耗时为85.7s，耗时是前者的1.5倍。</p><p>所以，<code>grouping sets</code>写法的sql不仅在表达上更加简洁，在查询性能上也更加高效。</p><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p><a href="https://juejin.cn/post/7223211123961200700">Hive分析函数详解：GROUPING SETS&#x2F;CUBE&#x2F;ROLLUP</a></p><p><a href="https://zhuanlan.zhihu.com/p/536981356">从源码深入理解 Spark SQL 中的 Grouping Sets 语句</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grouping sets </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于数据仓库建设的一些思考</title>
      <link href="/posts/49066d40.html"/>
      <url>/posts/49066d40.html</url>
      
        <content type="html"><![CDATA[<p>基于在海拍客的工作经历，沉淀了一些关于数据仓库的思考，没有框架，想到什么写什么</p><h1 id="数据仓库解决什么问题"><a href="#数据仓库解决什么问题？" class="headerlink" title="数据仓库解决什么问题？"></a>数据仓库解决什么问题？</h1><p>1、避免”烟囱式”开发，减少由于业务变化带来的维护成本<br>烟囱式开发，所有报表都是基于原始数据加工，SQL复杂度高，模型和指标无法复用，大量重复计算逻辑。一但某个业务变化，模型维护成本极高，而且大量的重复计算对资源消耗非常大，极易造成队列阻塞，影响数据产出</p><p>2、统一数据指标口径，保证数据一致性：定义一致、计算口径一致、数据源一致</p><ul><li>中文博大精深，一个简单的中文词经常包含些隐含信息。</li></ul><p>比如交易金额，当需求方说要取今天的交易额的时候，很多时候说的是今天的成功的交易金额，然而在逻辑角度，交易金额指的的订单表上支付金额+优惠券金额，不进行交易状态的条件过滤。</p><ul><li>鸡同鸭讲，说的是两个人沟通时说的不是同一个东西。</li></ul><p>比如订单的发货时间，对于财务业务，指的是订单表中的delivery_time字段，表示的是供货商提供的物流订单后第一次抓取到发货状态的时间；对于门店用户角度，指的是物流表中最终发送到用户手上那个订单的物流的发货时间。</p><p>3、数据结构清晰，方便数据查找和理解<br>数仓的分层设计能明晰每张表的作用域和职责，在需要查询使用时，能快速找到要用的表和理解每个字段的含义</p><h1 id="公司数仓存在什么问题"><a href="#公司数仓存在什么问题？" class="headerlink" title="公司数仓存在什么问题？"></a>公司数仓存在什么问题？</h1><ul><li>一张订单宽表打天下，其中包含了订单、交易、退款、物流、门店、供应商等多维度信息，下游以一个个数据烟囱的方式时间使用dw层的明细数据，无法收缩口径，保证数据的一致性。</li><li>基于onedate理论建设的数仓模型没有长期推广和取代订单宽表，人就有大量新报表从订单宽表获取数据</li></ul><h1 id="数据仓库模型分层设计方案"><a href="#数据仓库模型分层设计方案" class="headerlink" title="数据仓库模型分层设计方案"></a>数据仓库模型分层设计方案</h1><h2 id="基于主题域建设"><a href="#基于主题域建设" class="headerlink" title="基于主题域建设"></a>基于主题域建设</h2><h2 id><a href="#" class="headerlink" title></a></h2><h1 id="好的数据仓库设计评价标准"><a href="#好的数据仓库设计评价标准" class="headerlink" title="好的数据仓库设计评价标准"></a>好的数据仓库设计评价标准</h1><p>好的数仓设计标准应该是数据丰富完善、数据复用性强、数据规范性高。以下面的数仓架构为例<br><img src="https://i.328888.xyz/2023/04/24/iSq1LE.png" alt="iSq1LE.png"></p><h2 id="完善度"><a href="#完善度" class="headerlink" title="完善度"></a>完善度</h2><ul><li><p>dwd层完善度：衡量dwd层的完善度，看ods层被dw&#x2F;dws&#x2F;ads&#x2F;dim层依赖的数量（跨层引用率）。ods层被越多的非dwd层引用，说明越多任务基于原始数据进行开发，各种数据清洗、数据格式化存在重复计算。好的数仓设计一般要求ods层只能被dwd层引用，即跨层引用率为100%。</p></li><li><p>dw&#x2F;dws&#x2F;ads层完善度：衡量汇总数据的完善度，看仅靠dw&#x2F;dws&#x2F;ads层数据就能满足的查询比例（汇总层查询比例）。若汇总层数据无法满足查询要求，则需要从原始数据自行加工计算。汇总层查询比例不可能完全做到100%，但值越高，说明数仓上层模型建设越完善。</p></li></ul><h2 id="复用度"><a href="#复用度" class="headerlink" title="复用度"></a>复用度</h2><ul><li>模型引用系数：⼀个模型被读取，直接产出下游模型的平均数量。若对所有dwd层表（有下游）的模型引用系数取均值，则可衡量dwd层的模型引用系数。系数越大，说明数仓复用度越高。从数据血缘图来看，自下而上一条线的模型设计复用性差，复杂场景下这条线会极其长，而理想的模型设计应是交织的发散型结构<br><img src="https://i.328888.xyz/2023/04/24/ioZnHZ.png" alt="ioZnHZ.png"></li></ul><h2 id="规范度"><a href="#规范度" class="headerlink" title="规范度"></a>规范度</h2><ul><li><p>表分层规范：有多少表不能划属到数仓架构的某一层，一般从表命名前缀体现。</p></li><li><p>表命名规范：⼀个规范的表命名应该包括所属分层、所属主题域、调度周期、全量&#x2F;增量等信息。</p></li><li><p>字段命名规范：相同字段应在不同表保持一样的命名。同样是用户id，不能在A表叫user_id，在B表却叫u_id.</p></li></ul><p>数仓规范度越高，表名包含的信息越多，在数据地图查找表越方便，也更利于提高模型表复用度。</p><h1 id="一些博客文章"><a href="#一些博客文章" class="headerlink" title="一些博客文章"></a>一些博客文章</h1><p><a href="https://blog.51cto.com/u_15259710/2932712">如何避免数仓模型“烟囱式”建设</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>数仓建模之关于流量域建设</title>
      <link href="/posts/bbbc8dfb.html"/>
      <url>/posts/bbbc8dfb.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉，您输入的密码错误，请检查后重新输入。" data-whm="抱歉, 当前文章不能被校验, 不过您还是可以看看解密后的内容。">  <script id="hbeData" type="hbeData" data-hmacdigest="4304f858f102b5e1e05f225132d79abf84e95f7b78114b0fe4c30989e795a675">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">当前文章暂不对外可见，请输入密码后查看！</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> private </tag>
            
            <tag> 数仓建模 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在hive中嵌入自定义数据处理函数-UDF函数</title>
      <link href="/posts/cafe49d7.html"/>
      <url>/posts/cafe49d7.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 写UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之全面认识窗口函数</title>
      <link href="/posts/ed2327bc.html"/>
      <url>/posts/ed2327bc.html</url>
      
        <content type="html"><![CDATA[<p>本文内容来自文章<a href="https://mp.weixin.qq.com/s/VnQT-bidnJDduoLfJeRhxA">Hive SQL大厂必考常用窗口函数及面试题</a></p><p>受岗位性质和工作内容影响，在我从事数仓开发工作至今，对于窗口函数的使用场景都很基础，常用的也只有row_number、sum、max&#x2F;min，<br>偶尔碰到些其他场景，因为不熟悉，可能就需要反复查看官方文档确认。</p><p>所以在上面文章阅读过程中，基于个人理解，重新梳理写了本文</p><h1 id="窗口函数概述"><a href="#窗口函数概述" class="headerlink" title="窗口函数概述"></a>窗口函数概述</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics">hive官方介绍</a></p><p>窗口函数也称为OLAP函数，是数据分析最常用到的函数，熟练的掌握窗口函数的各种用法和骚操作对从事数据工作者是很重要的。</p><p>与聚合函数将多条记录聚合为一条不同，窗口函数每条记录都会执行，执行前后数据量不变，且窗口函数兼具分组和排序两种功能。</p><h2 id="窗口函数用法"><a href="#窗口函数用法" class="headerlink" title="窗口函数用法"></a>窗口函数用法</h2><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">窗口函数</span>&gt;</span> over ([partition by <span class="tag">&lt;<span class="name">列名</span>&gt;</span>] [order by <span class="tag">&lt;<span class="name">排序列名</span>&gt;</span>] [window_frame])</span><br></pre></td></tr></table></figure><p>其中：</p><p>&lt;窗口函数&gt;: 指需要使用的分析函数，如row_number()、sum()等。</p><p>over() : 用来指定函数执行的窗口范围，这个数据窗口大小可能会随着行的变化而变化。<br>如果括号中什么都不写，则意味着窗口包含满足where条件的所有行，窗口函数基于所有行进行计算</p><p>window_frame: 在分组窗口基础上，可以进一步指定窗口计算边界</p><h2 id="设置窗口"><a href="#设置窗口" class="headerlink" title="设置窗口"></a>设置窗口</h2><h3 id="1partition-by子句"><a href="#1）partition-by子句" class="headerlink" title="1）partition by子句"></a>1）partition by子句</h3><p>窗口划分分组条件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    uid,</span><br><span class="line">    score,</span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid) <span class="keyword">AS</span> sum_score</span><br><span class="line"><span class="keyword">FROM</span> exam_record</span><br></pre></td></tr></table></figure><h3 id="2order-by子句"><a href="#2）order-by子句" class="headerlink" title="2）order by子句"></a>2）order by子句</h3><p>窗口排序条件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    uid,</span><br><span class="line">    score,</span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">OVER</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> uid) <span class="keyword">AS</span> sum_score</span><br><span class="line"><span class="keyword">FROM</span> exam_record</span><br></pre></td></tr></table></figure><h3 id="3指定窗口大小"><a href="#3）指定窗口大小" class="headerlink" title="3）指定窗口大小"></a>3）指定窗口大小</h3><p>指定窗口大小，又称为窗口框架。框架是重新定义窗口计算边界，框架有两种范围限定方式：</p><ul><li><p>一种是使用 ROWS 子句，通过指定当前行之前或之后的固定数目的行来限制分区中的行数。</p></li><li><p>另一种是使用 RANGE 子句，按照排列序列的当前值，根据相同值来确定分区中的行数。</p></li></ul><p>语法<code>ORDER BY 字段名 RANGE|ROWS 边界规则0 | [BETWEEN 边界规则1 AND 边界规则2]</code>，边界规则的可取值如下：</p><ul><li><code>current row</code>：当前行</li><li><code>n preceding</code>：当前行及往前n行数据</li><li><code>unbounded preceding</code>：第一行至当前行数据</li><li><code>n following</code>：当前行及往后n行数据</li><li><code>unbounded following</code>：当前行至最后一行数据</li></ul><p>需要注意的是，</p><ul><li>使用框架时必须有order by子句</li><li>若仅有order by子句而未指定框架，则默认框架语句为<code>range unbounded preceding and current row</code>，<br><a href="https://llye-hub.github.io/posts/5af52219.html">详情见文章</a></li></ul><h3 id="4window_name"><a href="#4）window-name" class="headerlink" title="4）window_name"></a>4）window_name</h3><p>给窗口指定一个别名<code>WINDOW my_window_name AS (PARTITION BY uid ORDER BY score)</code>，<br>适用于一个窗口被多次使用，可以使sql简洁清晰，也易于维护</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    uid,</span><br><span class="line">    score,</span><br><span class="line">    <span class="built_in">rank</span>() <span class="keyword">OVER</span> my_window_name <span class="keyword">AS</span> rk_num,</span><br><span class="line">    <span class="built_in">row_number</span>() <span class="keyword">OVER</span> my_window_name <span class="keyword">AS</span> row_num,</span><br><span class="line">    <span class="built_in">dense_rank</span>() <span class="keyword">OVER</span> my_window_name <span class="keyword">AS</span> dr_num</span><br><span class="line"><span class="keyword">FROM</span> exam_record</span><br><span class="line"><span class="keyword">WHERE</span> score<span class="operator">&gt;=</span><span class="number">60</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> uid</span><br><span class="line"><span class="keyword">WINDOW</span> my_window_name <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> score)</span><br></pre></td></tr></table></figure><h2 id="窗口函数分类"><a href="#窗口函数分类" class="headerlink" title="窗口函数分类"></a>窗口函数分类</h2><p>窗口函数：</p><ul><li>first_value: 返回计算窗口内按排序条件的第一个值，语法<code>first_value(exp_str,true|false)</code></li><li>last_value: 返回计算窗口内按排序条件的最后一个值，语法<code>last_value(exp_str,true|false)</code></li><li>lag: 返回相对当前行，第前n行的数据，语法<code>lag(exp_str,offset,defval) over(partition by .. order by …)</code></li><li>lead: 返回相对当前行，第后n行的数据，语法<code>lead(exp_str,offset,defval) over(partition by .. order by …)</code></li></ul><p>配合over语句使用的聚合函数：</p><ul><li>sum</li><li>count([distinct])</li><li>max</li><li>min</li><li>avg</li></ul><p>分析函数：</p><ul><li>row_number: 连续排序——1、2、3、4</li><li>rank: 并列跳号排序——1、1、3、4</li><li>dense_rank: 并列连续排序——1、1、2、3</li><li>percent_rank: 将某个数值在数据集中的rank()排位作为数据集的百分比值返回，每行按照公式(rank-1) &#x2F; (rows-1)进行计算，百分比值的范围为 0 到 1。<br>可用于计算值在数据集内的相对位置。语法<code>percent_rank(exp_str)</code></li><li>cume_dist: 如果按升序排列，则统计：小于等于当前值的行数&#x2F;总行数。<br>       如果是降序排列，则统计：大于等于当前值的行数&#x2F;总行数。<br>       语法<code>cume_dist(exp_str)</code></li><li>ntiles: 将分组数据按照顺序平均切分成n组，并返回当前切片值。语法<code>ntiles(n)</code>。<br>    如果不能平均分配，则优先分配较小编号的切片，并且各个切片中能放的行数最多相差 1。<br>    可简单理解为，有 n 个桶，按编号 1-n 的顺序逐个将分组数据放到每个桶内，直至数据分配完毕。</li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hiveSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读书笔记之数据仓库工具箱维度建模权威指南(第3版)</title>
      <link href="/posts/4142350a.html"/>
      <url>/posts/4142350a.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉，您输入的密码错误，请检查后重新输入。" data-whm="抱歉, 当前文章不能被校验, 不过您还是可以看看解密后的内容。">  <script id="hbeData" type="hbeData" data-hmacdigest="fe1d605873a64a776899cd24903c2cfb2b3f6754276eb8f0e33aa5b5ff6dec62">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">当前文章暂不对外可见，请输入密码后查看！</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
            <tag> private </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内置函数之reflect</title>
      <link href="/posts/1d24daf8.html"/>
      <url>/posts/1d24daf8.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 内置函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建spark on yarn源码调试</title>
      <link href="/posts/5e1f3fb0.html"/>
      <url>/posts/5e1f3fb0.html</url>
      
        <content type="html"><![CDATA[<p><strong>参考文章</strong><br><a href="https://www.cnblogs.com/qixing/p/14017875.html">Spark3.0.1各种集群模式搭建及spark on yarn日志配置</a><br><a href="https://www.jianshu.com/p/aa6f3a366727">Spark on Yarn集群搭建详细过程</a></p><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>jdk8+Hadoop3.3.1 </p><h1 id="spark的几种部署方式"><a href="#spark的几种部署方式" class="headerlink" title="spark的几种部署方式"></a>spark的几种部署方式</h1><p>Spark作为准实时大数据计算引擎，Spark的运行需要依赖资源调度和任务管理，Spark自带了standalone模式资源调度和任务管理工具，运行在其他资源管理和任务调度平台上，如Yarn、Mesos、Kubernates容器等。</p><p>spark的搭建和Hadoop差不多，主要有下面几种部署方式：</p><ul><li><p>Local：多用于本地测试，如在eclipse，idea中写程序测试等。</p></li><li><p>Standalone：Standalone是Spark自带的一个资源调度框架，它支持完全分布式。</p></li><li><p>Yarn：Hadoop生态圈里面的一个资源调度框架，Spark也是可以基于Yarn来计算的。</p></li></ul><p>基于个人学习需求，本文仅记录Local模式部署过程。</p><h1 id="下载spark源码"><a href="#下载spark源码" class="headerlink" title="下载spark源码"></a>下载spark源码</h1><p><a href="https://archive.apache.org/dist/spark/">下载地址</a></p><p><strong>解压</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zcvf spark-3.2.0-bin-hadoop3.2.tgz</span><br></pre></td></tr></table></figure><p><strong>配置环境变量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SPARK_HOME=/Users/llye/workspace/spark-3.2.0-bin-hadoop3.2</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span></span><br><span class="line">vi ~/.bash_profile </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生效</span></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h1 id="本地local模式"><a href="#本地local模式" class="headerlink" title="本地local模式"></a>本地local模式</h1><p><strong>测试样例</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/bin </span><br><span class="line">run-example SparkPi 10  # 可计算出结果</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/03/24/iV6lXq.png" alt="iV6lXq.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell  # 启动成功，说明Local模式部署成功</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/03/24/iV6HUw.png" alt="iV6HUw.png"></p><p>启动成功后，访问<code>http://localhost:4040/</code> 即可进行web UI监控页面访问</p><h1 id="standalone模式未完成不具参考性"><a href="#Standalone模式（未完成，不具参考性）" class="headerlink" title="Standalone模式（未完成，不具参考性）"></a>Standalone模式（未完成，不具参考性）</h1><p>配置Spark on Yarn集群</p><p><strong>修改spark-env.sh文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/conf</span><br><span class="line">cat &gt; spark-env.sh &lt;&lt; EOF</span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_301.jdk/Contents/Home/</span><br><span class="line">SCALA_HOME=/Users/llye/soft/scala/</span><br><span class="line">HADOOP_HOME=/Users/llye/soft/hadoop-3.3.1/</span><br><span class="line">HADOOP_CONF_DIR=/Users/llye/soft/hadoop-3.3.1/etc/hadoop/</span><br><span class="line">YARN_CONF_DIR=/Users/llye/soft/hadoop-3.3.1/etc/hadoop/etc/hadoop/</span><br><span class="line">SPARK_MASTER_HOST=spark    # 主节点机器名称</span><br><span class="line">SPARK_MASTER_PORT=7077     # 默认端口号7077</span><br><span class="line">SPARK_HOME=/Users/llye/workspace/spark-3.2.0-bin-hadoop3.2/</span><br><span class="line">SPARK_LOCAL_DIRS=/Users/llye/workspace/spark-3.2.0-bin-hadoop3.2/</span><br><span class="line">SPARK_LIBARY_PATH=/Library/Java/JavaVirtualMachines/jdk1.8.0_301.jdk/Contents/Home/lib/:/Users/llye/soft/hadoop-3.3.1/lib/native/</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>修改slaves配置文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/sbin </span><br><span class="line">vi slaves</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark001</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark002</span></span><br></pre></td></tr></table></figure><p><strong>将spark目录发送到其他机器</strong></p><p>创建workers文件，指定Worker节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/conf</span><br><span class="line">cat &gt; workers &lt;&lt; EOF</span><br><span class="line">worker1</span><br><span class="line">worker2</span><br><span class="line">worker3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="启动spark-on-yarn集群"><a href="#启动Spark-on-Yarn集群" class="headerlink" title="启动Spark on Yarn集群"></a>启动Spark on Yarn集群</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/sbin</span><br></pre></td></tr></table></figure><p>在Spark节点上启动Spark Master节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-master.sh</span><br></pre></td></tr></table></figure><p>在Worker节点上启动Spark Worker节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-worker.sh spark://spark:7077</span><br></pre></td></tr></table></figure><h1 id="登录spark-on-yarn集群"><a href="#登录Spark-on-Yarn集群" class="headerlink" title="登录Spark on Yarn集群"></a>登录Spark on Yarn集群</h1><p>登录Master：</p><p>登录Worker：<code>http://localhost:8081/</code></p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark on yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive本机安装</title>
      <link href="/posts/47d5b7b0.html"/>
      <url>/posts/47d5b7b0.html</url>
      
        <content type="html"><![CDATA[<p><strong>参考文章</strong></p><p><a href="https://zhuanlan.zhihu.com/p/68748400">Hive源码系列（一）hive2.1.1+hadoop2.7.3环境搭建</a></p><p><a href="https://juejin.cn/post/7114252763621490719">Hive安装超详细教程</a></p><p><a href="https://www.cnblogs.com/swordfall/p/13426569.html#auto_id_14">Hive架构与源码分析</a></p><p><a href="https://datavalley.github.io/2015/10/16/Hive%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">Hive:源码解析之本地环境搭建</a></p><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>jdk8 + Hadoop3.3.1</p><h1 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h1><p><a href="https://dlcdn.apache.org/hive/">下载地址</a></p><p><strong>解压安装包</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.3.9-bin.tar.gz</span><br></pre></td></tr></table></figure><p><strong>环境变量配置</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HIVE_HOME=/Users/llye/soft/hive-2.3.9</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span></span></span><br><span class="line">vi ~/.bash_profile </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生效</span></span><br><span class="line">source ~/.bash_profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证</span></span><br><span class="line">hive --version</span><br></pre></td></tr></table></figure><p><strong>修改配置文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd $HIVE_HOME/conf</span><br></pre></td></tr></table></figure><p><code>vi hive-site.xml</code>编辑内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  以mysql作为hive元数据库  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="symbol">&amp;amp;</span>useSSL=false<span class="symbol">&amp;amp;</span>serverTimezone=GMT<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>hive metastore连接串<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.cj.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Hive metastore JDBC驱动<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Mysql登录账号<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rootroot<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Mysql登录密码<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 忽略HIVE 元数据库版本的校验，如果非要校验就得进入MYSQL升级版本 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--  配置hive用户名、密码  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.jdbc_passwd.auth.root<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rootroot<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.jdbc_passwd.auth.llye<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rootroot<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- hiveserver2 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  配置用户安全认证方式  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.authentication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>NONE<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">            Expects one of [nosasl, none, ldap, kerberos, pam, custom].</span><br><span class="line">            Client authentication types.</span><br><span class="line">            NONE: no authentication check</span><br><span class="line">            LDAP: LDAP/AD based authentication</span><br><span class="line">            KERBEROS: Kerberos/GSSAPI authentication</span><br><span class="line">            CUSTOM: Custom authentication provider</span><br><span class="line">            (Use with property hive.server2.custom.authentication.class)</span><br><span class="line">            PAM: Pluggable authentication module</span><br><span class="line">            NOSASL:  Raw transport</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.custom.authentication.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.contrib.auth.CustomPasswdAuthenticator<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>配置用于权限认证的类【这里实际没有】<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 hiveserver2 jdbc连接的 host+port --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>hiveserver2 jdbc连接的 host<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>hiveserver2 jdbc连接的端口号<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  配置webUI界面 host+port  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.webui.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.webui.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10002<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>下载连接MySQL的驱动包到hive的lib目录下</strong></p><p><code>mysql-connector-java-8.0.17.jar</code><a href="https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.17/mysql-connector-java-8.0.17.jar">下载地址</a></p><p><strong>初始化hive元数据库</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $HIVE_HOME/bin</span><br><span class="line">schematool -initSchema -dbType mysql -verbose</span><br></pre></td></tr></table></figure><p><strong>验证初始化是否成功</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- mysql的hivedb库中，若展示多个数据表，即代表初始化成功</span></span><br><span class="line"><span class="keyword">show</span> tables;</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/03/15/JxW5E.png" alt="JxW5E.png"></p><p><strong>启动hive</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/start-dfs.sh &amp;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/start-yarn.sh</span></span><br><span class="line">cd $HIVE_HOME </span><br><span class="line">hive </span><br></pre></td></tr></table></figure><p>遇到启动报错<code>org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /tmp/hive</code>时，执行命令<code>hdfs dfsadmin -safemode leave</code>关闭HDFS安全模式</p><p><strong>验证</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(id <span class="type">int</span>, name string);</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;abc&#x27;</span>);</span><br><span class="line"><span class="comment">-- 查询数据</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/03/15/J73cF.png" alt="J73cF.png"></p><h1 id="beeline连接hiveserver2"><a href="#beeline连接hiveserver2" class="headerlink" title="beeline连接hiveserver2"></a>beeline连接hiveserver2</h1><p><strong>hadoop配置</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd $HADOOP_HOME/etc/hadoop</span><br></pre></td></tr></table></figure><p><code>vi core-site.xml</code>补充内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.llye.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.llye.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>重启hadoop</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/stop-all.sh &amp;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/start-all.sh</span> </span><br></pre></td></tr></table></figure><p><strong>启动metastore</strong></p><p>配置了hive的环境变量，任意文件夹下执行即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore</span><br></pre></td></tr></table></figure><p><strong>启动hiveserver2</strong></p><p>配置了hive的环境变量，任意文件夹下执行即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hiveserver2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">hive --service hiveserver2</span><br></pre></td></tr></table></figure><p>若<code>hiveserver2</code>启动失败，检查1000端口是否被占用，命令<code>lsof -i:10000</code>和<code>kill -9 xxx</code></p><p><strong>beeline连接</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">beeline</span><br><span class="line"><span class="meta prompt_">beeline&gt; </span><span class="language-bash">!connect jdbc:hive2://localhost:10000/default</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">beeline -u jdbc:hive2://localhost:10000/default </span><br></pre></td></tr></table></figure><p>遇到报错问题的参考：</p><p><a href="https://blog.csdn.net/qq_16633405/article/details/82190440">beeline连接hiveserver2报错：User: root is not allowed to impersonate root</a></p><p><a href="https://developer.aliyun.com/article/606803">Hive JDBC：Permission denied: user&#x3D;anonymous, access&#x3D;EXECUTE, inode&#x3D;”&#x2F;tmp”</a></p><h1 id="客户端jdbc连接hive库"><a href="#客户端jdbc连接hive库" class="headerlink" title="客户端jdbc连接hive库"></a>客户端jdbc连接hive库</h1><p><strong>启动metastore和hiveserver2</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore</span><br><span class="line"></span><br><span class="line">hiveserver2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或</span></span><br><span class="line">hive --service hiveserver2</span><br></pre></td></tr></table></figure><p><code>DBeaver</code>连接，设置jdbc URL：<code>jdbc:hive2://localhost:10000/default</code></p><h1 id="hive源码编译"><a href="#hive源码编译" class="headerlink" title="hive源码编译"></a>hive源码编译</h1><p><strong>解压安装包</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.3.9-src.tar.gz</span><br></pre></td></tr></table></figure><p><strong>编译源码</strong></p><p>进入解压目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -DskipTests -Phadoop-2 -Pdist</span><br></pre></td></tr></table></figure><p>编译过程中报错<code>An error has occurred in JavaDocs report generation:Exit code: 1 - javadoc: error - invalid flag: -author</code>，<a href="https://stackoverflow.com/questions/19181236/an-error-has-occurred-in-javadocs-report-generationexit-code-1-javadoc-erro">解决方案</a>:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-javadoc-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;maven.javadoc.plugin.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>resourcesdoc.xml<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>javadoc<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">phase</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>$&#123;project.build.sourceEncoding&#125;<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">verbose</span>&gt;</span>true<span class="tag">&lt;/<span class="name">verbose</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">show</span>&gt;</span>public<span class="tag">&lt;/<span class="name">show</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">doclet</span>&gt;</span>com.sun.jersey.wadl.resourcedoc.ResourceDoclet<span class="tag">&lt;/<span class="name">doclet</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">docletArtifacts</span>&gt;</span></span><br><span class="line">          ……</span><br><span class="line">        <span class="tag">&lt;/<span class="name">docletArtifacts</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">additionalparam</span>&gt;</span>-output $&#123;project.build.outputDirectory&#125;/resourcedoc.xml<span class="tag">&lt;/<span class="name">additionalparam</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  pom文件中加上此项配置  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">useStandardDocletOptions</span>&gt;</span>false<span class="tag">&lt;/<span class="name">useStandardDocletOptions</span>&gt;</span>   </span><br><span class="line">      <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>导入idea</strong></p><p>从hive的解压目录中选择pom.xml文件导入</p><p><strong>调试代码</strong></p><p>进入解压目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd  packaging/target/apache-hive-2.3.9-bin/apache-hive-2.3.9-bin</span><br><span class="line"></span><br><span class="line">hive --debug -hiveconf hive.root.logger=DEBUG,console</span><br></pre></td></tr></table></figure><p>成功时，界面出现：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Listening for transport dt_socket at address: 8000</span><br></pre></td></tr></table></figure><p>JVM会监听8000端口，等待客户端调试连接。</p><p>进入idea配置远程连接如下：<br><img src="https://i.328888.xyz/2023/03/20/Pz9vq.png" alt="Pz9vq.png"></p><p>hive的CLI的入口类为：<code>src/java/org/apache/hadoop/hive/cli/CliDriver.java</code>，断点调试成功如下：<br><img src="https://i.328888.xyz/2023/03/20/PFA3q.png" alt="PFA3q.png"></p>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop本机安装</title>
      <link href="/posts/2cb81866.html"/>
      <url>/posts/2cb81866.html</url>
      
        <content type="html"><![CDATA[<p><strong>参考文章</strong></p><p><a href="https://zhuanlan.zhihu.com/p/68748400">Hive源码系列（一）hive2.1.1+hadoop2.7.3环境搭建</a></p><p><a href="https://cloud.tencent.com/developer/article/1708064">Hadoop【单机安装-测试程序WordCount】</a></p><p>Hadoop 安装有三种方式：</p><p><code>单机模式</code>：安装简单，几乎不用做任何配置，但仅限于调试用途；</p><p><code>伪分布模式</code>：在单节点上同时启动 NameNode、DataNode、JobTracker、TaskTracker、Secondary Namenode 等 5 个进程，模拟分布式运行的各个节点；</p><p><code>完全分布式模式</code>：正常的 Hadoop 集群，由多个各司其职的节点构成。</p><p>本人选择的是<code>伪分布模式</code>安装</p><p><strong>下载安装</strong></p><p><a href="https://hadoop.apache.org/">下载地址</a></p><ul><li>解压安装包</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.1.tar.gz</span><br></pre></td></tr></table></figure><p><strong>环境变量配置</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HADOOP_HOME=/Users/llye/soft/hadoop-3.3.1</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$PATH</span></span></span><br><span class="line">vi ~/.bash_profile </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生效</span></span><br><span class="line">source ~/.bash_profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证</span></span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure><p><strong>修改配置文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd $HADOOP_HOME/etc/hadoop</span><br></pre></td></tr></table></figure><p><code>vi core-site.xml</code>编辑内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置分布式文件系统的schema和ip以及port,默认8020--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi hdfs-site.xml</code>编辑内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置副本数，注意，伪分布模式只能是1。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi hadoop-env.sh</code>编辑内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_301.jdk/Contents/Home</span><br></pre></td></tr></table></figure><p><code>vi mapred-site.xml</code>编辑内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi yarn-site.xml</code>编辑内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>ssh免密码登录</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P &#x27;&#x27; -f ~/.ssh/id_rsa</span><br><span class="line">cat ~/.ssh/id_rsa.pub&gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 0600~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>以前安装其他软件已操作过，所以此步骤忽略</p><p><strong>格式化namenode</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>忽略<code>SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1</code></p><p>有<code>INFO common.Storage: Storage directory /tmp/hadoop-llye/dfs/name has been successfully formatted.</code>即说明操作成功。</p><p><strong>启动</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/start-dfs.sh &amp;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/start-yarn.sh</span></span><br></pre></td></tr></table></figure><p><strong>验证</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@localhost hadoop % jps</span><br><span class="line">11440 </span><br><span class="line">11169 </span><br><span class="line">74514 NameNode # 名称节点</span><br><span class="line">74756 SecondaryNameNode</span><br><span class="line">74999 ResourceManager</span><br><span class="line">75098 NodeManager</span><br><span class="line">75834 Jps</span><br><span class="line">19771 Launcher</span><br><span class="line">74620 DataNode  # 数据节点</span><br></pre></td></tr></table></figure><p><strong>访问UI：ip+port</strong></p><p>All Applications：<code>http://localhost:8088/cluster/apps</code></p><p>Applications running on this node：<code>http://localhost:8042/node/allApplications</code><br><img src="https://i.328888.xyz/2023/03/15/JCU0J.png" alt="JCU0J.png"></p><p>Browse Hdfs：<code>http://localhost:9870/</code><br><img src="https://i.328888.xyz/2023/03/15/J0HkZ.png" alt="J0HkZ.png"></p><p>这里需要注意的是，因为安装的是3.x版本，所以端口号为9870</p><p>若安装的是2.x版本，则端口号为50070</p><p><img src="https://i.328888.xyz/2023/03/15/J0ehd.png" alt="J0ehd.png"></p><p><strong>测试程序</strong></p><ul><li>测试一</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $HADOOP_HOME</span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 2 100</span><br></pre></td></tr></table></figure><ul><li>测试二</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cd $HADOOP_HOME</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建一个hdfs目录</span></span><br><span class="line">hdfs dfs -mkdir /wordcount</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">造数据</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hello hadoop</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hello world</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hello hadoop</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hello hangzhou</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hello hangzhou</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hello hadoop</span></span><br><span class="line">mkdir wordCount</span><br><span class="line">cd wordCount</span><br><span class="line">touch wc.input</span><br><span class="line">vi wc.input</span><br><span class="line">cat wc.input</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">上传本地文件到指定目录</span></span><br><span class="line">hdfs dfs -put wc.input /wordcount</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行mr程序</span></span><br><span class="line">cd $HADOOP_HOME</span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /wordcount/ /wordcount/output</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看mr计算结果</span></span><br><span class="line">hadoop fs -cat /wordcount/output/part-r-00000</span><br></pre></td></tr></table></figure><p><strong>停止</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/stop-dfs.sh &amp;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HADOOP_HOME/sbin/stop-yarn.sh</span></span><br></pre></td></tr></table></figure><p><strong>补充：hadoop启动会遇到的问题</strong></p><ul><li>namenode启动失败，使用<code>jps</code>命令查看时无namenode进程</li></ul><p>解决办法：</p><ol><li>执行命令<code>hdfs namenode -format</code>重新格式化namenode</li><li>执行命令<code>hadoop-daemon.sh start namenode</code>单独启动namenode，若是其他哪个进程挂了也可以采取这种方式。（本地尚未试过这种解决方式）</li><li>修改<code>core-site.xml</code>和<code>hdfs-site.xml</code>配置，再重新格式化namenode。因为系统重启后namenode和datanode的信息被清理了</li></ol><p><code>core-site.xml</code>补充配置如下：</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/Users/llye/hadoop/tmp&lt;/value&gt;&lt;description&gt;Abase for other temporary directories.&lt;/description&gt;&lt;/property&gt;</code></pre><p><code>hdfs-site.xml</code>补充配置如下：</p><pre><code class="xml">&lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;/Users/llye/hadoop/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;/Users/llye/hadoop/tmp/dfs/data&lt;/value&gt;&lt;/property&gt;</code></pre><p>若执行命令<code>hdfs namenode -format</code>报错：<br><img src="https://i.328888.xyz/2023/05/05/iTZ6qL.png" alt="iTZ6qL.png"><br>上面报错原因是权限不够，无法再目录内新建文件，解决办法是执行命令<code>sudo chmod -R a+w /Users/llye/hadoop</code>，再格式化namenode就ok了。</p><p>修改配置后，系统启动后会默认路径下的文件作为namenode、datanode的配置信息</p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>终端常用命令汇总</title>
      <link href="/posts/84fddb38.html"/>
      <url>/posts/84fddb38.html</url>
      
        <content type="html"><![CDATA[<h1 id="获取本机ip地址"><a href="#获取本机ip地址" class="headerlink" title="获取本机ip地址"></a>获取本机ip地址</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/sh</span></span><br><span class="line">local_ip=`ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk &#x27;&#123;print $2&#125;&#x27;|tr -d &quot;addr:&quot;​`</span><br><span class="line">echo &quot;$&#123;local_ip&#125;&quot;</span><br><span class="line"></span><br><span class="line">ifconfig -a        //和window下执行此命令一样道理，返回本机所有ip信息</span><br><span class="line">grep inet    //截取包含ip的行</span><br><span class="line">grep -v 127.0.0.1//去掉本地指向的那行</span><br><span class="line">grep -v inet6//去掉包含inet6的行</span><br><span class="line">awk &#123; print $2&#125;//$2 表示默认以空格分割的第二组 同理 $1表示第一组​</span><br><span class="line">tr -d &quot;addr:&quot;//删除&quot;addr:&quot;这个字符串</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/finghting321/article/details/108476650">https://blog.csdn.net/finghting321/article/details/108476650</a></p><h1 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -iname $filename 2&gt;  /dev/null</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>终端免密登录mysql</title>
      <link href="/posts/b31f5f52.html"/>
      <url>/posts/b31f5f52.html</url>
      
        <content type="html"><![CDATA[<p>参考资料：<a href="https://aijishu.com/a/1060000000088457">Mysql Shell免密登录的思考及实际应用案例</a></p><p>常见终端登录mysql的方式是通过命令<code>mysql -u&#123;user&#125; -p&#123;password&#125;</code>，每次登录都需要输入一长串命令和参数，我觉得麻烦，且这种方式下密码直接暴露出来是不安全的</p><p>虽然也可用命令<code>mysql -u&#123;user&#125; -p</code> + 手动输入密码，也是麻烦的，而且还要记密码</p><p>所以，如果仅用命令<code>mysql</code>即可实现登录，那得多方便</p><p>从网上搜索后发现，可以通过明文配置文件的方式实现mysql免密登录</p><p>具体命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑配置文件</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">[mysql]</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">user=xxx</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">password=xxx</span></span><br><span class="line">sudo vi /etc/my.cnf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看文件内容</span></span><br><span class="line">cat  /etc/my.cnf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定mysql server仅从这个配置文件读取参数</span></span><br><span class="line">mysql --defaults-file=/etc/my.cnf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证读取配置情况</span></span><br><span class="line">mysql --defaults-file=/etc/my.cnf --print-defaults</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/03/14/9EVXa.png" alt="9EVXa.png"></p>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 免密登录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql练习之连续登录问题</title>
      <link href="/posts/67cc9ac.html"/>
      <url>/posts/67cc9ac.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s?__biz=MzU5NTc1NzE2OA==&mid=2247484833&idx=1&sn=2a965091ec5ec09fe10185fecbf92779&chksm=fe6c54bec91bdda83229a33ceefb0358f408ed24ac5048eb48dbe6911b795b1536971a98ab57&scene=21#wechat_redirect">题目来源</a></p><h1 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h1><p>求出连续3天登录的用户id</p><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> if <span class="keyword">not</span> <span class="keyword">exists</span> one.user_login(</span><br><span class="line">     id <span class="type">int</span> COMMENT <span class="string">&#x27;用户主键&#x27;</span>,</span><br><span class="line">     dt <span class="type">varchar</span>(<span class="number">20</span>) COMMENT <span class="string">&#x27;登录日期&#x27;</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1001</span>,<span class="string">&#x27;2021-12-12&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1002</span>,<span class="string">&#x27;2021-12-12&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1001</span>,<span class="string">&#x27;2021-12-13&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1001</span>,<span class="string">&#x27;2021-12-14&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1001</span>,<span class="string">&#x27;2021-12-16&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1002</span>,<span class="string">&#x27;2021-12-16&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1001</span>,<span class="string">&#x27;2021-12-19&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1002</span>,<span class="string">&#x27;2021-12-17&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  user_login <span class="keyword">values</span>(<span class="number">1001</span>,<span class="string">&#x27;2021-12-20&#x27;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h1><h2 id="解法一自关联"><a href="#解法一：自关联" class="headerlink" title="解法一：自关联"></a>解法一：自关联</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">distinct</span> id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">a.id ,</span><br><span class="line">a.dt <span class="keyword">as</span> dt1 ,</span><br><span class="line">b.dt <span class="keyword">as</span> dt2</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">user_login a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> user_login b <span class="keyword">on</span></span><br><span class="line">a.id <span class="operator">=</span> b.id</span><br><span class="line"><span class="keyword">and</span> (b.dt <span class="keyword">between</span> DATE_SUB(a.dt, <span class="type">interval</span> <span class="number">2</span> <span class="keyword">day</span>) <span class="keyword">and</span> a.dt)</span><br><span class="line">) tmp1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">id,</span><br><span class="line">dt1</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line"><span class="built_in">count</span>(<span class="number">1</span>) <span class="operator">=</span> <span class="number">3</span></span><br><span class="line">)tmp2</span><br><span class="line">;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 题集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql练习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL之窗口函数的边界</title>
      <link href="/posts/5af52219.html"/>
      <url>/posts/5af52219.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>窗口函数常用于在SQL数据分析计算各种统计指标，也是日常sql开发中常见的函数了，但是最近发现自己在这方面存在一些误解</p><p>比如下面这段sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> col1</span><br><span class="line">    ,<span class="built_in">sum</span>(col2) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> col1 <span class="keyword">order</span> <span class="keyword">by</span> col3) <span class="keyword">as</span> sum1</span><br><span class="line">    ,<span class="built_in">sum</span>(col2) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> col1) <span class="keyword">as</span> sum2</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> (<span class="keyword">VALUES</span>(<span class="string">&#x27;a&#x27;</span>,<span class="number">1</span>,<span class="number">4</span>),(<span class="string">&#x27;a&#x27;</span>,<span class="number">2</span>,<span class="number">7</span>),(<span class="string">&#x27;a&#x27;</span>,<span class="number">3</span>,<span class="number">6</span>)) t(col1,col2,col3)) a</span><br></pre></td></tr></table></figure><p>第一眼感觉<code>sum1</code>和<code>sum2</code>字段计算值是一样的，但实际运行出来的结果为(<code>mysql</code>+<code>hiveSQL</code>)</p><table><thead><tr><th>col1</th><th>sum1</th><th>sum2</th></tr></thead><tbody><tr><td>a</td><td>1</td><td>6</td></tr><tr><td>a</td><td>4</td><td>6</td></tr><tr><td>a</td><td>6</td><td>6</td></tr></tbody></table><p>从执行结果上来看，<code>sum1</code>字段为窗口内的累加值，<code>sum2</code>字段值为窗口内所有值之和</p><h1 id="为什么有无order-by差异这么大"><a href="#为什么有无order-by差异这么大" class="headerlink" title="为什么有无order by差异这么大"></a>为什么有无order by差异这么大</h1><p>有人会说，聚合函数<code>sum()</code>的窗口内有<code>order by</code>子句时，计算结果本就是累加性质。从执行结果上来看，这么说是对的，但是这种解释太流于表面，并没有真正从函数定义上解释为什么</p><p>这里重新回顾下窗口函数基本语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&lt;</span>window_function<span class="operator">&gt;</span> <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="operator">&lt;</span>column_name<span class="operator">&gt;</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">&lt;</span>column_name<span class="operator">&gt;</span> <span class="operator">&lt;</span>window_frame<span class="operator">&gt;</span>)</span><br></pre></td></tr></table></figure><p>主要有四个部分：</p><ul><li>window_function：函数，比如：sum、row_number、first_value</li><li>partition by：窗口分区子句</li><li>order by：窗口排序子句</li><li>window_frame：窗口框架，限制窗口的边界大小</li></ul><p>对照基本语法，有<code>order by</code>子句的执行结果就是计算窗口边界为起始行至当前行的<code>sum</code>结果，<br>即<code>sum(col2) over(partition by col1 order by col3)</code><br>等同于<code>sum(col2) over(partition by col1 order by col3 rows between unbounded preceding and current row)</code> </p><h1 id="窗口函数的官方说明"><a href="#窗口函数的官方说明" class="headerlink" title="窗口函数的官方说明"></a>窗口函数的官方说明</h1><p><a href="https://dev.mysql.com/doc/refman/8.0/en/window-functions-frames.html">mysql官方文档</a> </p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics#LanguageManualWindowingAndAnalytics-PARTITIONBYwithonepartitioningcolumn,noORDERBYorwindowspecification">hive官方文档</a></p><p><code>mysql</code>关于窗口函数的window_frame有如下说明：<br><img src="https://i.328888.xyz/2023/03/01/6DIOt.png" alt="6DIOt.png"></p><p><code>hive</code>关于窗口函数的window_frame有如下说明：<br><img src="https://i.328888.xyz/2023/03/01/6DNeJ.png" alt="6DNeJ.png"></p><p>根据以上官方说明可知，<br>当<code>window_frame</code>子句和<code>order by</code>子句都没有时，窗口计算默认包含窗口内的所有数据，即<code>window_frame=ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</code>；<br>当仅有<code>order by</code>子句，没有<code>window_frame</code>子句时，窗口计算默认仅包含排序后起始行至当前行的数据，即<code>window_frame=ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 窗口函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Java PriorityQueue类的使用场景</title>
      <link href="/posts/76a5661e.html"/>
      <url>/posts/76a5661e.html</url>
      
        <content type="html"><![CDATA[<p>最近在leetcode刷题的时候，发现很多题推荐解法是用<strong>优先队列</strong>的特性，比如：<a href="https://leetcode.cn/problems/hua-dong-chuang-kou-de-zui-da-zhi-lcof/">滑动窗口的最大值</a> 、<a href="https://leetcode.cn/problems/chou-shu-lcof/">丑数</a></p><p>以前完全没有用个这个类，所以在此整理一下对优先队列的认识和刷题场景</p><h1 id="优先队列的特性"><a href="#优先队列的特性" class="headerlink" title="优先队列的特性"></a>优先队列的特性</h1><p>很明显，优先队列也是一种队列，只不过其出队顺序和一般队列不同，优先队列的出队顺序是按照一定的优先级来的，也就是说出队规则可以随意定制</p><p>优先队列ADT是一种数据结构，它支持插入、删除最小值操作（返回并删除最小元素）、删除最大值操作（返回并删除最大元素）</p><p>优先队列的主要操作：优先队列是元素的容器，每个元素有一个相关的键值</p><ul><li>insert(key, data)：插入键值为key的数据到优先队列中，元素以其key进行排序</li><li>deleteMin&#x2F;deleteMax：删除并返回最小&#x2F;最大键值的元素</li><li>getMinimum&#x2F;getMaximum：返回最小&#x2F;最大剑指的元素，但不删除它</li></ul><p>优先队列的辅助操作：</p><ul><li>第k最小&#x2F;第k最大：返回优先队列中键值为第k个最小&#x2F;最大的元素</li><li>大小（size）：返回优先队列中的元素个数</li><li>堆排序（Heap Sort）：基于键值的优先级将优先队列中的元素进行排序</li></ul><p>在某些场景下，比如要求队列中的最小元素先出即可用优先队列，在java中的实现类为<code>java.util.PriorityQueue</code>。</p><h1 id="认识下priorityqueue类的方法"><a href="#认识下PriorityQueue类的方法" class="headerlink" title="认识下PriorityQueue类的方法"></a>认识下PriorityQueue类的方法</h1><h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认情况下，优先级队列的头是队列中最小的元素，元素将按升序从队列中移除</span></span><br><span class="line">PriorityQueue&lt;Integer&gt; nums = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 借助 Comparator 接口自定义元素的顺序，头是队列中最大的元素，按降序从队列中移除</span></span><br><span class="line">PriorityQueue&lt;<span class="type">int</span>[]&gt; win = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;<span class="type">int</span>[]&gt;(<span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;<span class="type">int</span>[]&gt;() &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(<span class="type">int</span>[] a, <span class="type">int</span>[] b)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> a[<span class="number">0</span>] != b[<span class="number">0</span>] ? b[<span class="number">0</span>] - a[<span class="number">0</span>] : b[<span class="number">1</span>] - a[<span class="number">1</span>];</span><br><span class="line">        &#125;&#125;);</span><br></pre></td></tr></table></figure><h2 id="插入元素add-offer"><a href="#插入元素：add、offer" class="headerlink" title="插入元素：add、offer"></a>插入元素：add、offer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建优先队列</span></span><br><span class="line">PriorityQueue&lt;Integer&gt; numbers = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用add()方法，如果队列已满，则会引发异常</span></span><br><span class="line">numbers.add(<span class="number">4</span>);</span><br><span class="line">numbers.add(<span class="number">2</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;PriorityQueue: &quot;</span> + numbers);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用offer()方法，如果队列已满，则返回false</span></span><br><span class="line">numbers.offer(<span class="number">1</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;更新后的PriorityQueue: &quot;</span> + numbers);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 输出结果：</span></span><br><span class="line"><span class="comment"> * PriorityQueue: [2, 4]</span></span><br><span class="line"><span class="comment"> * 更新后的PriorityQueue: [1, 4, 2]</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 以上结果中，队列的头是最小元素</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="访问元素peek"><a href="#访问元素：peek" class="headerlink" title="访问元素：peek"></a>访问元素：peek</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用 peek() 方法</span></span><br><span class="line"><span class="type">int</span> <span class="variable">number</span> <span class="operator">=</span> nums.peek();</span><br><span class="line">System.out.println(<span class="string">&quot;访问元素: &quot;</span> + number);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 输出结果：</span></span><br><span class="line"><span class="comment"> * 访问元素: 1</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure><h2 id="删除元素remove-poll"><a href="#删除元素：remove、poll" class="headerlink" title="删除元素：remove、poll"></a>删除元素：remove、poll</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用remove()方法，从队列中删除指定的元素</span></span><br><span class="line"><span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> numbers.remove(<span class="number">2</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;元素2是否已删除? &quot;</span> + result);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用poll()方法，返回并删除队列的头</span></span><br><span class="line"><span class="type">int</span> <span class="variable">number</span> <span class="operator">=</span> numbers.poll();</span><br><span class="line">System.out.println(<span class="string">&quot;使用poll()删除的元素: &quot;</span> + number);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 输出结果：</span></span><br><span class="line"><span class="comment"> * 元素2是否已删除? true</span></span><br><span class="line"><span class="comment"> * 使用poll()删除的元素: 1</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure><h2 id="是否包含元素contains"><a href="#是否包含元素：contains" class="headerlink" title="是否包含元素：contains"></a>是否包含元素：contains</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用contains()方法，从队列中搜索指定的元素，找到则返回true，否则false。</span></span><br><span class="line"><span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> numbers.contains(<span class="number">4</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;队列中是否有元素 4 ？&quot;</span> + result);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 输出结果：</span></span><br><span class="line"><span class="comment"> * 队列中是否有元素 4 ？ true</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure><h1 id="刷题场景"><a href="#刷题场景" class="headerlink" title="刷题场景"></a>刷题场景</h1><h2 id="滑动窗口的最大值"><a href="#滑动窗口的最大值" class="headerlink" title="滑动窗口的最大值"></a><a href="https://leetcode.cn/problems/hua-dong-chuang-kou-de-zui-da-zhi-lcof/">滑动窗口的最大值</a></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//给定一个数组 nums 和滑动窗口的大小 k，请找出所有滑动窗口里的最大值。 </span></span><br><span class="line"><span class="comment">//示例: </span></span><br><span class="line"><span class="comment">//输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3</span></span><br><span class="line"><span class="comment">//输出: [3,3,5,5,6,7] </span></span><br><span class="line"><span class="comment">//解释:</span></span><br><span class="line"><span class="comment">//  滑动窗口的位置                最大值</span></span><br><span class="line"><span class="comment">//---------------               -----</span></span><br><span class="line"><span class="comment">//[1  3  -1] -3  5  3  6  7       3</span></span><br><span class="line"><span class="comment">// 1 [3  -1  -3] 5  3  6  7       3</span></span><br><span class="line"><span class="comment">// 1  3 [-1  -3  5] 3  6  7       5</span></span><br><span class="line"><span class="comment">// 1  3  -1 [-3  5  3] 6  7       5</span></span><br><span class="line"><span class="comment">// 1  3  -1  -3 [5  3  6] 7       6</span></span><br><span class="line"><span class="comment">// 1  3  -1  -3  5 [3  6  7]      7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.PriorityQueue;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 解题思路：利用优先队列的特性，规定堆顶元素就是窗口最大值</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span>[] maxSlidingWindow(<span class="type">int</span>[] nums, <span class="type">int</span> k) &#123;</span><br><span class="line"><span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> nums.length;</span><br><span class="line">PriorityQueue&lt;<span class="type">int</span>[]&gt; win = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;<span class="type">int</span>[]&gt;(<span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;<span class="type">int</span>[]&gt;() &#123;</span><br><span class="line"><span class="comment">// 重新定义出队规则</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(<span class="type">int</span>[] a, <span class="type">int</span>[] b)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> a[<span class="number">0</span>] != b[<span class="number">0</span>] ? b[<span class="number">0</span>] - a[<span class="number">0</span>] : b[<span class="number">1</span>] - a[<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 初始化窗口</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; k; ++i) &#123;</span><br><span class="line">win.offer(<span class="keyword">new</span> <span class="title class_">int</span>[]&#123;nums[i], i&#125;);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建指定长度的结果数组</span></span><br><span class="line"><span class="type">int</span>[] res = <span class="keyword">new</span> <span class="title class_">int</span>[len-k+<span class="number">1</span>];</span><br><span class="line"><span class="comment">// 第一个窗口的最大值</span></span><br><span class="line">res[<span class="number">0</span>] = win.peek()[<span class="number">0</span>];</span><br><span class="line"><span class="comment">// 遍历滑动窗口</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> k; i &lt; len; ++i) &#123;</span><br><span class="line"><span class="comment">// 添加新元素</span></span><br><span class="line">win.offer(<span class="keyword">new</span> <span class="title class_">int</span>[]&#123;nums[i], i&#125;);</span><br><span class="line"><span class="comment">// 删除窗口长度外的元素</span></span><br><span class="line"><span class="keyword">while</span> (win.peek()[<span class="number">1</span>] &lt;= i - k) &#123;</span><br><span class="line">win.poll();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 返回当前窗口的最大值</span></span><br><span class="line">res[i - k + <span class="number">1</span>] = win.peek()[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="丑数"><a href="#丑数" class="headerlink" title="丑数"></a><a href="https://leetcode.cn/problems/chou-shu-lcof/">丑数</a></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 我们把只包含质因子 2、3 和 5 的数称作丑数（Ugly Number）。求按从小到大的顺序的第 n 个丑数。</span></span><br><span class="line"><span class="comment">// 示例: </span></span><br><span class="line"><span class="comment">// 输入: n = 10</span></span><br><span class="line"><span class="comment">// 输出: 12</span></span><br><span class="line"><span class="comment">// 解释: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 是前 10 个丑数。 </span></span><br><span class="line"><span class="comment">// 说明:</span></span><br><span class="line"><span class="comment">// 1 是丑数。 </span></span><br><span class="line"><span class="comment">// n 不超过1690。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.PriorityQueue;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 解题思路：最小堆，需借助java的PriorityQueue类的特性实现：https://www.cainiaojc.com/java/java-priorityqueue.html</span></span><br><span class="line"><span class="comment"> * 初始化堆，将最小丑数1放入堆</span></span><br><span class="line"><span class="comment"> * 每次取出堆顶元素x，x元素也是堆中最小的丑数，需排除重复元素，依次将 2x,3x,5x 加入堆</span></span><br><span class="line"><span class="comment"> * 第n次取出的堆顶元素就是第n个丑数</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">nthUglyNumber</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line"><span class="type">int</span>[] factors = &#123;<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>&#125;;</span><br><span class="line">PriorityQueue&lt;Long&gt; heap = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;Long&gt;();   <span class="comment">//优先级队列的头是队列中最小的元素</span></span><br><span class="line">heap.offer(<span class="number">1L</span>);     <span class="comment">// 初始化最小堆，放入最小丑数1</span></span><br><span class="line"><span class="type">int</span> <span class="variable">ugly</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line"><span class="type">long</span> <span class="variable">cur</span> <span class="operator">=</span> heap.poll();     <span class="comment">//返回并删除队列的头，即最小元素</span></span><br><span class="line">ugly = (<span class="type">int</span>) cur;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> factor : factors)&#123;</span><br><span class="line"><span class="type">long</span> <span class="variable">next</span> <span class="operator">=</span> cur*factor;</span><br><span class="line"><span class="comment">//检查是否有重复元素</span></span><br><span class="line"><span class="keyword">if</span>(!heap.contains(next))&#123;</span><br><span class="line">heap.offer(next);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ugly;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="数据流中的中位数"><a href="#数据流中的中位数" class="headerlink" title="数据流中的中位数"></a><a href="https://leetcode.cn/problems/shu-ju-liu-zhong-de-zhong-wei-shu-lcof/solution/shu-ju-liu-zhong-de-zhong-wei-shu-by-lee-um4f/">数据流中的中位数</a></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 解题思路：优先队列</span></span><br><span class="line"><span class="comment">* 利用优先队列的特性实现，队头是最大值</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MedianFinder</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化两个优先队列，分别存放 小于等于 和 大于 中位数的数值</span></span><br><span class="line">    PriorityQueue&lt;Integer&gt; queueMin;</span><br><span class="line">    PriorityQueue&lt;Integer&gt; queueMax;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** initialize your data structure here. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">MedianFinder</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// queueMin队头为队列最大值，queueMax对头为队列最小值</span></span><br><span class="line">        queueMin = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;Integer&gt;((a,b) -&gt;(b-a));</span><br><span class="line">        queueMax = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;Integer&gt;((a,b) -&gt;(a-b));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addNum</span><span class="params">(<span class="type">int</span> num)</span> &#123;</span><br><span class="line">        <span class="comment">// num小于等于中位数，则num放入queueMin队列；num大于中位数，则num放入queueMax队列</span></span><br><span class="line">        <span class="comment">// 注意if条件语句的先后顺序很重要</span></span><br><span class="line">        <span class="keyword">if</span> (queueMin.isEmpty() || num &lt;= queueMin.peek()) &#123;</span><br><span class="line">            queueMin.add(num);</span><br><span class="line">            <span class="comment">// queueMin队列大小超出，则将max(queueMin)元素放入queueMax队列</span></span><br><span class="line">            <span class="keyword">if</span> (queueMin.size() &gt; queueMax.size() + <span class="number">1</span>) &#123;</span><br><span class="line">                queueMax.add(queueMin.poll());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            queueMax.add(num);</span><br><span class="line">            <span class="comment">// queueMax队列大小超出，则将min(queueMin)元素放入queueMin队列</span></span><br><span class="line">            <span class="keyword">if</span> (queueMax.size() &gt; queueMin.size()) &#123;</span><br><span class="line">                queueMin.add(queueMax.poll());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="type">double</span> <span class="title function_">findMedian</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 从数据流中读出奇数个数值</span></span><br><span class="line">        <span class="keyword">if</span> (queueMin.size() &gt; queueMax.size()) &#123;</span><br><span class="line">            <span class="keyword">return</span> queueMin.peek();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 从数据流中读出偶数个数值</span></span><br><span class="line">        <span class="keyword">return</span> (queueMin.peek() + queueMax.peek()) / <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Your MedianFinder object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"> * MedianFinder obj = new MedianFinder();</span></span><br><span class="line"><span class="comment"> * obj.addNum(num);</span></span><br><span class="line"><span class="comment"> * double param_2 = obj.findMedian();</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://zhuanlan.zhihu.com/p/39615266">数据结构与算法(4)——优先队列和堆</a> </p><p><a href="https://www.cainiaojc.com/java/java-priorityqueue.html">Java PriorityQueue</a></p>]]></content>
      
      
      <categories>
          
          <category> 题集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive、Spark和Maxcompute的SQL语法对比分析</title>
      <link href="/posts/e6b1209.html"/>
      <url>/posts/e6b1209.html</url>
      
        <content type="html"><![CDATA[<h1 id="having-差异"><a href="#having-差异" class="headerlink" title="having 差异"></a>having 差异</h1><h4 id="差异点"><a href="#差异点" class="headerlink" title="差异点"></a>差异点</h4><p>hive和spark支持窗口函数后带having<br>maxcomputer 的having语法只支持 在 group 和 distinct 后使用</p><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> order_id</span><br><span class="line">,<span class="built_in">sum</span>(trd_amt) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> province) <span class="keyword">as</span> trd_amt_std</span><br><span class="line"><span class="keyword">from</span> <span class="keyword">order</span></span><br><span class="line"><span class="keyword">having</span> trd_amt_std<span class="operator">&gt;</span><span class="number">0</span></span><br></pre></td></tr></table></figure><p>以上sql在hive中可以运行，但是在maxcomputer中会提示错误，错误如下：<br><img src="https://i.328888.xyz/2023/02/21/gtHTp.png" alt="gtHTp.png"></p><h4 id="替换方案"><a href="#替换方案" class="headerlink" title="替换方案"></a>替换方案</h4><p>在语句中使用子查询，将having替换为where</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> order_id</span><br><span class="line">,<span class="built_in">sum</span>(trd_amt) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> province) <span class="keyword">as</span> trd_amt_std</span><br><span class="line"><span class="keyword">from</span> <span class="keyword">order</span></span><br><span class="line">) a</span><br><span class="line"><span class="keyword">where</span> a.trd_amt_std<span class="operator">&gt;</span><span class="number">0</span></span><br></pre></td></tr></table></figure><h1 id="maxcomputer-cross-join-超过一定条数后依然会提示笛卡尔积风险"><a href="#maxcomputer-cross-join-超过一定条数后，依然会提示笛卡尔积风险" class="headerlink" title="maxcomputer - cross join 超过一定条数后，依然会提示笛卡尔积风险"></a>maxcomputer - cross join 超过一定条数后，依然会提示笛卡尔积风险</h1><h4 id="差异点"><a href="#差异点-1" class="headerlink" title="差异点"></a>差异点</h4><p>hive可以使用 cross join语法来表示笛卡尔积关联<br>maxcomputer 的cross join，在条数超过一定数据量后，会提示笛卡尔积风险</p><h4 id="举例"><a href="#举例-1" class="headerlink" title="举例"></a>举例</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span>,b.<span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_a</span><br><span class="line">) a</span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_b</span><br><span class="line">) b</span><br></pre></td></tr></table></figure><p>以上sql在hive中可以运行，但是在maxcomputer中会提示错误，错误如下：<br><img src="https://i.328888.xyz/2023/02/22/xfOrA.png" alt="xfOrA.png"></p><h4 id="替换方案"><a href="#替换方案-1" class="headerlink" title="替换方案"></a>替换方案</h4><p>在左右笛卡尔积表中新增常量字段，用于关联</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span>,b.<span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="operator">*</span>,<span class="number">1</span> <span class="keyword">as</span> cro_col <span class="keyword">from</span> table_a</span><br><span class="line">) a</span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> <span class="operator">*</span>,<span class="number">1</span> <span class="keyword">as</span> cro_col <span class="keyword">from</span> table_b</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">on</span> a.cro_col<span class="operator">=</span>b.cro_col</span><br></pre></td></tr></table></figure><h1 id="不等值join-差异"><a href="#不等值join-差异" class="headerlink" title="不等值join 差异"></a>不等值join 差异</h1><h4 id="差异点"><a href="#差异点-2" class="headerlink" title="差异点"></a>差异点</h4><p>1、spark 支持不等值join语法<br>2、hive 2.2.0版本之前不支持不等值语法，<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins">2.2.0及以后支持不等值join语法</a><br><img src="https://i.328888.xyz/2023/02/21/gtrN5.png" alt="gtrN5.png"><br>3、<a href="https://help.aliyun.com/document_detail/73783.html">maxcomputer不支持不等值语法</a><br><img src="https://i.328888.xyz/2023/02/21/gt3h8.png" alt="gt3h8.png"></p><h4 id="举例"><a href="#举例-2" class="headerlink" title="举例"></a>举例</h4><p>测试sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> table_a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id_a</span><br><span class="line">,<span class="string">&#x27;testa&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">select</span> <span class="number">4</span> <span class="keyword">as</span> id_a</span><br><span class="line">    ,<span class="string">&#x27;testd&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line">)</span><br><span class="line">,table_b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">3</span> <span class="keyword">as</span> id_b</span><br><span class="line">,<span class="string">&#x27;testc&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">select</span> <span class="number">2</span> <span class="keyword">as</span> id_b</span><br><span class="line">    ,<span class="string">&#x27;testb&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> table_a.id_a</span><br><span class="line">,table_a.value_a</span><br><span class="line">,table_b.id_b</span><br><span class="line">,table_b.value_b</span><br><span class="line"><span class="keyword">from</span>  table_a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> table_b</span><br><span class="line"><span class="keyword">on</span> table_a.id_a <span class="operator">&lt;</span> table_b.id_b</span><br></pre></td></tr></table></figure><p>sql说明 :该sql准备了两张表table_a和table_b用于连接测试<br>使用left join on语法，但是关联关系使用的是 &lt; 不等值关联符号</p><p><strong>maxcomputer运行结果</strong><br><img src="https://i.328888.xyz/2023/02/22/xfcBz.png" alt="xfcBz.png"><br>maxcomputer会报异常：  FAILED: ODPS-0130071:[15,4] Semantic analysis exception - expect equality expression (i.e., only use ‘&#x3D;’ and ‘AND’) for join condition without mapjoin hint</p><p>提示的是期望join的是等值表达式</p><p><strong>hive1.2.1运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gtmHF.png" alt="gtmHF.png"></p><p>hive会报错： Error while compiling statement: FAILED: SemanticException [Error 10017]: line 15:3 Both left and right aliases encountered in JOIN ‘id_b’</p><p>提示的是在join中遇到左右别名</p><p>不得不说，hive的错误信息有点云里雾里，其实就是不等值join造成的。</p><p><strong>hive2.2.3运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gtpeH.png" alt="gtpeH.png"></p><p>hive 2.2.0+版本顺利得到正确结果</p><p><strong>spark运行结果</strong></p><p><img src="https://i.328888.xyz/2023/02/22/xfGXy.png" alt="xfGXy.png"> </p><p>spark2.3也顺利得到结果</p><h4 id="替换方案"><a href="#替换方案-2" class="headerlink" title="替换方案"></a>替换方案</h4><p>针对不等值join的替换方案有两种</p><p>1、针对小表，使用mapjoin，避免join操作</p><p>2、将on的不等值关联语句放入where语句中</p><p>由于mapjoin避免shuffle，性能较好，再可以的情况下，优先使用方案1</p><p><strong>1、针对小表，使用mapjoin，避免join操作</strong><br>maxcomputer中的mapjoin hint语法为： &#x2F;*+ mapjoin(<table_name>) *&#x2F; ，详情请查看<a href="https://help.aliyun.com/document_detail/73785.htm?spm=a2c4g.11186623.0.0.534268c4fHc5iD#concept-bf5-tkb-wdb">mapjoin hint</a></table_name></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> table_a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id_a</span><br><span class="line">,<span class="string">&#x27;testa&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line">)</span><br><span class="line">,table_b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">2</span> <span class="keyword">as</span> id_b</span><br><span class="line">,<span class="string">&#x27;testb&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(table_b) */</span></span><br><span class="line">table_a.id_a</span><br><span class="line">,table_a.value_a</span><br><span class="line">,table_b.id_b</span><br><span class="line">,table_b.value_b</span><br><span class="line"><span class="keyword">from</span>  table_a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> table_b</span><br><span class="line"><span class="keyword">on</span> table_a.id_a<span class="operator">&lt;</span>table_b.id_b</span><br></pre></td></tr></table></figure><p>可以看到，使用mapjoin hint语法后，sql在maxcomputer中运行正确，顺利拿到了预期结果<br><img src="https://i.328888.xyz/2023/02/21/gt79P.png" alt="gt79P.png"></p><p><strong>2、将on的不等值关联语句放入where语句中</strong><br>inner join 比较简单</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> table_a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id_a</span><br><span class="line">,<span class="string">&#x27;testa&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line">,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">select</span> <span class="number">4</span> <span class="keyword">as</span> id_a</span><br><span class="line">    ,<span class="string">&#x27;testd&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line">    ,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line">)</span><br><span class="line">,table_b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">2</span> <span class="keyword">as</span> id_b</span><br><span class="line">,<span class="string">&#x27;testb&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line">,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">select</span> <span class="number">3</span> <span class="keyword">as</span> id_b</span><br><span class="line">    ,<span class="string">&#x27;testc&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line">    ,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">table_a.id_a</span><br><span class="line">,table_a.value_a</span><br><span class="line">,table_b.id_b</span><br><span class="line">,table_b.value_b</span><br><span class="line"><span class="keyword">from</span>  table_a</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> table_b</span><br><span class="line"><span class="keyword">on</span> table_a.join_col<span class="operator">=</span>table_b.join_col</span><br><span class="line"><span class="keyword">where</span> table_a.id_a<span class="operator">&lt;</span>table_b.id_b</span><br></pre></td></tr></table></figure><p>可以看到，将&lt;判断语句放入where后，sql在maxcomputer运行正确，顺利拿到了预期结果<br><img src="https://i.328888.xyz/2023/02/21/gteAX.png" alt="gteAX.png"></p><p>left join 比较复杂，建议使用map hint，实在没办法在使用此方案</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> table_a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">as</span> id_a</span><br><span class="line">,<span class="string">&#x27;testa&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line">,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">select</span> <span class="number">4</span> <span class="keyword">as</span> id_a</span><br><span class="line">    ,<span class="string">&#x27;testd&#x27;</span> <span class="keyword">as</span> value_a</span><br><span class="line">    ,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line">)</span><br><span class="line">,table_b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> <span class="number">2</span> <span class="keyword">as</span> id_b</span><br><span class="line">,<span class="string">&#x27;testb&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line">,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">select</span> <span class="number">3</span> <span class="keyword">as</span> id_b</span><br><span class="line">    ,<span class="string">&#x27;testc&#x27;</span> <span class="keyword">as</span> value_b</span><br><span class="line">    ,<span class="number">1</span> <span class="keyword">as</span> join_col</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 能关联上的部分</span></span><br><span class="line">,join_part <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">table_a.id_a</span><br><span class="line">,table_a.value_a</span><br><span class="line">,table_b.id_b</span><br><span class="line">,table_b.value_b</span><br><span class="line"><span class="keyword">from</span>  table_a</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> table_b</span><br><span class="line"><span class="keyword">on</span> table_a.join_col<span class="operator">=</span>table_b.join_col</span><br><span class="line"><span class="keyword">where</span> table_a.id_a<span class="operator">&lt;</span>table_b.id_b</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 以自己为主表，left join能关联上的部分，实现 left join不等值效果</span></span><br><span class="line"><span class="keyword">select</span> table_a.id_a</span><br><span class="line">,table_a.value_a</span><br><span class="line">,join_part.id_b</span><br><span class="line">,join_part.value_b</span><br><span class="line"><span class="keyword">from</span>  table_a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> join_part</span><br><span class="line"><span class="keyword">on</span> table_a.id_a<span class="operator">=</span>join_part.id_a</span><br></pre></td></tr></table></figure><p>可以看到，将&lt;判断语句放入where后，sql在maxcomputer运行正确，顺利拿到了预期结果<br><img src="https://i.328888.xyz/2023/02/21/gt6hJ.png" alt="gt6hJ.png"></p><h1 id="array_contains-差异"><a href="#array-contains-差异" class="headerlink" title="array_contains 差异"></a>array_contains 差异</h1><h4 id="差异点"><a href="#差异点-3" class="headerlink" title="差异点"></a>差异点</h4><p>spark的array_contains支持类型的隐式转换<br>hive和maxcomputer array_contains不支持，只支持同类型使用</p><h4 id="举例"><a href="#举例-3" class="headerlink" title="举例"></a>举例</h4><p>测试sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> array_contains(split(&quot;1,2,3,4&quot;,&quot;,&quot;),<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>sql说明</strong><br>该sql首先使用split一个字符串获取一个array对象用于测试，之后使用array_contains函数进行判断<br>split后的array对象为一个string数组，而判断被包含的数字【1】为一个int 对象</p><p><strong>maxcomputer运行结果</strong><br><img src="https://i.328888.xyz/2023/02/22/xf5D8.png" alt="xf5D8.png"><br>maxcomputer会报异常：  FAILED: ODPS-0130071:[1,44] Semantic analysis exception - invalid type INT of argument 2 for function array_contains, expect STRING, implicit conversion is not allowed</p><p>提示的是array_contains第二个参数期望的是string，但是传入的是int，隐式类型转换不支持</p><p><strong>hive运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gtdOA.png" alt="gtdOA.png"></p><p>hive会报错： Error while compiling statement: FAILED: SemanticException [Error 10016]: line 1:43 Argument type mismatch ‘1’: “string” expected at function ARRAY_CONTAINS, but “int” is found</p><p>提示的是array_contains函数期望的是string，但是传入的是int，类型不匹配</p><p><strong>spark运行结果</strong></p><p><img src="https://i.328888.xyz/2023/02/22/xf1DJ.png" alt="xf1DJ.png"></p><p>spark能顺利产出结果，结果为true，那么为什么spark可以成功呢？</p><p>大概率是spark智能的将1从int转换为了string类型，使得类型得以匹配，通过explain查看物理执行计划来验证</p><p><img src="https://i.328888.xyz/2023/02/21/gtJAq.png" alt="gtJAq.png"></p><p>在上图标红的地方可以看到，spark在物理执行计划层面，将int的1隐式的转换为了string类型，验证了我们一开始的猜想。</p><h4 id="替换方案"><a href="#替换方案-3" class="headerlink" title="替换方案"></a>替换方案</h4><p>既然知道了在hive和maxcomputer中是类型不匹配导致的array_contains函数报错，那么只需要显示的将类型进行转换即可</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> array_contains(split(&quot;1,2,3,4&quot;,&quot;,&quot;),<span class="built_in">cast</span>(<span class="number">1</span> <span class="keyword">as</span> string))</span><br></pre></td></tr></table></figure><h1 id="字段类型转换-arrayltgt-to-string"><a href="#字段类型转换-ARRAY-lt-gt-to-STRING" class="headerlink" title="字段类型转换 ARRAY&lt;&gt; to STRING"></a>字段类型转换 ARRAY&lt;&gt; to STRING</h1><h4 id="差异点"><a href="#差异点-4" class="headerlink" title="差异点"></a>差异点</h4><p>spark的array_contains支持类型的隐式转换</p><p>hive和maxcomputer array_contains不支持，只支持同类型使用</p><h4 id="举例"><a href="#举例-4" class="headerlink" title="举例"></a>举例</h4><p>测试sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">cast</span>(<span class="keyword">array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="keyword">as</span> string) <span class="keyword">as</span> array_to_string;</span><br></pre></td></tr></table></figure><p><strong>maxcompute运行结果</strong></p><p><img src="https://i.328888.xyz/2023/02/22/xfQ3c.png" alt="xfQ3c.png"></p><p>maxcompute报异常：FAILED: ODPS-0130141:[1,8] Illegal implicit type cast - cannot cast from ARRAY<int> to STRING</int></p><p>提示的是 ARRAY&lt;&gt;类型字段  不能强制转换为 STRING 类型</p><p><strong>hive运行结果</strong></p><p><img src="https://i.328888.xyz/2023/02/22/xyi6V.png" alt="xyi6V.png"></p><p>hive报异常：SQL语义错误: Error while compiling statement: FAILED: ClassCastException org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo cannot be cast to org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo</p><p>提示的是不同类型不能强转</p><p><strong>spark运行结果</strong></p><p><img src="https://i.328888.xyz/2023/02/21/gtPZa.png" alt="gtPZa.png"></p><p>spark能顺利产出结果</p><p><strong>替换方案</strong><br>使用 <a href="https://help.aliyun.com/document_detail/293597.htm?spm=a2c4g.11186623.0.0.5be36f60Wo6eDb#section-pc4-90e-0rl">array_join函数</a> 将array的元素拼接成字符串，再在首尾加上 ‘[ ‘ 和 ‘]’ 字符可以还原spark上的运行结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> concat(<span class="string">&#x27;[&#x27;</span>,array_join(<span class="keyword">array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>),<span class="string">&#x27;,&#x27;</span>),<span class="string">&#x27;]&#x27;</span>) <span class="keyword">as</span> array_to_string;</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/02/21/gtTOx.png" alt="gtTOx.png"></p><p>📢注意<br>macxcompute的array_join函数默认会忽略null元素，可在array_join函数中设置 nullreplacement 参数替代NULL元素<br><img src="https://i.328888.xyz/2023/02/21/gtazk.png" alt="gtazk.png"><br><img src="https://i.328888.xyz/2023/02/21/gt1aL.png" alt="gt1aL.png"></p><h1 id="日期格式to_datexxxyyyymmddhhmmss"><a href="#日期格式to-date-‘xxx’-’yyyyMMddHHmmss’" class="headerlink" title="日期格式to_date(‘xxx’,’yyyyMMddHHmmss’)"></a>日期格式to_date(‘xxx’,’yyyyMMddHHmmss’)</h1><h4 id="差异点"><a href="#差异点-5" class="headerlink" title="差异点"></a>差异点</h4><p>hive语法中，to_date函数用法为：to_date(string timestamp)，返回DATE类型，格式为 yyyy-mm-dd ，仅有一个参数，支持用format格式解析</p><p>spark语法中，to_date函数用法为：to_date(date_str[, fmt]) ，返回DATE类型，格式为 yyyy-mm-dd ，支持用format格式解析日期</p><p>maxcompute语法中，to_date函数用法为：to_date(string <date>, string <format>)，返回DATETIME类型，格式为 yyyy-mm-dd hh:mi:ss ，支持用format格式解析日期</format></date></p><p>📢这里要注意的是，虽然spark和maxcompute中，to_date函数都支持用format格式解析日期，format格式是有差异的，主要表现在 分钟 位的格式</p><p>spark的format格式：yyyy为4位数的年，MM为2位数的月，dd为2位数的日，HH为24小时制的时，mm为2位数的分钟，ss为2位数的秒，ff3为3位精度毫秒<br>maxcompute的format格式：yyyy为4位数的年，mm为2位数的月，dd为2位数的日，hh为24小时制的时，mi为2位数的分钟，ss为2位数的秒，ff3为3位精度毫秒</p><h4 id="举例"><a href="#举例-5" class="headerlink" title="举例"></a>举例</h4><p>测试sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> to_date(<span class="string">&#x27;20221118123456&#x27;</span>,<span class="string">&#x27;yyyyMMddHHmmss&#x27;</span>),to_date(<span class="string">&#x27;2022-11-18 12:34:56&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd HH:mm:ss&#x27;</span>);</span><br></pre></td></tr></table></figure><p><strong>maxcompute运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gtY0p.png" alt="gtY0p.png"></p><p>maxcompute报异常： FAILED: ODPS-0121095:Invalid arguments - format string has second part, but doesn’t have minute part : yyyyMMddHHmmss</p><p><strong>hive运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gtqXU.png" alt="gtqXU.png"></p><p>hive报异常： Arguments length mismatch ‘’yyyyMMddhhmmss’’: to_date() requires 1 argument, got 2</p><p>提示的是to_date函数仅有1个参数</p><p>去掉format参数后的运行结果为：<br><img src="https://i.328888.xyz/2023/02/21/gtuJv.png" alt="gtuJv.png"></p><p>从结果可以看到，to_date不能解析 yyyyMMddhhmmss 和 yyyyMMdd 格式</p><p><strong>spark运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWiD3.png" alt="gWiD3.png"></p><p>spark能顺利产出结果</p><h4 id="替换方案"><a href="#替换方案-4" class="headerlink" title="替换方案"></a>替换方案</h4><p>format格式修改：yyyy为4位数的年，mm为2位数的月，dd为2位数的日，hh为24小时制的时，mi为2位数的分钟，ss为2位数的秒，ff3为3位精度毫秒</p><p>修改后的能正常产出结果：<br><img src="https://i.328888.xyz/2023/02/21/gW4i8.png" alt="gW4i8.png"></p><p>另，常见使用to_date报错sql为 date_format(date_add(to_date(pay_time,’yyyyMMddHHmmss’),2),’yyyyMMddHHmmss’) ，解读sql的作用是对 pay_time 加 2 天，建议用 UDF 修改这段sql为 yt_date_add(pay_time,2)，修改后简洁明了</p><h1 id="date日期函数"><a href="#date日期函数" class="headerlink" title="date日期函数"></a>date日期函数</h1><h4 id="差异点"><a href="#差异点-6" class="headerlink" title="差异点"></a>差异点</h4><p>spark和hive的date函数支持将标准的日期string转换为date类型</p><p>maxcomputer date函数只支持标准的日期string，带时分秒的时间string不支持</p><h4 id="举例"><a href="#举例-6" class="headerlink" title="举例"></a>举例</h4><p>测试sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="type">date</span>(<span class="string">&#x27;2022-12-21&#x27;</span>),<span class="type">date</span>(<span class="string">&#x27;2022-12-21 01:22:01&#x27;</span>);</span><br></pre></td></tr></table></figure><p><strong>maxcompute运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWt0Q.png" alt="gWt0Q.png"></p><p>maxcomputer对标准的日期string【2022-12-21】转换正确</p><p>但是对带时分秒的string转为错误，直接为null</p><p><strong>hive运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWWXE.png" alt="gWWXE.png"></p><p>结果符合预期</p><p><strong>spark运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWkKC.png" alt="gWkKC.png"></p><p>spark能顺利产出结果</p><h4 id="替换方案"><a href="#替换方案-5" class="headerlink" title="替换方案"></a>替换方案</h4><p>如果是为了格式转换，使用自定义 yt_date_format 函数</p><p>如果是为了获取date类型，使用 to_date函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> yt_date_format(<span class="string">&#x27;2022-12-21 01:22:01&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd&#x27;</span>)</span><br><span class="line">,to_date(<span class="string">&#x27;2022-12-21 01:22:01&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/02/21/gWCDP.png" alt="gWCDP.png"></p><h1 id="from_unixtime函数"><a href="#from-unixtime函数" class="headerlink" title="from_unixtime函数"></a>from_unixtime函数</h1><h4 id="差异点"><a href="#差异点-7" class="headerlink" title="差异点"></a>差异点</h4><p>spark和hive的from_unixtime函数将时间戳转换成格式化string类型，当时间戳为负数时，正常转换</p><p>maxcomputer from_unixtime函数转换负数时间戳时，存在时间便宜</p><h4 id="举例"><a href="#举例-7" class="headerlink" title="举例"></a>举例</h4><p>测试sql<br>select ‘1018-10-15 00:00:00’  – yyyyMMddHHmmss 时间戳<br>,unix_timestamp(‘1018-10-15 00:00:00’) –时间戳<br>,from_unixtime(unix_timestamp(‘1018-10-15 00:00:00’),’yyyyMMddHHmmss’) –转换格式</p><p><strong>maxcompute运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWHiJ.png" alt="gWHiJ.png"></p><p>可以看到，原先日期为 ‘1018-10-15 00:00:00’,转换成yyyyMMddHHmmss格式原本期望为  10181015000000</p><p>但是实际结果为10181008235417,和预期不符合</p><p><strong>hive运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWObc.png" alt="gWObc.png"></p><p>hive结果符合预期</p><p><strong>spark运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWb6A.png" alt="gWb6A.png"></p><p>spark产出结果正确</p><h4 id="替换方案"><a href="#替换方案-6" class="headerlink" title="替换方案"></a>替换方案</h4><p>使用自定义 yt_date_format 函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="string">&#x27;1018-10-15 00:00:00&#x27;</span> <span class="comment">-- yyyyMMddHHmmss 时间戳</span></span><br><span class="line">,unix_timestamp(<span class="string">&#x27;1018-10-15 00:00:00&#x27;</span>) <span class="comment">--时间戳</span></span><br><span class="line">,yt_date_format(<span class="string">&#x27;1018-10-15 00:00:00&#x27;</span>,<span class="string">&#x27;yyyyMMddHHmmss&#x27;</span>) <span class="comment">--转换格式</span></span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/02/21/gWICN.png" alt="gWICN.png"><br>使用自定义udf后正确</p><h1 id="concat_ws差异"><a href="#concat-ws差异" class="headerlink" title="concat_ws差异"></a>concat_ws差异</h1><h4 id="差异点"><a href="#差异点-8" class="headerlink" title="差异点"></a>差异点</h4><p>spark的concat_ws会支持类型的隐式转换</p><p>hive和maxcomputer concat_ws不支持，只支持同类型使用</p><h4 id="举例"><a href="#举例-8" class="headerlink" title="举例"></a>举例</h4><p>测试sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> concat_ws(&quot;,&quot;,<span class="keyword">array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><strong>maxcompute运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWNgV.png" alt="gWNgV.png"></p><p>报错提示数据类型不对，concat_ws只能处理ARRAY<string>数据类型，而sql中是ARRAY<int>数据类型，<a href="https://help.aliyun.com/document_detail/455513.html?spm=a2c4g.11186623.0.0.28686fd3N35cvE">官方文档</a> 中有详细说明<br><img src="https://i.328888.xyz/2023/02/21/gWmSz.png" alt="gWmSz.png"></int></string></p><p><strong>hive运行结果</strong><br><img src="https://i.328888.xyz/2023/02/21/gWBVw.png" alt="gWBVw.png"></p><p>报错提示数据类型不对，与maxcompute一个意思，concat_ws传入数组必须是Array<int>类型</int></p><p><strong>spark运行结果</strong></p><p><img src="https://i.328888.xyz/2023/02/21/gWXba.png" alt="gWXba.png"></p><p>spark执行结果符合预期</p><h4 id="替换方案"><a href="#替换方案-7" class="headerlink" title="替换方案"></a>替换方案</h4><p>使用阿里云提供的array_join函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> array_join(<span class="keyword">array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),&quot;,&quot;);</span><br></pre></td></tr></table></figure><p><img src="https://i.328888.xyz/2023/02/21/gWegp.png" alt="gWegp.png"></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Maxcompute </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql和hiveSQL的语法差别</title>
      <link href="/posts/e912f2da.html"/>
      <url>/posts/e912f2da.html</url>
      
        <content type="html"><![CDATA[<p>最近在牛客网上刷sql题，但编程语言居然只支持mysql，一些函数用法上与平时工作使用的hiveSQL有较大差别，所以在这篇博客中整理一下两种语法的函数使用差异</p><p><a href="https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html">mysql内置函数</a></p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF?spm=a2c4g.11186623.0.0.3c267254Ka3fUh#LanguageManualUDF-get_json_object">hive内置函数</a></p><h1 id="日期-时间函数"><a href="#日期、时间函数" class="headerlink" title="日期、时间函数"></a>日期、时间函数</h1><table><thead><tr><th>函数用途</th><th>mysql函数</th><th>mysql用法</th><th>hive函数</th><th>hiveSQL用法</th></tr></thead><tbody><tr><td>日期、时间格式化</td><td>date_format</td><td>date_format(‘2008-08-08 22:23:01’, ‘%Y%m%d%H%i%s’)</td><td>date_format</td><td>date_format(‘2008-08-08 22:23:01’, ‘yyyyMMddHHmmss’)</td></tr><tr><td>日期、时间加</td><td>date_add</td><td>date_add(‘2008-08-08 22:23:01’,interval 1 day&#x2F;hour&#x2F;minute&#x2F;second&#x2F;microsecond&#x2F;week&#x2F;month&#x2F;quarter&#x2F;year)，返回dateTime格式</td><td>date_add</td><td>date_add(‘2008-08-08 22:23:01’,1)，只加days，返回date格式</td></tr><tr><td>日期、时间减</td><td>date_sub</td><td>date_sub(‘2008-08-08 22:23:01’,interval 1 day&#x2F;hour&#x2F;minute&#x2F;second&#x2F;microsecond&#x2F;week&#x2F;month&#x2F;quarter&#x2F;year)，返回dateTime格式</td><td>date_sub</td><td>date_sub(‘2008-08-08 22:23:01’,1)，只加days，返回date格式</td></tr><tr><td>日期相差</td><td>datediff</td><td>datediff(‘2008-08-08 22:22:00’,’2008-08-07 22:23:00’)</td><td>datediff</td><td>datediff(‘2008-08-08 22:22:00’,’2008-08-07 22:23:00’)</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>排序算法</title>
      <link href="/posts/735e5788.html"/>
      <url>/posts/735e5788.html</url>
      
        <content type="html"><![CDATA[<p>整理一些数据结构中常用的排序算法原理和java实现</p><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>在数组中找到一个基准值<code>t</code>，将小于<code>t</code>的值放它前面，大于<code>t</code>的值放它后面，再以此方法对子数组递归进行快速排序</p><h2 id="java代码"><a href="#java代码" class="headerlink" title="java代码"></a>java代码</h2>]]></content>
      
      
      <categories>
          
          <category> 题集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>解题思路之动态规划</title>
      <link href="/posts/d6cdfd6a.html"/>
      <url>/posts/d6cdfd6a.html</url>
      
        <content type="html"><![CDATA[<h1 id="什么是动态规划"><a href="#什么是动态规划" class="headerlink" title="什么是动态规划"></a>什么是动态规划</h1><p>动态规划，英文：Dynamic Programming，简称DP。<br>简单理解，动态规划的每一个状态都能由上一个状态推导而来</p><h1 id="解题步骤"><a href="#解题步骤" class="headerlink" title="解题步骤"></a>解题步骤</h1><p>以斐波那契数列为例，动态规划问题可以拆解为五步曲：</p><p>1、确定dp数组和下标含义：第n个斐波那契数是<code>dp[n]</code></p><p>2、确定递推公式（也可叫状态转移方程）：<code>dp[n] = dp[n-1] + dp[n-2]</code></p><p>3、dp数组初始化：<code>dp[0] = 0; dp[1] = 1</code></p><p>4、确定遍历顺序：从前到后遍历，<code>dp[n]</code>依赖<code>dp[n-1]</code>和<code>dp[n-2]</code></p><p>5、举例推导dp数组：当<code>n=10</code>时，dp数组应该为：<code>0 1 1 2 3 5 8 13 21 34 55</code></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://programmercarl.com/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92">代码随想录之动态规划</a></p>]]></content>
      
      
      <categories>
          
          <category> 题集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL优化之数据倾斜</title>
      <link href="/posts/faab1ad7.html"/>
      <url>/posts/faab1ad7.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在Spark作业优化场景中，最常见且比较棘手的就是数据倾斜问题。个人认为，具备数据倾斜调优能力对从事数仓开发人员是必备的基本要求。当然，数据倾斜的场景是比较复杂的，针对不同的数据倾斜有不同的处理方案。</p><h1 id="如何辨别和定位数据倾斜"><a href="#如何辨别和定位数据倾斜" class="headerlink" title="如何辨别和定位数据倾斜"></a>如何辨别和定位数据倾斜</h1><p>从Spark作业的执行计划看，若出现某个task任务比其他task任务执行耗时极其久，比如：某个stage有100个task，其中99个task在1min左右就执行成功，但是有1个task却执行了1个小时甚至更久，这种情况显然是出现了数据倾斜。</p><p>数据倾斜问题仅出现在shuffle过程，一些会触发shuffle的算子：distinct、groupByKey、reduceByKey、aggregateByKey、countByKey、join、cogroup、repartition等。<br>对应提交的SparkSQL中可能有distinct、count(distinct)、group by、partition by、join等关键词。</p><h1 id="常见的数据倾斜场景及解决方案"><a href="#常见的数据倾斜场景及解决方案" class="headerlink" title="常见的数据倾斜场景及解决方案"></a>常见的数据倾斜场景及解决方案</h1><h1 id="碰到的数据倾斜案例"><a href="#碰到的数据倾斜案例" class="headerlink" title="碰到的数据倾斜案例"></a>碰到的数据倾斜案例</h1><h2 id="窗口分组数据倾斜"><a href="#窗口分组数据倾斜" class="headerlink" title="窗口分组数据倾斜"></a>窗口分组数据倾斜</h2><p><strong>倾斜场景</strong><br>业务上有一张消息记录表msg_records，sql要求是取下一次回复消息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> msg_tmp <span class="keyword">as</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span>  id                  <span class="comment">-- 唯一键，消息id</span></span><br><span class="line">           ,from_chat_id        <span class="comment">-- 消息发送者id</span></span><br><span class="line">           ,to_chat_id          <span class="comment">-- 消息接受者id</span></span><br><span class="line">           ,msg_time            <span class="comment">-- 消息时间</span></span><br><span class="line">    <span class="keyword">from</span> msg_records</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span>  id</span><br><span class="line">       ,msg_time</span><br><span class="line">       ,<span class="built_in">first_value</span>(if(type <span class="operator">=</span> <span class="string">&#x27;reply&#x27;</span>,id,<span class="keyword">null</span>),<span class="literal">true</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> from_chat_id,to_chat_id <span class="keyword">order</span> <span class="keyword">by</span> msg_time,id <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> following <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> reply_msg_id_n1t <span class="comment">-- 取下一次回复消息</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span>  id</span><br><span class="line">           ,from_chat_id</span><br><span class="line">           ,to_chat_id</span><br><span class="line">           ,msg_time</span><br><span class="line">           ,<span class="string">&#x27;send&#x27;</span> <span class="keyword">as</span> type</span><br><span class="line">    <span class="keyword">from</span> msg_tmp</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="comment">-- 调转，取返回消息</span></span><br><span class="line">    <span class="keyword">select</span>  id</span><br><span class="line">           ,to_chat_id   <span class="keyword">as</span> from_chat_id</span><br><span class="line">           ,from_chat_id <span class="keyword">as</span> to_chat_id</span><br><span class="line">           ,msg_time</span><br><span class="line">           ,<span class="string">&#x27;reply&#x27;</span>      <span class="keyword">as</span> type</span><br><span class="line">    <span class="keyword">from</span> msg_tmp</span><br><span class="line">) t1</span><br></pre></td></tr></table></figure><p><strong>sql执行分析</strong><br>有一个task执行耗时1h<br><img src="https://i.328888.xyz/2023/02/10/R2j3w.png" alt="R2j3w.png"></p><p><strong>数据倾斜分析</strong><br>根据窗口函数的分组<code>from_chat_id + to_chat_id</code>分析，数据量出现严重倾斜，表总数据量1亿多，其中，分组<code>from_chat_id=12 and to_chat_id=81867</code>的数据量有30w，其他分组数据量至多3w。</p><p>另外，分组<code>from_chat_id=12 and to_chat_id=81867</code>的数据在业务上可定义为脏数据，且first_value()函数计算出的值全为null。</p><p>经过测试验证发现，没有 <strong>rows between语句</strong> 或是 <strong>过滤倾斜数据</strong> 时，SQL执行很快</p><p>综上分析，再对照spark执行计划基本可以定位倾斜原因为<strong>窗口数据倾斜和rows between计算耗时</strong></p><p><strong>解决方案</strong><br>结合业务知识，在sql逻辑中过滤<code>from_chat_id=12 and to_chat_id=81867</code>的数据</p><p>最终，任务执行耗时从<code>1h</code>优化至<code>10min</code></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">美团技术团队：Spark性能优化指南——高级篇</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之生成连续数字</title>
      <link href="/posts/3ce3d37f.html"/>
      <url>/posts/3ce3d37f.html</url>
      
        <content type="html"><![CDATA[<h1 id="sql要求"><a href="#sql要求" class="headerlink" title="sql要求"></a>sql要求</h1><p>生成100以内的全部整数</p><h1 id="涉及udtf函数"><a href="#涉及udtf函数" class="headerlink" title="涉及udtf函数"></a>涉及udtf函数</h1><p><strong>posexplode</strong>(ARRAY&lt;T&gt; a) <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF?spm=a2c4g.11186623.0.0.3c267254Ka3fUh#LanguageManualUDF-posexplode(array)">官方说明</a><br><strong>Return</strong>: Returns a row-set with two columns (pos int,val T), one row for each element from the array.<br><strong>Description</strong>: posexplode() is similar to explode but instead of just returning the elements of the array it returns the element as well as its position in the original array.</p><p><strong>用法示例</strong>：<br>有如下一张表myTable</p><table><thead><tr><th align="center">(array&lt;int&gt;)myCol</th></tr></thead><tbody><tr><td align="center">[100,200,300]</td></tr><tr><td align="center">[400,500,600]</td></tr></tbody></table><p>执行hive sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 造数据</span></span><br><span class="line"><span class="keyword">with</span> myTable <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">array</span>(<span class="number">100</span>,<span class="number">200</span>,<span class="number">300</span>) <span class="keyword">as</span> myCol</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span> </span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">array</span>(<span class="number">300</span>,<span class="number">400</span>,<span class="number">500</span>) <span class="keyword">as</span> myCol</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 查询sql</span></span><br><span class="line"><span class="keyword">SELECT</span> posexplode(myCol) <span class="keyword">AS</span> (pos, val) <span class="keyword">FROM</span> myTable</span><br></pre></td></tr></table></figure><p>得到结果为：</p><table><thead><tr><th align="center">(int)pos</th><th align="center">(int)val</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">100</td></tr><tr><td align="center">1</td><td align="center">200</td></tr><tr><td align="center">2</td><td align="center">300</td></tr><tr><td align="center">0</td><td align="center">400</td></tr><tr><td align="center">1</td><td align="center">500</td></tr><tr><td align="center">2</td><td align="center">600</td></tr></tbody></table><h1 id="sql实现"><a href="#sql实现" class="headerlink" title="sql实现"></a>sql实现</h1><p>借助posexplode返回的pos即可实现</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> posexplode(split(space(<span class="number">99</span>), <span class="string">&#x27; &#x27;</span>)) <span class="keyword">as</span> (pos, val)</span><br><span class="line"><span class="comment">-- 返回的pos字段即为[0,99]区间的100个整数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 或者下面这种写法</span></span><br><span class="line"><span class="keyword">select</span> posexplode(split(repeat(<span class="string">&#x27;,&#x27;</span>,<span class="number">99</span>), <span class="string">&#x27;,&#x27;</span>)) <span class="keyword">as</span> (pos, val)</span><br></pre></td></tr></table></figure><h1 id="实例场景"><a href="#实例场景" class="headerlink" title="实例场景"></a>实例场景</h1><h2 id="数据重复扩容10倍"><a href="#数据重复扩容10倍" class="headerlink" title="数据重复扩容10倍"></a>数据重复扩容10倍</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 造数据</span></span><br><span class="line"><span class="keyword">with</span> myTable <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="string">&#x27;张三&#x27;</span> <span class="keyword">as</span> name</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span> </span><br><span class="line">    <span class="keyword">select</span> <span class="string">&#x27;李四&#x27;</span> <span class="keyword">as</span> name</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 将myTable的每行数据重复复制为5行</span></span><br><span class="line"><span class="keyword">SELECT</span> name</span><br><span class="line">       ,posexplode(split(space(<span class="number">4</span>), <span class="string">&#x27; &#x27;</span>)) <span class="keyword">AS</span> (pos, val) </span><br><span class="line"><span class="keyword">FROM</span> myTable</span><br></pre></td></tr></table></figure><p>得到结果为：</p><table><thead><tr><th align="center">name</th><th align="center">pos</th><th align="center">val</th></tr></thead><tbody><tr><td align="center">张三</td><td align="center">0</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">1</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">2</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">3</td><td align="center"></td></tr><tr><td align="center">张三</td><td align="center">4</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">0</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">1</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">2</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">3</td><td align="center"></td></tr><tr><td align="center">李四</td><td align="center">4</td><td align="center"></td></tr></tbody></table><h2 id="生成指定范围内的连续日期"><a href="#生成指定范围内的连续日期" class="headerlink" title="生成指定范围内的连续日期"></a>生成指定范围内的连续日期</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> subquery <span class="keyword">as</span>  (</span><br><span class="line">    <span class="keyword">select</span> split(space(datediff(<span class="string">&#x27;2023-1-31&#x27;</span>,<span class="string">&#x27;2022-11-30&#x27;</span>)), <span class="string">&#x27; &#x27;</span>)  <span class="keyword">as</span> x</span><br><span class="line">) </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    date_add(<span class="string">&#x27;2022-11-30&#x27;</span>, pos) <span class="keyword">as</span> new_date</span><br><span class="line"><span class="keyword">from</span>  </span><br><span class="line">    subquery t</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> </span><br><span class="line">    posexplode(x) pe <span class="keyword">as</span> pos, val</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL高级语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL之conf参数</title>
      <link href="/posts/5e220c44.html"/>
      <url>/posts/5e220c44.html</url>
      
        <content type="html"><![CDATA[<h1 id="官方说明传送门"><a href="#官方说明传送门" class="headerlink" title="官方说明传送门"></a><a href>官方说明传送门</a></h1><h1 id="资源参数"><a href="#资源参数" class="headerlink" title="资源参数"></a>资源参数</h1><h2 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h2><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h2 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h2><ul><li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1&#x2F;3~1&#x2F;2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><p>##executor-cores</p><ul><li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1&#x2F;3~1&#x2F;2左右比较合适，也是避免影响其他同学的作业运行。</li></ul><h2 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h2><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h2 id="sparkdefaultparallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h2><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li><li>参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h2 id="sparkstoragememoryfraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h2><ul><li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h2 id="sparkshufflememoryfraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h2><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h1 id="广播相关"><a href="#广播相关" class="headerlink" title="广播相关"></a>广播相关</h1><h2 id="sparksqlbroadcasttimeout"><a href="#spark-sql-broadcastTimeout" class="headerlink" title="spark.sql.broadcastTimeout"></a>spark.sql.broadcastTimeout</h2><h2 id="sparkkryoserializerbuffermaxx3d128m"><a href="#spark-kryoserializer-buffer-max-x3D-128M" class="headerlink" title="spark.kryoserializer.buffer.max&#x3D;128M"></a>spark.kryoserializer.buffer.max&#x3D;128M</h2><h2 id="sparksqlshufflepartitionsx3d1000"><a href="#spark-sql-shuffle-partitions-x3D-1000" class="headerlink" title="spark.sql.shuffle.partitions&#x3D;1000"></a>spark.sql.shuffle.partitions&#x3D;1000</h2><h2 id="sparksqlorccompressioncodecx3dzlib"><a href="#spark-sql-orc-compression-codec-x3D-zlib" class="headerlink" title="spark.sql.orc.compression.codec&#x3D;zlib"></a>spark.sql.orc.compression.codec&#x3D;zlib</h2><h2 id="sparksqlfilesmaxpartitionbytesx3d65536"><a href="#spark-sql-files-maxPartitionBytes-x3D-65536" class="headerlink" title="spark.sql.files.maxPartitionBytes&#x3D;65536"></a>spark.sql.files.maxPartitionBytes&#x3D;65536</h2>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL之set参数</title>
      <link href="/posts/4547a6e2.html"/>
      <url>/posts/4547a6e2.html</url>
      
        <content type="html"><![CDATA[<h1 id="官方传送门"><a href="#官方传送门" class="headerlink" title="官方传送门"></a><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties">官方传送门</a></h1><h2 id="hivemergemapfiles"><a href="#hive-merge-mapfiles" class="headerlink" title="hive.merge.mapfiles"></a>hive.merge.mapfiles</h2><p>Default Value: true<br>map-only任务结束时合并小文件</p><h2 id="hivemergemapredfiles"><a href="#hive-merge-mapredfiles" class="headerlink" title="hive.merge.mapredfiles"></a>hive.merge.mapredfiles</h2><p>Default Value: true<br>map-reduce任务结束时合并小文件</p><h2 id="hiveoptimizectematerializethreshold"><a href="#hive-optimize-cte-materialize-threshold" class="headerlink" title="hive.optimize.cte.materialize.threshold"></a>hive.optimize.cte.materialize.threshold</h2><p>默认情况下是-1（关闭）；当开启（大于0），比如设置为2，则如果with..as语句被引用2次及以上时，会把with..as语句生成的table物化，从而做到with..as语句只执行一次，来提高效率</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hiveSQL命令之alter partition</title>
      <link href="/posts/8a94c1da.html"/>
      <url>/posts/8a94c1da.html</url>
      
        <content type="html"><![CDATA[<p>msck repair table</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 改分区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop命令之distcp分布式拷贝</title>
      <link href="/posts/bcc5bdf2.html"/>
      <url>/posts/bcc5bdf2.html</url>
      
        <content type="html"><![CDATA[<h1 id="distcp用途"><a href="#distcp用途" class="headerlink" title="distcp用途"></a>distcp用途</h1><p>DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。<br>使用Map&#x2F;Reduce实现文件分发，错误处理和恢复，以及报告生成。<br>DistCp将文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 </p><h1 id="distcp用法"><a href="#distcp用法" class="headerlink" title="distcp用法"></a>distcp用法</h1><p>命令行中可以指定多个源目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">hadoop distcp source_dir1 [source_dir2 source_dir3……] target_dir</span><br></pre></td></tr></table></figure><p>集群内拷贝</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">hadoop distcp [hdfs://nn:8020]/db/table_a/partition=1 [hdfs://nn:8020]/db/table_b/partition=1</span><br></pre></td></tr></table></figure><p>不同集群间拷贝，DistCp必须运行在目标端集群上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">hadoop distcp hdfs://nn1:8020/db/table_a/partition=1 hdfs://nn2:8020/db/table_b/partition=1</span><br></pre></td></tr></table></figure><h1 id="常用参数选项"><a href="#常用参数选项" class="headerlink" title="常用参数选项"></a>常用参数选项</h1><h2 id="-overwrite"><a href="#overwrite" class="headerlink" title="-overwrite"></a>-overwrite</h2><p>源文件覆盖同名目标文件</p><h2 id="-update"><a href="#update" class="headerlink" title="-update"></a>-update</h2><p>拷贝目标目录下不存在而源目录下存在的文件，当文件大小不一致时，源文件覆盖同名目标文件</p><h2 id="-delete"><a href="#delete" class="headerlink" title="-delete"></a>-delete</h2><p>删除目标目录下存在，但源目录下不存在的文件，需要配合<code>-update</code>或<code>-overwrite</code>使用</p><h2 id="-prbugpcaxt"><a href="#p-rbugpcaxt" class="headerlink" title="-p[rbugpcaxt]"></a>-p[rbugpcaxt]</h2><p>控制是否保留源文件的属性，<code>-p</code>默认全部保留，常用的为<code>-pbugp</code>。<br>修改次数不会被保留。并且当指定 -update 时，更新的状态不会 被同步，除非文件大小不同（比如文件被重新创建）。</p><table><thead><tr><th>标识</th><th>含义</th><th>备注</th></tr></thead><tbody><tr><td>r</td><td>replication number</td><td>文件副本数</td></tr><tr><td>b</td><td>block size</td><td>文件块大小</td></tr><tr><td>u</td><td>user</td><td>用户</td></tr><tr><td>g</td><td>group</td><td>组</td></tr><tr><td>p</td><td>permission</td><td>文件权限</td></tr><tr><td>c</td><td>checksum-type</td><td>校验和类型</td></tr><tr><td>a</td><td>acl</td><td></td></tr><tr><td>x</td><td>xattr</td><td></td></tr><tr><td>t</td><td>timestamp</td><td>时间戳</td></tr></tbody></table><h2 id="-m"><a href="#m" class="headerlink" title="-m"></a>-m</h2><p>控制拷贝时的map任务最大个数<br>如果没使用-m选项，DistCp会尝试在调度工作时指定map数目&#x3D;min(total_bytes&#x2F;bytes.per.map,20*num_task_trackers)， 其中bytes.per.map默认是256MB。</p><h1 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h1><h2 id="表结构一致的两表互相拷贝数据"><a href="#表结构一致的两表互相拷贝数据" class="headerlink" title="表结构一致的两表互相拷贝数据"></a>表结构一致的两表互相拷贝数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#********************************************************************************</span></span><br><span class="line"><span class="comment"># **  功能描述：通过hdfs文件路径拷贝的方式，实现表结构完全相同的表互相拷贝数据</span></span><br><span class="line"><span class="comment">#********************************************************************************</span></span><br><span class="line"><span class="comment"># 指定源路径、目标路径</span></span><br><span class="line">source_dir=/db/table_a/partition=1</span><br><span class="line">target_dir=/db/table_b/partition=1</span><br><span class="line">db_name=db_a</span><br><span class="line">target_tbl_name=db_a.table_b</span><br><span class="line"><span class="comment"># 判断源路径是否存在，不存在则返回</span></span><br><span class="line">hadoop fs -<span class="built_in">test</span> -e <span class="variable">$source_dir</span></span><br><span class="line"><span class="keyword">if</span> [ $? -ne 0 ];<span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;源路径<span class="variable">$source_dir</span>不存在&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># 判断目标路径是否存在，不存在则创建</span></span><br><span class="line">hadoop fs -<span class="built_in">test</span> -e <span class="variable">$target_dir</span></span><br><span class="line"><span class="keyword">if</span> [ $? -ne 0 ];<span class="keyword">then</span></span><br><span class="line">  hadoop fs -<span class="built_in">mkdir</span> <span class="variable">$target_dir</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;目标路径<span class="variable">$target_dir</span>不存在，创建成功&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># 开始拷贝</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;开始hdfs文件拷贝，source_dir=<span class="variable">$source_dir</span>，target_dir=<span class="variable">$target_dir</span>&quot;</span></span><br><span class="line">hadoop distcp -overwrite -delete -pbugp <span class="variable">$source_dir</span> <span class="variable">$target_dir</span></span><br><span class="line"><span class="keyword">if</span> [ $? -eq 0 ];<span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;hdfs文件拷贝成功&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;hdfs文件拷贝失败&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> -1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># 刷新目标表的metastore信息</span></span><br><span class="line">hive -database <span class="variable">$db_name</span> -v -e <span class="string">&quot;msck repair table <span class="variable">$target_tbl_name</span>;&quot;</span></span><br><span class="line"><span class="keyword">if</span> [ $? -eq 0 ];<span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$target_tbl_name</span>表的metastore信息刷新成功&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://hadoop.apache.org/docs/r1.0.4/cn/distcp.html">DistCp使用指南</a><br><a href="https://hadoop.org.cn/docs/hadoop-distcp/DistCp.html">Hadoop中文网：DistCp</a></p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop命令 </tag>
            
            <tag> hdfs文件拷贝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell命令之set-e</title>
      <link href="/posts/ba81765c.html"/>
      <url>/posts/ba81765c.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop基本命令</title>
      <link href="/posts/b24f0feb.html"/>
      <url>/posts/b24f0feb.html</url>
      
        <content type="html"><![CDATA[<p>hadoop fs -cp<br>hadoop fs -rm -r<br>hadoop distcp -overwrite -delete -p<br>hadoop fs -mkdir -p</p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive动态分区</title>
      <link href="/posts/44d3528f.html"/>
      <url>/posts/44d3528f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态分区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL无法处理hive表中的空ORC文件</title>
      <link href="/posts/1f69e18b.html"/>
      <url>/posts/1f69e18b.html</url>
      
        <content type="html"><![CDATA[<h1 id="碰到了什么问题"><a href="#碰到了什么问题" class="headerlink" title="碰到了什么问题"></a>碰到了什么问题</h1><p>起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo<br><img src="https://i.328888.xyz/2022/12/19/AfHSH.png" alt="AfHSH.png"><br><img src="https://i.328888.xyz/2022/12/19/AfbiQ.png" alt="AfbiQ.png"><br>之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客</p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><h2 id="定位问题"><a href="#定位问题" class="headerlink" title="定位问题"></a>定位问题</h2><p>根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段。</p><h2 id="查看hdfs文件"><a href="#查看HDFS文件" class="headerlink" title="查看HDFS文件"></a>查看HDFS文件</h2><p>查看表存储路径下的文件，发现有1个空文件<br><img src="https://i.328888.xyz/2022/12/19/AfjbE.png" alt="AfjbE.png"></p><h2 id="为什么会有空文件"><a href="#为什么会有空文件" class="headerlink" title="为什么会有空文件"></a>为什么会有空文件</h2><p>1、sparkSQL建表<br>2、表写入数据时，sql最后做了distribute by操作，产生了空文件</p><p>sparksql读取空文件的时候，因为表是orc格式的，导致sparkSQL解析orc文件出错。但是用hive却可以正常读取。</p><h2 id="网上搜罗的解决办法"><a href="#网上搜罗的解决办法" class="headerlink" title="网上搜罗的解决办法"></a>网上搜罗的解决办法</h2><p>问题原因基本清晰了，就是读取空文件导致的报错，如果非得用SparkSQL执行查询语句，这里提供几种解决方案：</p><h4 id="1-修改表存储格式为parquet"><a href="#1、修改表存储格式为parquet" class="headerlink" title="1、修改表存储格式为parquet"></a>1、修改表存储格式为parquet</h4><p>这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐</p><h4 id="2-参数设置set-hiveexecorcsplitstrategyetl"><a href="#2、参数设置：set-hive-exec-orc-split-strategy-ETL" class="headerlink" title="2、参数设置：set hive.exec.orc.split.strategy=ETL"></a>2、参数设置：<code>set hive.exec.orc.split.strategy=ETL</code></h4><p>既然已经定位到是空文件读取的问题，那就从文件读取层面解决。</p><p>自建集群<code>Spark</code>源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java</span></span><br><span class="line"><span class="keyword">switch</span>(context.splitStrategyKind) &#123;</span><br><span class="line">    <span class="keyword">case</span> BI:</span><br><span class="line">    <span class="comment">// BI strategy requested through config</span></span><br><span class="line">    splitStrategy = <span class="keyword">new</span> <span class="title class_">BISplitStrategy</span>(context, fs, dir, children, isOriginal,</span><br><span class="line">        deltas, covered);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> ETL:</span><br><span class="line">    <span class="comment">// ETL strategy requested through config</span></span><br><span class="line">    splitStrategy = <span class="keyword">new</span> <span class="title class_">ETLSplitStrategy</span>(context, fs, dir, children, isOriginal,</span><br><span class="line">        deltas, covered);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    <span class="comment">// HYBRID strategy</span></span><br><span class="line">    <span class="keyword">if</span> (avgFileSize &gt; context.maxSize) &#123;</span><br><span class="line">        splitStrategy = <span class="keyword">new</span> <span class="title class_">ETLSplitStrategy</span>(context, fs, dir, children, isOriginal, deltas,</span><br><span class="line">            covered);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        splitStrategy = <span class="keyword">new</span> <span class="title class_">BISplitStrategy</span>(context, fs, dir, children, isOriginal, deltas,</span><br><span class="line">            covered);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ./repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf.class</span></span><br><span class="line">HIVE_ORC_SPLIT_STRATEGY(<span class="string">&quot;hive.exec.orc.split.strategy&quot;</span>, <span class="string">&quot;HYBRID&quot;</span>, <span class="keyword">new</span> <span class="title class_">StringSet</span>(<span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;HYBRID&quot;</span>, <span class="string">&quot;BI&quot;</span>, <span class="string">&quot;ETL&quot;</span>&#125;), <span class="string">&quot;This is not a user level config. BI strategy is used when the requirement is to spend less time in split generation as opposed to query execution (split generation does not read or cache file footers). ETL strategy is used when spending little more time in split generation is acceptable (split generation reads and caches file footers). HYBRID chooses between the above strategies based on heuristics.&quot;</span>)      </span><br><span class="line">  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。</p><ul><li>BI策略以文件为粒度进行split划分</li><li>ETL策略会将文件进行切分，多个stripe组成一个split</li><li>HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。</li></ul><p>ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;OrcSplit&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  List&lt;OrcSplit&gt; splits = Lists.newArrayList();</span><br><span class="line">  <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">    String[] hosts = SHIMS</span><br><span class="line">        .getLocationsWithOffset(fs, fileStatus) <span class="comment">// 对空文件会返回一个空的TreeMap</span></span><br><span class="line">        .firstEntry()  <span class="comment">// null</span></span><br><span class="line">        .getValue()    <span class="comment">// NPE</span></span><br><span class="line">        .getHosts();</span><br><span class="line">    <span class="type">OrcSplit</span> <span class="variable">orcSplit</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OrcSplit</span>(fileStatus.getPath(), <span class="number">0</span>, fileStatus.getLen(), hosts,</span><br><span class="line">                                     <span class="literal">null</span>, isOriginal, <span class="literal">true</span>, deltas, -<span class="number">1</span>);</span><br><span class="line">    splits.add(orcSplit);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// add uncovered ACID delta splits</span></span><br><span class="line">  splits.addAll(<span class="built_in">super</span>.getSplits());</span><br><span class="line">  <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;SplitInfo&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    List&lt;SplitInfo&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">for</span> (FileStatus file : files) &#123;</span><br><span class="line">    <span class="type">FileInfo</span> <span class="variable">info</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (context.cacheStripeDetails) &#123;</span><br><span class="line">        info = verifyCachedFileInfo(file);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ignore files of 0 length（此处对空文件做了过滤）</span></span><br><span class="line">    <span class="keyword">if</span> (file.getLen() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        result.add(<span class="keyword">new</span> <span class="title class_">SplitInfo</span>(context, fs, file, info, isOriginal, deltas, <span class="literal">true</span>, dir, covered));</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 本质上是一个BUG，<code>Spark2.4</code>版本中解决了这个问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;OrcSplit&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    List&lt;OrcSplit&gt; splits = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">    String[] hosts = SHIMS.getLocationsWithOffset(fs, fileStatus).firstEntry().getValue()</span><br><span class="line">        .getHosts();</span><br><span class="line">    <span class="type">OrcSplit</span> <span class="variable">orcSplit</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OrcSplit</span>(fileStatus.getPath(), <span class="number">0</span>, fileStatus.getLen(), hosts,</span><br><span class="line">        <span class="literal">null</span>, isOriginal, <span class="literal">true</span>, deltas, -<span class="number">1</span>);</span><br><span class="line">    splits.add(orcSplit);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add uncovered ACID delta splits</span></span><br><span class="line">    splits.addAll(<span class="built_in">super</span>.getSplits());</span><br><span class="line">    <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>了解了spark读取orc文件策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.orc.split.strategy<span class="operator">=</span>ETL</span><br></pre></td></tr></table></figure><p>经测试无效。原因分析：<br>1、参数未生效<br>2、hdfs文件有两个，大小为49B和7.45G，文件的平均大小肯定是大于256M的，所以按默认HYBRID策略规则应本就是采取的ETL策略split ORC文件</p><h4 id="3-参数设置sparksqlhiveconvertmetastoreorctrue"><a href="#3、参数设置：spark-sql-hive-convertMetastoreOrc-true" class="headerlink" title="3、参数设置：spark.sql.hive.convertMetastoreOrc=true"></a>3、参数设置：<code>spark.sql.hive.convertMetastoreOrc=true</code></h4><p>关于参数的<a href="https://spark.apache.org/docs/2.3.3/sql-programming-guide.html#orc-files">官方介绍</a></p><blockquote><p>Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true.</p></blockquote><p>经测试有效。若仍报错，可尝试搭配spark.sql.orc.impl&#x3D;native使用。</p><h1 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a>补充知识</h1><p><img src="https://i.328888.xyz/2022/12/19/Af23F.jpeg" alt="Af23F.jpeg"><br>hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。<br>另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.freesion.com/article/8054484645/">SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTION</a><br><a href="https://blog.csdn.net/weixin_45240507/article/details/124689323?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-124689323-blog-100524131.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-124689323-blog-100524131.pc_relevant_default&utm_relevant_index=7">SparkSQL读取ORC表时遇到空文件</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ORC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资料汇总</title>
      <link href="/posts/76d5a95a.html"/>
      <url>/posts/76d5a95a.html</url>
      
        <content type="html"><![CDATA[<h1 id="sparksql"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><p><a href="https://cloud.tencent.com/developer/article/1348071">Spark SQL Limit 介绍及优化</a></p><p><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html">Spark性能优化指南——基础篇</a></p><p><a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">Spark性能优化指南——高级篇</a></p><h1 id="大数据笔记"><a href="#大数据笔记" class="headerlink" title="大数据笔记"></a>大数据笔记</h1><p><a href="https://github.com/heibaiying/BigData-Notes">大数据入门指南</a></p><h1 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h1><p><a href="https://programmercarl.com/other/algo_pdf.html">代码随想录</a></p><h1 id="数据治理"><a href="#数据治理" class="headerlink" title="数据治理"></a>数据治理</h1><p><a href="https://www.infoq.cn/article/mBgvbc3bQWAbavwmBhXR">存储和计算资源都节省 30%，网易云音乐数据治理实践</a></p><h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><p><a href="https://www.infoq.cn/article/0rSVq2VIfUE0YLedLe5o">数据库内核杂谈系列</a></p><h1 id="流量数仓"><a href="#流量数仓" class="headerlink" title="流量数仓"></a>流量数仓</h1><p><a href="https://juejin.cn/post/6930083471037562893">用户流量数仓建设思考（一）：基于漏斗模型的通用建设</a></p><h1 id="乱七八糟的"><a href="#乱七八糟的" class="headerlink" title="乱七八糟的"></a>乱七八糟的</h1><p><a href="https://mp.weixin.qq.com/s/g5aQVHMUVkSrEO6QWDI1yw">2万字揭秘阿里巴巴数据治理平台建设经验</a></p>]]></content>
      
      
      <categories>
          
          <category> 资料 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>get_json_object在sql中的高级用法</title>
      <link href="/posts/5f45fcd7.html"/>
      <url>/posts/5f45fcd7.html</url>
      
        <content type="html"><![CDATA[<h2 id="语法介绍"><a href="#语法介绍" class="headerlink" title="语法介绍"></a>语法介绍</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get_json_object(String json_string, String path)</span><br><span class="line"><span class="comment">-- return string</span></span><br></pre></td></tr></table></figure><p>get_json_object函数是用来根据指定路径提取json字符串中的json对象，并返回json对象的json字符串</p><h2 id="现有困惑"><a href="#现有困惑" class="headerlink" title="现有困惑"></a>现有困惑</h2><p>关于这个函数最常见的用法就是<code>get_json_object(&#39;&#123;&quot;a&quot;:&quot;b&quot;&#125;&#39;, &#39;$.a&#39;)</code>，返回结果<code>b</code><br>但<code>$.a</code>这种path写法仅适用于简单的多层嵌套json字符串解析，碰到嵌套层有json数组时就难以解析了<br>比如，要提取下面这段json中的所有<code>weight</code>对象的值</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"> <span class="attr">&quot;store&quot;</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;fruit&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span><span class="number">8</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;apple&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span><span class="number">9</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;pear&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span>  <span class="comment">//json数组</span></span><br><span class="line">         <span class="attr">&quot;bicycle&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;price&quot;</span><span class="punctuation">:</span><span class="number">19.95</span><span class="punctuation">,</span><span class="attr">&quot;color&quot;</span><span class="punctuation">:</span><span class="string">&quot;red&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span><span class="punctuation">,</span> </span><br><span class="line"> <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span><span class="string">&quot;amy@only_for_json_udf_test.net&quot;</span><span class="punctuation">,</span> </span><br><span class="line"> <span class="attr">&quot;owner&quot;</span><span class="punctuation">:</span><span class="string">&quot;amy&quot;</span> </span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>通过<code>$.store.fruit.weight</code>路径是无法提取的，<code>$.store.fruit[0].weight</code>这种写法仅能获取json数组中第一个json字符串中<code>weight</code>对象的值，也总不能用<code>[0]、[1]、[2]……</code>的方式无穷尽取值吧</p><p>到这里思维就限制住了，遇到这种情况时，以前的方式是通过正则表达式处理<br>具体实现如下：<br>首先将<code>item_properties</code>按指定分隔符split为array数组，再利用explode函数将array数组的元素逐行输出，最终得到的<code>item_propertie</code>即为单个json字符串，可根据<code>$.</code>提取指定json对象的值，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- item_properties = [&#123;&quot;id&quot;:42,&quot;name&quot;:&quot;包装&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:43,&quot;name&quot;:&quot;种类&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:44,&quot;name&quot;:&quot;规格&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:63,&quot;name&quot;:&quot;保质期(天)&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:100,&quot;name&quot;:&quot;适用年龄&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;</span></span><br><span class="line"><span class="comment">--                   ,&#123;&quot;id&quot;:101,&quot;name&quot;:&quot;储存条件&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> get_json_object(item_propertie,<span class="string">&#x27;$.id&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> table_a</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(regexp_replace(substr(item_properties,<span class="number">2</span>,length(item_properties)<span class="number">-2</span>),<span class="string">&#x27;\\&#125;\\,\\&#123;&#x27;</span>,<span class="string">&#x27;\\&#125;\\|\\|\\&#123;&#x27;</span>),<span class="string">&#x27;\\|\\|&#x27;</span>)) tmp <span class="keyword">as</span> item_propertie</span><br></pre></td></tr></table></figure><p>但上面这种处理方式存在bug，将json数据split为array数组时，必须保证指定分隔符不出现在单个json字符串中，比如上述case中是用<code>&#125;,&#123;</code>替换为<code>&#125;||&#123;</code>，再以<code>||</code>作为分隔符split，如若在单个json字符串中也出现了<code>&#125;,&#123;</code>或是<code>||</code>就会导致解析失败</p><h2 id="怎么高级了"><a href="#怎么高级了" class="headerlink" title="怎么高级了"></a>怎么高级了</h2><p>突然有一天在翻看<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF?spm=a2c4g.11186623.0.0.3c267254Ka3fUh#LanguageManualUDF-get_json_object">hive官方文档</a>时发现path支持的通配符<code>*</code><br><a href="https://imgloc.com/i/2Pw2H"><img src="https://i.328888.xyz/2023/01/17/2Pw2H.png" alt="2Pw2H.png"></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ : 表示根节点</span><br><span class="line">. : 表示子节点</span><br><span class="line">[] : [number]表示数组下标，从0开始</span><br><span class="line">* : []的通配符，返回整个数组</span><br></pre></td></tr></table></figure><p>所以，一开始的问题应该按如下解法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- jsonArray = &#123;&quot;store&quot;:</span></span><br><span class="line"><span class="comment">--                     &#123;</span></span><br><span class="line"><span class="comment">--                     &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],</span></span><br><span class="line"><span class="comment">--                     &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;</span></span><br><span class="line"><span class="comment">--                     &#125;, </span></span><br><span class="line"><span class="comment">--             &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, </span></span><br><span class="line"><span class="comment">--             &quot;owner&quot;:&quot;amy&quot;&#125;</span></span><br><span class="line"><span class="keyword">select</span> get_json_object(jsonArray, <span class="string">&#x27;$.store.fruit[*].weight&#x27;</span>);</span><br><span class="line"><span class="comment">-- return [8,9]</span></span><br></pre></td></tr></table></figure><p>笔者个人认为，高级之处在于写法极其清爽，按照以前用正则表达式的处理方法，需要多道处理才能得到结果<code>[8,9]</code>，而且其中还有隐性风险，但是现在<code>$.store.fruit[*].weight</code>这种极简语法既避免了风险，又清晰易理解</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL高级语法 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
