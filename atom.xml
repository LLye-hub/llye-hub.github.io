<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LLye</title>
  
  
  <link href="https://llye-hub.github.io/atom.xml" rel="self"/>
  
  <link href="https://llye-hub.github.io/"/>
  <updated>2022-12-21T08:35:26.765Z</updated>
  <id>https://llye-hub.github.io/</id>
  
  <author>
    <name>yuye</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SparkSQL无法处理hive表中的空ORC文件</title>
    <link href="https://llye-hub.github.io/posts/1f69e18b.html"/>
    <id>https://llye-hub.github.io/posts/1f69e18b.html</id>
    <published>2022-12-16T09:56:46.000Z</published>
    <updated>2022-12-21T08:35:26.765Z</updated>
    
    <content type="html"><![CDATA[<h3><span id="参考资料">参考资料</span></h3><p><a href="https://www.freesion.com/article/8054484645/">SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTION</a><br><a href="https://blog.csdn.net/weixin_45240507/article/details/124689323?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-124689323-blog-100524131.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-124689323-blog-100524131.pc_relevant_default&utm_relevant_index=7">SparkSQL读取ORC表时遇到空文件</a></p><h1><span id="为什么碰到这个问题">为什么碰到这个问题</span></h1><p>起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo<br><img src="https://i.328888.xyz/2022/12/19/AfHSH.png" alt="AfHSH.png"><br><img src="https://i.328888.xyz/2022/12/19/AfbiQ.png" alt="AfbiQ.png"><br>之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客</p><h1><span id="问题分析">问题分析</span></h1><h4><span id="定位问题">定位问题</span></h4><p>根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段</p><h4><span id="查看hdfs文件">查看HDFS文件</span></h4><p>查看表存储路径下的文件，发现有1个空文件<br><img src="https://i.328888.xyz/2022/12/19/AfjbE.png" alt="AfjbE.png"></p><h4><span id="为什么会有空文件">为什么会有空文件</span></h4><p>空文件是根据map个数产生的小文件，启动select 查询必然启动MR 那就避免不了Map阶段的产生</p><h4><span id="解决办法">解决办法</span></h4><p>问题原因基本清晰了，就是读取空文件导致的报错，但是计算过程中无法避免空文件产生，如果非得用SparkSQL执行查询语句，这里提供几种解决方案：</p><h5><span id="1-修改表存储格式为parquet">1、修改表存储格式为parquet</span></h5><p>这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐</p><h5><span id="2-参数设置set-hiveexecorcsplitstrategyetl">2、参数设置：<code>set hive.exec.orc.split.strategy=ETL</code></span></h5><p>既然已经定位到是空文件读取的问题，那就从文件读取层面解决。</p><p>关于参数的<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties">官方介绍</a>：</p><blockquote><p><strong>hive.exec.orc.split.strategy</strong><br>Default Value: HYBRID<br>Added In: Hive 1.2.0 with HIVE-10114</p><p>What strategy ORC should use to create splits for execution. The available options are “BI”, “ETL” and “HYBRID”.<br>The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS blocksize. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.</p></blockquote><p>相关源码<code>Spark 2.12</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(context.splitStrategyKind) &#123;</span><br><span class="line"><span class="keyword">case</span> BI:</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">OrcInputFormat</span>.BISplitStrategy(context, fs, dir, baseFiles, isOriginal, deltas, covered, allowSyntheticFileIds);</span><br><span class="line"><span class="keyword">case</span> ETL:</span><br><span class="line">    <span class="keyword">return</span> combineOrCreateETLStrategy(combinedCtx, context, fs, dir, baseFiles, deltas, covered, readerTypes, isOriginal, ugi, allowSyntheticFileIds);</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">if</span> (avgFileSize &lt;= context.maxSize &amp;&amp; totalFiles &gt; context.etlFileThreshold) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">OrcInputFormat</span>.BISplitStrategy(context, fs, dir, baseFiles, isOriginal, deltas, covered, allowSyntheticFileIds);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> combineOrCreateETLStrategy(combinedCtx, context, fs, dir, baseFiles, deltas, covered, readerTypes, isOriginal, ugi, allowSyntheticFileIds);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。</p><ul><li>BI策略以文件为粒度进行split划分</li><li>ETL策略会将文件进行切分，多个stripe组成一个split</li><li>HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。</li></ul><p>ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;OrcSplit&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  List&lt;OrcSplit&gt; splits = Lists.newArrayList();</span><br><span class="line">  <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">    String[] hosts = SHIMS</span><br><span class="line">        .getLocationsWithOffset(fs, fileStatus) <span class="comment">// 对空文件会返回一个空的TreeMap</span></span><br><span class="line">        .firstEntry()  <span class="comment">// null</span></span><br><span class="line">        .getValue()    <span class="comment">// NPE</span></span><br><span class="line">        .getHosts();</span><br><span class="line">    <span class="type">OrcSplit</span> <span class="variable">orcSplit</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OrcSplit</span>(fileStatus.getPath(), <span class="number">0</span>, fileStatus.getLen(), hosts,</span><br><span class="line">                                     <span class="literal">null</span>, isOriginal, <span class="literal">true</span>, deltas, -<span class="number">1</span>);</span><br><span class="line">    splits.add(orcSplit);</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// add uncovered ACID delta splits</span></span><br><span class="line">  splits.addAll(<span class="built_in">super</span>.getSplits());</span><br><span class="line">  <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplits</span></span><br><span class="line"><span class="keyword">public</span> List&lt;SplitInfo&gt; <span class="title function_">getSplits</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    List&lt;SplitInfo&gt; result = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(files.size());</span><br><span class="line">    ……</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; files.size(); ++i) &#123;</span><br><span class="line">        ……</span><br><span class="line">        <span class="comment">// Ignore files eliminated by PPD, or of 0 length.（此处对空文件做了过滤）</span></span><br><span class="line">        <span class="keyword">if</span> (ppdResult != FooterCache.NO_SPLIT_AFTER_PPD &amp;&amp; file.getFileStatus().getLen() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        result.add(<span class="keyword">new</span> <span class="title class_">SplitInfo</span>(context, dir.fs, file, orcTail, readerTypes,</span><br><span class="line">            isOriginal, deltas, <span class="literal">true</span>, dir.dir, covered, ppdResult));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">dirIx</span> <span class="operator">=</span> -<span class="number">1</span>, fileInDirIx = -<span class="number">1</span>, filesInDirCount = <span class="number">0</span>;</span><br><span class="line">    <span class="type">ETLDir</span> <span class="variable">dir</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">for</span> (HdfsFileStatusWithId file : files) &#123;</span><br><span class="line">        ……</span><br><span class="line">        <span class="comment">// ignore files of 0 length（此处对空文件做了过滤）</span></span><br><span class="line">        <span class="keyword">if</span> (file.getFileStatus().getLen() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        result.add(<span class="keyword">new</span> <span class="title class_">Sp</span> litInfo(context, dir.fs, file, <span class="literal">null</span>, readerTypes,</span><br><span class="line">            isOriginal, deltas, <span class="literal">true</span>, dir.dir, covered, <span class="literal">null</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 本质上是一个Hive的BUG，Spark2.4版本中解决了这个问题。<br>知道读取数据的策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.orc.split.strategy<span class="operator">=</span>ETL</span><br></pre></td></tr></table></figure><h5><span id="3-参数设置sparksqlhiveconvertmetastoreorctrue">3、参数设置：<code>spark.sql.hive.convertMetastoreOrc=true</code></span></h5><p>关于参数的<a href="https://spark.apache.org/docs/2.3.3/sql-programming-guide.html#orc-files">官方介绍</a></p><blockquote><p>Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true.</p></blockquote><p>最后的最后，以上3种解决方案仅供参考，为笔者对问题剖析之后，再结合网上资料整理的，尚未经过实际检验。笔者对这个问题的解决方法就是不用SparkSQL查就好，hiveSQL不会有此问题。</p><h1><span id="补充知识">补充知识</span></h1><p><img src="https://i.328888.xyz/2022/12/19/Af23F.jpeg" alt="Af23F.jpeg"><br>hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。<br>另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3&gt;&lt;span id=&quot;参考资料&quot;&gt;参考资料&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://www.freesion.com/article/8054484645/&quot;&gt;SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTION&lt;/a&gt;&lt;br</summary>
      
    
    
    
    
    <category term="HDFS" scheme="https://llye-hub.github.io/tags/HDFS/"/>
    
    <category term="ORC" scheme="https://llye-hub.github.io/tags/ORC/"/>
    
  </entry>
  
  <entry>
    <title>get_json_object在sql中的高级用法</title>
    <link href="https://llye-hub.github.io/posts/5f45fcd7.html"/>
    <id>https://llye-hub.github.io/posts/5f45fcd7.html</id>
    <published>2022-12-16T03:46:24.000Z</published>
    <updated>2022-12-20T06:59:07.095Z</updated>
    
    
    
    
    
    <category term="SQL" scheme="https://llye-hub.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>大数据相关资料</title>
    <link href="https://llye-hub.github.io/posts/76d5a95a.html"/>
    <id>https://llye-hub.github.io/posts/76d5a95a.html</id>
    <published>2022-12-16T03:46:24.000Z</published>
    <updated>2022-12-20T06:59:07.096Z</updated>
    
    
    
    
    
    <category term="大数据" scheme="https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
