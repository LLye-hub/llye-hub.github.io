{"meta":{"title":"LLye","subtitle":"","description":"scsadad","author":"LLye","url":"https://llye-hub.github.io","root":"/"},"pages":[{"title":"Repositories","date":"2023-01-16T07:15:53.818Z","updated":"2023-01-16T07:15:53.818Z","comments":false,"path":"index.html","permalink":"https://llye-hub.github.io/index.html","excerpt":"","text":""},{"title":"About Me","date":"2023-01-17T08:41:32.555Z","updated":"2023-01-17T08:41:32.555Z","comments":false,"path":"about/index.html","permalink":"https://llye-hub.github.io/about/index.html","excerpt":"","text":"工作经历"},{"title":"友情链接","date":"2023-01-16T07:30:43.399Z","updated":"2023-01-16T07:30:43.399Z","comments":true,"path":"links/index.html","permalink":"https://llye-hub.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-01-16T07:29:50.652Z","updated":"2023-01-16T07:29:50.652Z","comments":false,"path":"tags/index.html","permalink":"https://llye-hub.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-02-17T05:46:56.411Z","updated":"2023-02-17T05:46:56.411Z","comments":false,"path":"categories/index.html","permalink":"https://llye-hub.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"SQL之窗口函数的边界","slug":"SQL/SQL之窗口函数的边界","date":"2023-02-28T08:25:40.000Z","updated":"2023-03-01T07:30:20.176Z","comments":true,"path":"posts/5af52219.html","link":"","permalink":"https://llye-hub.github.io/posts/5af52219.html","excerpt":"","text":"前言窗口函数常用于在SQL数据分析计算各种统计指标，也是日常sql开发中常见的函数了，但是最近发现自己在这方面存在一些误解 比如下面这段sql 1234select col1 ,sum(col2) over(partition by col1 order by col3) as sum1 ,sum(col2) over(partition by col1) as sum2from (select * from (VALUES(&#x27;a&#x27;,1,4),(&#x27;a&#x27;,2,7),(&#x27;a&#x27;,3,6)) t(col1,col2,col3)) a 第一眼感觉sum1和sum2字段计算值是一样的，但实际运行出来的结果为(mysql+hiveSQL) col1 sum1 sum2 a 1 6 a 4 6 a 6 6 从执行结果上来看，sum1字段为窗口内的累加值，sum2字段值为窗口内所有值之和 为什么有无order by差异这么大有人会说，聚合函数sum()的窗口内有order by子句时，计算结果本就是累加性质。从执行结果上来看，这么说是对的，但是这种解释太流于表面，并没有真正从函数定义上解释为什么 这里重新回顾下窗口函数基本语法： 1&lt;window_function&gt; over (partition by &lt;column_name&gt; order by &lt;column_name&gt; &lt;window_frame&gt;) 主要有四个部分： window_function：函数，比如：sum、row_number、first_value partition by：窗口分区子句 order by：窗口排序子句 window_frame：窗口框架，限制窗口的边界大小 对照基本语法，有order by子句的执行结果就是计算窗口边界为起始行至当前行的sum结果，即sum(col2) over(partition by col1 order by col3)等同于sum(col2) over(partition by col1 order by col3 rows between unbounded preceding and current row) 窗口函数的官方说明mysql官方文档 hive官方文档 mysql关于窗口函数的window_frame有如下说明： hive关于窗口函数的window_frame有如下说明： 根据以上官方说明可知，当window_frame子句和order by子句都没有时，窗口计算默认包含窗口内的所有数据，即window_frame=ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING；当仅有order by子句，没有window_frame子句时，窗口计算默认仅包含排序后起始行至当前行的数据，即window_frame=ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"窗口函数","slug":"窗口函数","permalink":"https://llye-hub.github.io/tags/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"}]},{"title":"关于Java PriorityQueue类的使用场景","slug":"刷题/关于Java-PriorityQueue类的使用场景","date":"2023-02-23T08:53:35.000Z","updated":"2023-02-24T02:30:16.907Z","comments":true,"path":"posts/76a5661e.html","link":"","permalink":"https://llye-hub.github.io/posts/76a5661e.html","excerpt":"","text":"最近在leetcode刷题的时候，发现很多题推荐解法是用优先队列的特性，比如：滑动窗口的最大值 、丑数 以前完全没有用个这个类，所以在此整理一下对优先队列的认识和刷题场景 优先队列的特性很明显，优先队列也是一种队列，只不过其出队顺序和一般队列不同，优先队列的出队顺序是按照一定的优先级来的，也就是说出队规则可以随意定制 优先队列ADT是一种数据结构，它支持插入、删除最小值操作（返回并删除最小元素）、删除最大值操作（返回并删除最大元素） 优先队列的主要操作：优先队列是元素的容器，每个元素有一个相关的键值 insert(key, data)：插入键值为key的数据到优先队列中，元素以其key进行排序 deleteMin&#x2F;deleteMax：删除并返回最小&#x2F;最大键值的元素 getMinimum&#x2F;getMaximum：返回最小&#x2F;最大剑指的元素，但不删除它 优先队列的辅助操作： 第k最小&#x2F;第k最大：返回优先队列中键值为第k个最小&#x2F;最大的元素 大小（size）：返回优先队列中的元素个数 堆排序（Heap Sort）：基于键值的优先级将优先队列中的元素进行排序 在某些场景下，比如要求队列中的最小元素先出即可用优先队列，在java中的实现类为java.util.PriorityQueue。 认识下PriorityQueue类的方法创建对象12345678// 默认情况下，优先级队列的头是队列中最小的元素，元素将按升序从队列中移除PriorityQueue&lt;Integer&gt; nums = new PriorityQueue&lt;&gt;();// 借助 Comparator 接口自定义元素的顺序，头是队列中最大的元素，按降序从队列中移除PriorityQueue&lt;int[]&gt; win = new PriorityQueue&lt;int[]&gt;(new Comparator&lt;int[]&gt;() &#123; public int compare(int[] a, int[] b) &#123; return a[0] != b[0] ? b[0] - a[0] : b[1] - a[1]; &#125;&#125;); 插入元素：add、offer12345678910111213141516171819202122232425class Main &#123; public static void main(String[] args) &#123; //创建优先队列 PriorityQueue&lt;Integer&gt; numbers = new PriorityQueue&lt;&gt;(); //使用add()方法，如果队列已满，则会引发异常 numbers.add(4); numbers.add(2); System.out.println(&quot;PriorityQueue: &quot; + numbers); //使用offer()方法，如果队列已满，则返回false numbers.offer(1); System.out.println(&quot;更新后的PriorityQueue: &quot; + numbers); &#125;&#125;/* * 输出结果： * PriorityQueue: [2, 4] * 更新后的PriorityQueue: [1, 4, 2] * * 以上结果中，队列的头是最小元素 */ 访问元素：peek12345678//使用 peek() 方法int number = nums.peek();System.out.println(&quot;访问元素: &quot; + number);/* * 输出结果： * 访问元素: 1 */ 删除元素：remove、poll12345678910111213//使用remove()方法，从队列中删除指定的元素boolean result = numbers.remove(2);System.out.println(&quot;元素2是否已删除? &quot; + result);//使用poll()方法，返回并删除队列的头int number = numbers.poll();System.out.println(&quot;使用poll()删除的元素: &quot; + number);/* * 输出结果： * 元素2是否已删除? true * 使用poll()删除的元素: 1 */ 是否包含元素：contains12345678//使用contains()方法，从队列中搜索指定的元素，找到则返回true，否则false。boolean result = numbers.contains(4);System.out.println(&quot;队列中是否有元素 4 ？&quot; + result);/* * 输出结果： * 队列中是否有元素 4 ？ true */ 刷题场景滑动窗口的最大值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//给定一个数组 nums 和滑动窗口的大小 k，请找出所有滑动窗口里的最大值。 //示例: //输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3//输出: [3,3,5,5,6,7] //解释:// 滑动窗口的位置 最大值//--------------- -----//[1 3 -1] -3 5 3 6 7 3// 1 [3 -1 -3] 5 3 6 7 3// 1 3 [-1 -3 5] 3 6 7 5// 1 3 -1 [-3 5 3] 6 7 5// 1 3 -1 -3 [5 3 6] 7 6// 1 3 -1 -3 5 [3 6 7] 7import java.util.Comparator;import java.util.PriorityQueue;/* * 解题思路：利用优先队列的特性，规定堆顶元素就是窗口最大值 * */class Solution &#123; public int[] maxSlidingWindow(int[] nums, int k) &#123; int len = nums.length; PriorityQueue&lt;int[]&gt; win = new PriorityQueue&lt;int[]&gt;(new Comparator&lt;int[]&gt;() &#123; // 重新定义出队规则 @Override public int compare(int[] a, int[] b) &#123; return a[0] != b[0] ? b[0] - a[0] : b[1] - a[1]; &#125; &#125;); // 初始化窗口 for (int i = 0; i &lt; k; ++i) &#123; win.offer(new int[]&#123;nums[i], i&#125;); &#125; // 创建指定长度的结果数组 int[] res = new int[len-k+1]; // 第一个窗口的最大值 res[0] = win.peek()[0]; // 遍历滑动窗口 for (int i = k; i &lt; len; ++i) &#123; // 添加新元素 win.offer(new int[]&#123;nums[i], i&#125;); // 删除窗口长度外的元素 while (win.peek()[1] &lt;= i - k) &#123; win.poll(); &#125; // 返回当前窗口的最大值 res[i - k + 1] = win.peek()[0]; &#125; return res; &#125;&#125; 丑数12345678910111213141516171819202122232425262728293031323334353637383940// 我们把只包含质因子 2、3 和 5 的数称作丑数（Ugly Number）。求按从小到大的顺序的第 n 个丑数。// 示例: // 输入: n = 10// 输出: 12// 解释: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 是前 10 个丑数。 // 说明:// 1 是丑数。 // n 不超过1690。import java.util.HashSet;import java.util.PriorityQueue;import java.util.Set;/* * 解题思路：最小堆，需借助java的PriorityQueue类的特性实现：https://www.cainiaojc.com/java/java-priorityqueue.html * 初始化堆，将最小丑数1放入堆 * 每次取出堆顶元素x，x元素也是堆中最小的丑数，需排除重复元素，依次将 2x,3x,5x 加入堆 * 第n次取出的堆顶元素就是第n个丑数 * */class Solution &#123; public int nthUglyNumber(int n) &#123; int[] factors = &#123;2,3,5&#125;; PriorityQueue&lt;Long&gt; heap = new PriorityQueue&lt;Long&gt;(); //优先级队列的头是队列中最小的元素 heap.offer(1L); // 初始化最小堆，放入最小丑数1 int ugly = 0; for(int i=0; i&lt;n; i++)&#123; long cur = heap.poll(); //返回并删除队列的头，即最小元素 ugly = (int) cur; for (int factor : factors)&#123; long next = cur*factor; //检查是否有重复元素 if(!heap.contains(next))&#123; heap.offer(next); &#125; &#125; &#125; return ugly; &#125;&#125; 参考资料数据结构与算法(4)——优先队列和堆 Java PriorityQueue","categories":[{"name":"刷题","slug":"刷题","permalink":"https://llye-hub.github.io/categories/%E5%88%B7%E9%A2%98/"}],"tags":[{"name":"java","slug":"java","permalink":"https://llye-hub.github.io/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://llye-hub.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"hive、Spark和Maxcompute的SQL语法对比分析","slug":"SQL/hive、Spark和Maxcompute的SQL语法对比分析","date":"2023-02-21T02:33:16.000Z","updated":"2023-02-22T06:42:32.480Z","comments":true,"path":"posts/e6b1209.html","link":"","permalink":"https://llye-hub.github.io/posts/e6b1209.html","excerpt":"","text":"having 差异差异点hive和spark支持窗口函数后带havingmaxcomputer 的having语法只支持 在 group 和 distinct 后使用 举例1234select order_id,sum(trd_amt) over(partition by province) as trd_amt_stdfrom orderhaving trd_amt_std&gt;0 以上sql在hive中可以运行，但是在maxcomputer中会提示错误，错误如下： 替换方案在语句中使用子查询，将having替换为where 1234567select *from(select order_id,sum(trd_amt) over(partition by province) as trd_amt_stdfrom order) awhere a.trd_amt_std&gt;0 maxcomputer - cross join 超过一定条数后，依然会提示笛卡尔积风险差异点hive可以使用 cross join语法来表示笛卡尔积关联maxcomputer 的cross join，在条数超过一定数据量后，会提示笛卡尔积风险 举例1234567select a.*,b.*from(select * from table_a) across join(select * from table_b) b 以上sql在hive中可以运行，但是在maxcomputer中会提示错误，错误如下： 替换方案在左右笛卡尔积表中新增常量字段，用于关联 12345678select a.*,b.*from(select *,1 as cro_col from table_a) across join(select *,1 as cro_col from table_b) bon a.cro_col=b.cro_col 不等值join 差异差异点1、spark 支持不等值join语法2、hive 2.2.0版本之前不支持不等值语法，2.2.0及以后支持不等值join语法3、maxcomputer不支持不等值语法 举例测试sql 12345678910111213141516171819202122232425with table_a as (select 1 as id_a,&#x27;testa&#x27; as value_a union all select 4 as id_a ,&#x27;testd&#x27; as value_a),table_b as (select 3 as id_b,&#x27;testc&#x27; as value_b union all select 2 as id_b ,&#x27;testb&#x27; as value_b)select table_a.id_a,table_a.value_a,table_b.id_b,table_b.value_bfrom table_aleft join table_bon table_a.id_a &lt; table_b.id_b sql说明 :该sql准备了两张表table_a和table_b用于连接测试使用left join on语法，但是关联关系使用的是 &lt; 不等值关联符号 maxcomputer运行结果maxcomputer会报异常： FAILED: ODPS-0130071:[15,4] Semantic analysis exception - expect equality expression (i.e., only use ‘&#x3D;’ and ‘AND’) for join condition without mapjoin hint 提示的是期望join的是等值表达式 hive1.2.1运行结果 hive会报错： Error while compiling statement: FAILED: SemanticException [Error 10017]: line 15:3 Both left and right aliases encountered in JOIN ‘id_b’ 提示的是在join中遇到左右别名 不得不说，hive的错误信息有点云里雾里，其实就是不等值join造成的。 hive2.2.3运行结果 hive 2.2.0+版本顺利得到正确结果 spark运行结果 spark2.3也顺利得到结果 替换方案针对不等值join的替换方案有两种 1、针对小表，使用mapjoin，避免join操作 2、将on的不等值关联语句放入where语句中 由于mapjoin避免shuffle，性能较好，再可以的情况下，优先使用方案1 1、针对小表，使用mapjoin，避免join操作maxcomputer中的mapjoin hint语法为： &#x2F;*+ mapjoin() *&#x2F; ，详情请查看mapjoin hint 12345678910111213141516with table_a as (select 1 as id_a,&#x27;testa&#x27; as value_a),table_b as (select 2 as id_b,&#x27;testb&#x27; as value_b)select /*+ mapjoin(table_b) */table_a.id_a,table_a.value_a,table_b.id_b,table_b.value_bfrom table_aleft join table_bon table_a.id_a&lt;table_b.id_b 可以看到，使用mapjoin hint语法后，sql在maxcomputer中运行正确，顺利拿到了预期结果 2、将on的不等值关联语句放入where语句中inner join 比较简单 12345678910111213141516171819202122232425262728293031with table_a as (select 1 as id_a,&#x27;testa&#x27; as value_a,1 as join_col union all select 4 as id_a ,&#x27;testd&#x27; as value_a ,1 as join_col),table_b as (select 2 as id_b,&#x27;testb&#x27; as value_b,1 as join_col union all select 3 as id_b ,&#x27;testc&#x27; as value_b ,1 as join_col)selecttable_a.id_a,table_a.value_a,table_b.id_b,table_b.value_bfrom table_ainner join table_bon table_a.join_col=table_b.join_colwhere table_a.id_a&lt;table_b.id_b 可以看到，将&lt;判断语句放入where后，sql在maxcomputer运行正确，顺利拿到了预期结果 left join 比较复杂，建议使用map hint，实在没办法在使用此方案 123456789101112131415161718192021222324252627282930313233343536373839404142with table_a as (select 1 as id_a,&#x27;testa&#x27; as value_a,1 as join_col union all select 4 as id_a ,&#x27;testd&#x27; as value_a ,1 as join_col),table_b as (select 2 as id_b,&#x27;testb&#x27; as value_b,1 as join_col union all select 3 as id_b ,&#x27;testc&#x27; as value_b ,1 as join_col)-- 能关联上的部分,join_part as (selecttable_a.id_a,table_a.value_a,table_b.id_b,table_b.value_bfrom table_ainner join table_bon table_a.join_col=table_b.join_colwhere table_a.id_a&lt;table_b.id_b)-- 以自己为主表，left join能关联上的部分，实现 left join不等值效果select table_a.id_a,table_a.value_a,join_part.id_b,join_part.value_bfrom table_aleft join join_parton table_a.id_a=join_part.id_a 可以看到，将&lt;判断语句放入where后，sql在maxcomputer运行正确，顺利拿到了预期结果 array_contains 差异差异点spark的array_contains支持类型的隐式转换hive和maxcomputer array_contains不支持，只支持同类型使用 举例测试sql 1select array_contains(split(&quot;1,2,3,4&quot;,&quot;,&quot;),1) sql说明该sql首先使用split一个字符串获取一个array对象用于测试，之后使用array_contains函数进行判断split后的array对象为一个string数组，而判断被包含的数字【1】为一个int 对象 maxcomputer运行结果maxcomputer会报异常： FAILED: ODPS-0130071:[1,44] Semantic analysis exception - invalid type INT of argument 2 for function array_contains, expect STRING, implicit conversion is not allowed 提示的是array_contains第二个参数期望的是string，但是传入的是int，隐式类型转换不支持 hive运行结果 hive会报错： Error while compiling statement: FAILED: SemanticException [Error 10016]: line 1:43 Argument type mismatch ‘1’: “string” expected at function ARRAY_CONTAINS, but “int” is found 提示的是array_contains函数期望的是string，但是传入的是int，类型不匹配 spark运行结果 spark能顺利产出结果，结果为true，那么为什么spark可以成功呢？ 大概率是spark智能的将1从int转换为了string类型，使得类型得以匹配，通过explain查看物理执行计划来验证 在上图标红的地方可以看到，spark在物理执行计划层面，将int的1隐式的转换为了string类型，验证了我们一开始的猜想。 替换方案既然知道了在hive和maxcomputer中是类型不匹配导致的array_contains函数报错，那么只需要显示的将类型进行转换即可 1select array_contains(split(&quot;1,2,3,4&quot;,&quot;,&quot;),cast(1 as string)) 字段类型转换 ARRAY&lt;&gt; to STRING差异点spark的array_contains支持类型的隐式转换 hive和maxcomputer array_contains不支持，只支持同类型使用 举例测试sql 1select cast(array(1, 2, 3, 4) as string) as array_to_string; maxcompute运行结果 maxcompute报异常：FAILED: ODPS-0130141:[1,8] Illegal implicit type cast - cannot cast from ARRAY to STRING 提示的是 ARRAY&lt;&gt;类型字段 不能强制转换为 STRING 类型 hive运行结果 hive报异常：SQL语义错误: Error while compiling statement: FAILED: ClassCastException org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo cannot be cast to org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo 提示的是不同类型不能强转 spark运行结果 spark能顺利产出结果 替换方案使用 array_join函数 将array的元素拼接成字符串，再在首尾加上 ‘[ ‘ 和 ‘]’ 字符可以还原spark上的运行结果 1select concat(&#x27;[&#x27;,array_join(array(1, 2, 3, 4),&#x27;,&#x27;),&#x27;]&#x27;) as array_to_string; 📢注意macxcompute的array_join函数默认会忽略null元素，可在array_join函数中设置 nullreplacement 参数替代NULL元素 日期格式to_date(‘xxx’,’yyyyMMddHHmmss’)差异点hive语法中，to_date函数用法为：to_date(string timestamp)，返回DATE类型，格式为 yyyy-mm-dd ，仅有一个参数，支持用format格式解析 spark语法中，to_date函数用法为：to_date(date_str[, fmt]) ，返回DATE类型，格式为 yyyy-mm-dd ，支持用format格式解析日期 maxcompute语法中，to_date函数用法为：to_date(string , string )，返回DATETIME类型，格式为 yyyy-mm-dd hh:mi:ss ，支持用format格式解析日期 📢这里要注意的是，虽然spark和maxcompute中，to_date函数都支持用format格式解析日期，format格式是有差异的，主要表现在 分钟 位的格式 spark的format格式：yyyy为4位数的年，MM为2位数的月，dd为2位数的日，HH为24小时制的时，mm为2位数的分钟，ss为2位数的秒，ff3为3位精度毫秒maxcompute的format格式：yyyy为4位数的年，mm为2位数的月，dd为2位数的日，hh为24小时制的时，mi为2位数的分钟，ss为2位数的秒，ff3为3位精度毫秒 举例测试sql 1select to_date(&#x27;20221118123456&#x27;,&#x27;yyyyMMddHHmmss&#x27;),to_date(&#x27;2022-11-18 12:34:56&#x27;,&#x27;yyyy-MM-dd HH:mm:ss&#x27;); maxcompute运行结果 maxcompute报异常： FAILED: ODPS-0121095:Invalid arguments - format string has second part, but doesn’t have minute part : yyyyMMddHHmmss hive运行结果 hive报异常： Arguments length mismatch ‘’yyyyMMddhhmmss’’: to_date() requires 1 argument, got 2 提示的是to_date函数仅有1个参数 去掉format参数后的运行结果为： 从结果可以看到，to_date不能解析 yyyyMMddhhmmss 和 yyyyMMdd 格式 spark运行结果 spark能顺利产出结果 替换方案format格式修改：yyyy为4位数的年，mm为2位数的月，dd为2位数的日，hh为24小时制的时，mi为2位数的分钟，ss为2位数的秒，ff3为3位精度毫秒 修改后的能正常产出结果： 另，常见使用to_date报错sql为 date_format(date_add(to_date(pay_time,’yyyyMMddHHmmss’),2),’yyyyMMddHHmmss’) ，解读sql的作用是对 pay_time 加 2 天，建议用 UDF 修改这段sql为 yt_date_add(pay_time,2)，修改后简洁明了 date日期函数差异点spark和hive的date函数支持将标准的日期string转换为date类型 maxcomputer date函数只支持标准的日期string，带时分秒的时间string不支持 举例测试sql 1select date(&#x27;2022-12-21&#x27;),date(&#x27;2022-12-21 01:22:01&#x27;); maxcompute运行结果 maxcomputer对标准的日期string【2022-12-21】转换正确 但是对带时分秒的string转为错误，直接为null hive运行结果 结果符合预期 spark运行结果 spark能顺利产出结果 替换方案如果是为了格式转换，使用自定义 yt_date_format 函数 如果是为了获取date类型，使用 to_date函数 12select yt_date_format(&#x27;2022-12-21 01:22:01&#x27;,&#x27;yyyy-MM-dd&#x27;),to_date(&#x27;2022-12-21 01:22:01&#x27;); from_unixtime函数差异点spark和hive的from_unixtime函数将时间戳转换成格式化string类型，当时间戳为负数时，正常转换 maxcomputer from_unixtime函数转换负数时间戳时，存在时间便宜 举例测试sqlselect ‘1018-10-15 00:00:00’ – yyyyMMddHHmmss 时间戳,unix_timestamp(‘1018-10-15 00:00:00’) –时间戳,from_unixtime(unix_timestamp(‘1018-10-15 00:00:00’),’yyyyMMddHHmmss’) –转换格式 maxcompute运行结果 可以看到，原先日期为 ‘1018-10-15 00:00:00’,转换成yyyyMMddHHmmss格式原本期望为 10181015000000 但是实际结果为10181008235417,和预期不符合 hive运行结果 hive结果符合预期 spark运行结果 spark产出结果正确 替换方案使用自定义 yt_date_format 函数 123select &#x27;1018-10-15 00:00:00&#x27; -- yyyyMMddHHmmss 时间戳,unix_timestamp(&#x27;1018-10-15 00:00:00&#x27;) --时间戳,yt_date_format(&#x27;1018-10-15 00:00:00&#x27;,&#x27;yyyyMMddHHmmss&#x27;) --转换格式 使用自定义udf后正确 concat_ws差异差异点spark的concat_ws会支持类型的隐式转换 hive和maxcomputer concat_ws不支持，只支持同类型使用 举例测试sql 1select concat_ws(&quot;,&quot;,array(1,2,3)) maxcompute运行结果 报错提示数据类型不对，concat_ws只能处理ARRAY数据类型，而sql中是ARRAY数据类型，官方文档 中有详细说明 hive运行结果 报错提示数据类型不对，与maxcompute一个意思，concat_ws传入数组必须是Array类型 spark运行结果 spark执行结果符合预期 替换方案使用阿里云提供的array_join函数 1select array_join(array(1,2,3),&quot;,&quot;);","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"Maxcompute","slug":"Maxcompute","permalink":"https://llye-hub.github.io/tags/Maxcompute/"}]},{"title":"mysql和hiveSQL的语法差别","slug":"SQL/mysql和hiveSQL的语法差别","date":"2023-02-17T08:06:16.000Z","updated":"2023-02-21T06:25:32.460Z","comments":true,"path":"posts/e912f2da.html","link":"","permalink":"https://llye-hub.github.io/posts/e912f2da.html","excerpt":"","text":"最近在牛客网上刷sql题，但编程语言居然只支持mysql，一些函数用法上与平时工作使用的hiveSQL有较大差别，所以在这篇博客中整理一下两种语法的函数使用差异 mysql内置函数 hive内置函数 日期、时间函数 函数用途 mysql函数 mysql用法 hive函数 hiveSQL用法 日期、时间格式化 date_format date_format(‘2008-08-08 22:23:01’, ‘%Y%m%d%H%i%s’) date_format date_format(‘2008-08-08 22:23:01’, ‘yyyyMMddHHmmss’) 日期、时间加 date_add date_add(‘2008-08-08 22:23:01’,interval 1 day&#x2F;hour&#x2F;minute&#x2F;second&#x2F;microsecond&#x2F;week&#x2F;month&#x2F;quarter&#x2F;year)，返回dateTime格式 date_add date_add(‘2008-08-08 22:23:01’,1)，只加days，返回date格式 日期、时间减 date_sub date_sub(‘2008-08-08 22:23:01’,interval 1 day&#x2F;hour&#x2F;minute&#x2F;second&#x2F;microsecond&#x2F;week&#x2F;month&#x2F;quarter&#x2F;year)，返回dateTime格式 date_sub date_sub(‘2008-08-08 22:23:01’,1)，只加days，返回date格式 日期相差 datediff datediff(‘2008-08-08 22:22:00’,’2008-08-07 22:23:00’) datediff datediff(‘2008-08-08 22:22:00’,’2008-08-07 22:23:00’)","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[]},{"title":"排序算法","slug":"刷题/排序算法","date":"2023-02-17T05:46:19.000Z","updated":"2023-02-17T05:59:36.946Z","comments":true,"path":"posts/735e5788.html","link":"","permalink":"https://llye-hub.github.io/posts/735e5788.html","excerpt":"","text":"整理一些数据结构中常用的排序算法原理和java实现 快速排序原理在数组中找到一个基准值t，将小于t的值放它前面，大于t的值放它后面，再以此方法对子数组递归进行快速排序 java代码","categories":[{"name":"刷题","slug":"刷题","permalink":"https://llye-hub.github.io/categories/%E5%88%B7%E9%A2%98/"}],"tags":[]},{"title":"解题思路之动态规划","slug":"刷题/解题思路之动态规划","date":"2023-02-16T09:11:19.000Z","updated":"2023-02-20T09:07:20.705Z","comments":true,"path":"posts/d6cdfd6a.html","link":"","permalink":"https://llye-hub.github.io/posts/d6cdfd6a.html","excerpt":"","text":"什么是动态规划动态规划，英文：Dynamic Programming，简称DP。简单理解，动态规划的每一个状态都能由上一个状态推导而来 解题步骤以斐波那契数列为例，动态规划问题可以拆解为五步曲： 1、确定dp数组和下标含义：第n个斐波那契数是dp[n] 2、确定递推公式（也可叫状态转移方程）：dp[n] = dp[n-1] + dp[n-2] 3、dp数组初始化：dp[0] = 0; dp[1] = 1 4、确定遍历顺序：从前到后遍历，dp[n]依赖dp[n-1]和dp[n-2] 5、举例推导dp数组：当n=10时，dp数组应该为：0 1 1 2 3 5 8 13 21 34 55 参考资料代码随想录之动态规划","categories":[{"name":"刷题","slug":"刷题","permalink":"https://llye-hub.github.io/categories/%E5%88%B7%E9%A2%98/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://llye-hub.github.io/tags/LeetCode/"}]},{"title":"SparkSQL优化之数据倾斜","slug":"SQL/SparkSQL优化之数据倾斜","date":"2023-02-09T02:19:52.000Z","updated":"2023-02-28T08:29:17.852Z","comments":true,"path":"posts/faab1ad7.html","link":"","permalink":"https://llye-hub.github.io/posts/faab1ad7.html","excerpt":"","text":"前言在Spark作业优化场景中，最常见且比较棘手的就是数据倾斜问题。个人认为，具备数据倾斜调优能力对从事数仓开发人员是必备的基本要求。当然，数据倾斜的场景是比较复杂的，针对不同的数据倾斜有不同的处理方案。 如何辨别和定位数据倾斜从Spark作业的执行计划看，若出现某个task任务比其他task任务执行耗时极其久，比如：某个stage有100个task，其中99个task在1min左右就执行成功，但是有1个task却执行了1个小时甚至更久，这种情况显然是出现了数据倾斜。 数据倾斜问题仅出现在shuffle过程，一些会触发shuffle的算子：distinct、groupByKey、reduceByKey、aggregateByKey、countByKey、join、cogroup、repartition等。对应提交的SparkSQL中可能有distinct、count(distinct)、group by、partition by、join等关键词。 常见的数据倾斜场景及解决方案碰到的数据倾斜案例窗口分组数据倾斜倾斜场景业务上有一张消息记录表msg_records，sql要求是取下一次回复消息 12345678910111213141516171819202122232425262728WITH msg_tmp as( select id -- 唯一键，消息id ,from_chat_id -- 消息发送者id ,to_chat_id -- 消息接受者id ,msg_time -- 消息时间 from msg_records)select id ,msg_time ,first_value(if(type = &#x27;reply&#x27;,id,null),true) over(partition by from_chat_id,to_chat_id order by msg_time,id rows between 1 following and unbounded following) as reply_msg_id_n1t -- 取下一次回复消息from( select id ,from_chat_id ,to_chat_id ,msg_time ,&#x27;send&#x27; as type from msg_tmp union all -- 调转，取返回消息 select id ,to_chat_id as from_chat_id ,from_chat_id as to_chat_id ,msg_time ,&#x27;reply&#x27; as type from msg_tmp) t1 sql执行分析有一个task执行耗时1h 数据倾斜分析根据窗口函数的分组from_chat_id + to_chat_id分析，数据量出现严重倾斜，表总数据量1亿多，其中，分组from_chat_id=12 and to_chat_id=81867的数据量有30w，其他分组数据量至多3w。 另外，分组from_chat_id=12 and to_chat_id=81867的数据在业务上可定义为脏数据，且first_value()函数计算出的值全为null。 经过测试验证发现，没有 rows between语句 或是 过滤倾斜数据 时，SQL执行很快 综上分析，再对照spark执行计划基本可以定位倾斜原因为窗口数据倾斜和rows between计算耗时 解决方案结合业务知识，在sql逻辑中过滤from_chat_id=12 and to_chat_id=81867的数据 最终，任务执行耗时从1h优化至10min 参考资料美团技术团队：Spark性能优化指南——高级篇","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"数据倾斜","slug":"数据倾斜","permalink":"https://llye-hub.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"}]},{"title":"hiveSQL之生成连续数字","slug":"SQL/hiveSQL之生成连续数字","date":"2023-02-08T08:03:36.000Z","updated":"2023-02-20T03:39:51.495Z","comments":true,"path":"posts/3ce3d37f.html","link":"","permalink":"https://llye-hub.github.io/posts/3ce3d37f.html","excerpt":"","text":"sql要求生成100以内的全部整数 涉及udtf函数posexplode(ARRAY&lt;T&gt; a) 官方说明 Return: Returns a row-set with two columns (pos int,val T), one row for each element from the array. Description: posexplode() is similar to explode but instead of just returning the elements of the array it returns the element as well as its position in the original array. 用法示例：有如下一张表myTable (array&lt;int&gt;)myCol [100,200,300] [400,500,600] 执行hive sql 12345678-- 造数据with myTable as ( select array(100,200,300) as myCol union all select array(300,400,500) as myCol)-- 查询sqlSELECT posexplode(myCol) AS (pos, val) FROM myTable 得到结果为： (int)pos (int)val 0 100 1 200 2 300 0 400 1 500 2 600 sql实现借助posexplode返回的pos即可实现 12345select posexplode(split(space(99), &#x27; &#x27;)) as (pos, val)-- 返回的pos字段即为[0,99]区间的100个整数-- 或者下面这种写法select posexplode(split(repeat(&#x27;,&#x27;,99), &#x27;,&#x27;)) as (pos, val) 实例场景数据重复扩容10倍12345678910-- 造数据with myTable as ( select &#x27;张三&#x27; as name union all select &#x27;李四&#x27; as name)-- 将myTable的每行数据重复复制为5行SELECT name ,posexplode(split(space(4), &#x27; &#x27;)) AS (pos, val) FROM myTable 得到结果为： name pos val 张三 0 张三 1 张三 2 张三 3 张三 4 李四 0 李四 1 李四 2 李四 3 李四 4 生成指定范围内的连续日期123456789with subquery as ( select split(space(datediff(&#x27;2023-1-31&#x27;,&#x27;2022-11-30&#x27;)), &#x27; &#x27;) as x) select date_add(&#x27;2022-11-30&#x27;, pos) as new_datefrom subquery t lateral view posexplode(x) pe as pos, val","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"SQL高级语法","slug":"SQL高级语法","permalink":"https://llye-hub.github.io/tags/SQL%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"}]},{"title":"SparkSQL之conf参数","slug":"SQL/SparkSQL之conf参数","date":"2023-02-03T08:15:46.000Z","updated":"2023-02-20T05:44:02.740Z","comments":true,"path":"posts/5e220c44.html","link":"","permalink":"https://llye-hub.github.io/posts/5e220c44.html","excerpt":"","text":"资源参数num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1&#x2F;3~1&#x2F;2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 ##executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1&#x2F;3~1&#x2F;2左右比较合适，也是避免影响其他同学的作业运行。 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 广播相关spark.sql.broadcastTimeoutspark.kryoserializer.buffer.max&#x3D;128Mspark.sql.shuffle.partitions&#x3D;1000spark.sql.orc.compression.codec&#x3D;zlibspark.sql.files.maxPartitionBytes&#x3D;65536","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[]},{"title":"hiveSQL之set参数","slug":"SQL/hiveSQL之set参数","date":"2023-02-03T08:14:57.000Z","updated":"2023-02-20T07:52:21.554Z","comments":true,"path":"posts/4547a6e2.html","link":"","permalink":"https://llye-hub.github.io/posts/4547a6e2.html","excerpt":"","text":"hive.merge.mapfilesDefault Value: truemap-only任务结束时合并小文件 hive.merge.mapredfilesDefault Value: truemap-reduce任务结束时合并小文件 hive.optimize.cte.materialize.threshold默认情况下是-1（关闭）；当开启（大于0），比如设置为2，则如果with..as语句被引用2次及以上时，会把with..as语句生成的table物化，从而做到with..as语句只执行一次，来提高效率","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[]},{"title":"hiveSQL命令之alter partition","slug":"SQL/hiveSQL命令之alter-partition","date":"2023-02-02T06:02:26.000Z","updated":"2023-02-21T06:18:34.677Z","comments":true,"path":"posts/8a94c1da.html","link":"","permalink":"https://llye-hub.github.io/posts/8a94c1da.html","excerpt":"","text":"msck repair table https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"改分区","slug":"改分区","permalink":"https://llye-hub.github.io/tags/%E6%94%B9%E5%88%86%E5%8C%BA/"}]},{"title":"hadoop命令之distcp分布式拷贝","slug":"hadoop/hadoop命令之distcp分布式拷贝","date":"2023-02-01T02:06:03.000Z","updated":"2023-02-20T08:58:42.949Z","comments":true,"path":"posts/bcc5bdf2.html","link":"","permalink":"https://llye-hub.github.io/posts/bcc5bdf2.html","excerpt":"","text":"distcp用途DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。使用Map&#x2F;Reduce实现文件分发，错误处理和恢复，以及报告生成。DistCp将文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 distcp用法命令行中可以指定多个源目录 12# hadoop distcp source_dir1 [source_dir2 source_dir3……] target_dir 集群内拷贝 12# hadoop distcp [hdfs://nn:8020]/db/table_a/partition=1 [hdfs://nn:8020]/db/table_b/partition=1 不同集群间拷贝，DistCp必须运行在目标端集群上 12# hadoop distcp hdfs://nn1:8020/db/table_a/partition=1 hdfs://nn2:8020/db/table_b/partition=1 常用参数选项-overwrite源文件覆盖同名目标文件 -update拷贝目标目录下不存在而源目录下存在的文件，当文件大小不一致时，源文件覆盖同名目标文件 -delete删除目标目录下存在，但源目录下不存在的文件，需要配合-update或-overwrite使用 -p[rbugpcaxt]控制是否保留源文件的属性，-p默认全部保留，常用的为-pbugp。修改次数不会被保留。并且当指定 -update 时，更新的状态不会 被同步，除非文件大小不同（比如文件被重新创建）。 标识 含义 备注 r replication number 文件副本数 b block size 文件块大小 u user 用户 g group 组 p permission 文件权限 c checksum-type 校验和类型 a acl x xattr t timestamp 时间戳 -m控制拷贝时的map任务最大个数如果没使用-m选项，DistCp会尝试在调度工作时指定map数目&#x3D;min(total_bytes&#x2F;bytes.per.map,20*num_task_trackers)， 其中bytes.per.map默认是256MB。 应用实例表结构一致的两表互相拷贝数据1234567891011121314151617181920212223242526272829303132333435#********************************************************************************# ** 功能描述：通过hdfs文件路径拷贝的方式，实现表结构完全相同的表互相拷贝数据#********************************************************************************# 指定源路径、目标路径source_dir=/db/table_a/partition=1target_dir=/db/table_b/partition=1db_name=db_atarget_tbl_name=db_a.table_b# 判断源路径是否存在，不存在则返回hadoop fs -test -e $source_dirif [ $? -ne 0 ];then echo &quot;源路径$source_dir不存在&quot; exit 1fi# 判断目标路径是否存在，不存在则创建hadoop fs -test -e $target_dirif [ $? -ne 0 ];then hadoop fs -mkdir $target_dir echo &quot;目标路径$target_dir不存在，创建成功&quot;fi# 开始拷贝echo &quot;开始hdfs文件拷贝，source_dir=$source_dir，target_dir=$target_dir&quot;hadoop distcp -overwrite -delete -pbugp $source_dir $target_dirif [ $? -eq 0 ];then echo &quot;hdfs文件拷贝成功&quot;else echo &quot;hdfs文件拷贝失败&quot; exit -1fi# 刷新目标表的metastore信息hive -database $db_name -v -e &quot;msck repair table $target_tbl_name;&quot;if [ $? -eq 0 ];then echo &quot;$target_tbl_name表的metastore信息刷新成功&quot; exit 0fi 参考资料DistCp使用指南Hadoop中文网：DistCp","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"},{"name":"hdfs文件拷贝","slug":"hdfs文件拷贝","permalink":"https://llye-hub.github.io/tags/hdfs%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D/"}]},{"title":"Shell命令之set-e","slug":"shell/Shell命令之set-e","date":"2023-01-31T09:26:54.000Z","updated":"2023-02-20T08:35:55.810Z","comments":true,"path":"posts/ba81765c.html","link":"","permalink":"https://llye-hub.github.io/posts/ba81765c.html","excerpt":"","text":"","categories":[{"name":"shell","slug":"shell","permalink":"https://llye-hub.github.io/categories/shell/"}],"tags":[{"name":"shell命令","slug":"shell命令","permalink":"https://llye-hub.github.io/tags/shell%E5%91%BD%E4%BB%A4/"}]},{"title":"hadoop基本命令","slug":"hadoop/hadoop基本命令","date":"2023-01-31T09:10:11.000Z","updated":"2023-02-20T08:35:55.810Z","comments":true,"path":"posts/b24f0feb.html","link":"","permalink":"https://llye-hub.github.io/posts/b24f0feb.html","excerpt":"","text":"hadoop fs -cphadoop fs -rm -rhadoop distcp -overwrite -delete -phadoop fs -mkdir -p","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"}]},{"title":"hive动态分区","slug":"hive/hive动态分区","date":"2023-01-30T09:01:49.000Z","updated":"2023-02-20T08:35:55.811Z","comments":true,"path":"posts/44d3528f.html","link":"","permalink":"https://llye-hub.github.io/posts/44d3528f.html","excerpt":"","text":"","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"动态分区","slug":"动态分区","permalink":"https://llye-hub.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"}]},{"title":"SparkSQL无法处理hive表中的空ORC文件","slug":"spark/Spark SQL无法处理hive表中的空ORC文件","date":"2022-12-16T09:56:46.000Z","updated":"2023-02-20T03:40:11.835Z","comments":true,"path":"posts/1f69e18b.html","link":"","permalink":"https://llye-hub.github.io/posts/1f69e18b.html","excerpt":"","text":"碰到了什么问题起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客 问题分析定位问题根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段。 查看HDFS文件查看表存储路径下的文件，发现有1个空文件 为什么会有空文件1、sparkSQL建表2、表写入数据时，sql最后做了distribute by操作，产生了空文件 sparksql读取空文件的时候，因为表是orc格式的，导致sparkSQL解析orc文件出错。但是用hive却可以正常读取。 网上搜罗的解决办法问题原因基本清晰了，就是读取空文件导致的报错，如果非得用SparkSQL执行查询语句，这里提供几种解决方案： 1、修改表存储格式为parquet这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐 2、参数设置：set hive.exec.orc.split.strategy=ETL既然已经定位到是空文件读取的问题，那就从文件读取层面解决。 自建集群Spark源码： 12345678910111213141516171819202122232425262728// org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.javaswitch(context.splitStrategyKind) &#123; case BI: // BI strategy requested through config splitStrategy = new BISplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); break; case ETL: // ETL strategy requested through config splitStrategy = new ETLSplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); break; default: // HYBRID strategy if (avgFileSize &gt; context.maxSize) &#123; splitStrategy = new ETLSplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); &#125; else &#123; splitStrategy = new BISplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); &#125; break;&#125;// ./repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf.classHIVE_ORC_SPLIT_STRATEGY(&quot;hive.exec.orc.split.strategy&quot;, &quot;HYBRID&quot;, new StringSet(new String[]&#123;&quot;HYBRID&quot;, &quot;BI&quot;, &quot;ETL&quot;&#125;), &quot;This is not a user level config. BI strategy is used when the requirement is to spend less time in split generation as opposed to query execution (split generation does not read or cache file footers). ETL strategy is used when spending little more time in split generation is acceptable (split generation reads and caches file footers). HYBRID chooses between the above strategies based on heuristics.&quot;) 也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。 BI策略以文件为粒度进行split划分 ETL策略会将文件进行切分，多个stripe组成一个split HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。 ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。 123456789101112131415161718192021222324252627282930313233// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplitspublic List&lt;OrcSplit&gt; getSplits() throws IOException &#123; List&lt;OrcSplit&gt; splits = Lists.newArrayList(); for (FileStatus fileStatus : fileStatuses) &#123; String[] hosts = SHIMS .getLocationsWithOffset(fs, fileStatus) // 对空文件会返回一个空的TreeMap .firstEntry() // null .getValue() // NPE .getHosts(); OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), 0, fileStatus.getLen(), hosts, null, isOriginal, true, deltas, -1); splits.add(orcSplit); &#125; // add uncovered ACID delta splits splits.addAll(super.getSplits()); return splits;&#125;// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplitspublic List&lt;SplitInfo&gt; getSplits() throws IOException &#123; List&lt;SplitInfo&gt; result = Lists.newArrayList(); for (FileStatus file : files) &#123; FileInfo info = null; if (context.cacheStripeDetails) &#123; info = verifyCachedFileInfo(file); &#125; // ignore files of 0 length（此处对空文件做了过滤） if (file.getLen() &gt; 0) &#123; result.add(new SplitInfo(context, fs, file, info, isOriginal, deltas, true, dir, covered)); &#125; &#125; return result;&#125; 本质上是一个BUG，Spark2.4版本中解决了这个问题。 123456789101112131415// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplitspublic List&lt;OrcSplit&gt; getSplits() throws IOException &#123; List&lt;OrcSplit&gt; splits = Lists.newArrayList(); for (FileStatus fileStatus : fileStatuses) &#123; String[] hosts = SHIMS.getLocationsWithOffset(fs, fileStatus).firstEntry().getValue() .getHosts(); OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), 0, fileStatus.getLen(), hosts, null, isOriginal, true, deltas, -1); splits.add(orcSplit); &#125; // add uncovered ACID delta splits splits.addAll(super.getSplits()); return splits;&#125; 了解了spark读取orc文件策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取 1set hive.exec.orc.split.strategy=ETL 经测试无效。原因分析：1、参数未生效2、hdfs文件有两个，大小为49B和7.45G，文件的平均大小肯定是大于256M的，所以按默认HYBRID策略规则应本就是采取的ETL策略split ORC文件 3、参数设置：spark.sql.hive.convertMetastoreOrc=true关于参数的官方介绍 Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true. 经测试有效。若仍报错，可尝试搭配spark.sql.orc.impl&#x3D;native使用。 补充知识hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。 参考资料SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTIONSparkSQL读取ORC表时遇到空文件","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"}]},{"title":"get_json_object在sql中的高级用法","slug":"SQL/get_json_object在sql中的高级用法","date":"2022-12-16T03:46:24.000Z","updated":"2023-02-20T03:39:56.457Z","comments":true,"path":"posts/5f45fcd7.html","link":"","permalink":"https://llye-hub.github.io/posts/5f45fcd7.html","excerpt":"","text":"语法介绍12get_json_object(String json_string, String path)-- return string get_json_object函数是用来根据指定路径提取json字符串中的json对象，并返回json对象的json字符串 现有困惑关于这个函数最常见的用法就是get_json_object(&#39;&#123;&quot;a&quot;:&quot;b&quot;&#125;&#39;, &#39;$.a&#39;)，返回结果b但$.a这种path写法仅适用于简单的多层嵌套json字符串解析，碰到嵌套层有json数组时就难以解析了比如，要提取下面这段json中的所有weight对象的值 123456789&#123; &quot;store&quot;: &#123; &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;], //json数组 &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125; &#125;, &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, &quot;owner&quot;:&quot;amy&quot; &#125; 通过$.store.fruit.weight路径是无法提取的，$.store.fruit[0].weight这种写法仅能获取json数组中第一个json字符串中weight对象的值，也总不能用[0]、[1]、[2]……的方式无穷尽取值吧 到这里思维就限制住了，遇到这种情况时，以前的方式是通过正则表达式处理具体实现如下：首先将item_properties按指定分隔符split为array数组，再利用explode函数将array数组的元素逐行输出，最终得到的item_propertie即为单个json字符串，可根据$.提取指定json对象的值， 12345678910-- item_properties = [&#123;&quot;id&quot;:42,&quot;name&quot;:&quot;包装&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;-- ,&#123;&quot;id&quot;:43,&quot;name&quot;:&quot;种类&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;-- ,&#123;&quot;id&quot;:44,&quot;name&quot;:&quot;规格&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:63,&quot;name&quot;:&quot;保质期(天)&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:100,&quot;name&quot;:&quot;适用年龄&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:101,&quot;name&quot;:&quot;储存条件&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;]select get_json_object(item_propertie,&#x27;$.id&#x27;)from table_alateral view explode(split(regexp_replace(substr(item_properties,2,length(item_properties)-2),&#x27;\\\\&#125;\\\\,\\\\&#123;&#x27;,&#x27;\\\\&#125;\\\\|\\\\|\\\\&#123;&#x27;),&#x27;\\\\|\\\\|&#x27;)) tmp as item_propertie 但上面这种处理方式存在bug，将json数据split为array数组时，必须保证指定分隔符不出现在单个json字符串中，比如上述case中是用&#125;,&#123;替换为&#125;||&#123;，再以||作为分隔符split，如若在单个json字符串中也出现了&#125;,&#123;或是||就会导致解析失败 怎么高级了突然有一天在翻看hive官方文档时发现path支持的通配符* 1234$ : 表示根节点. : 表示子节点[] : [number]表示数组下标，从0开始* : []的通配符，返回整个数组 所以，一开始的问题应该按如下解法： 123456789-- jsonArray = &#123;&quot;store&quot;:-- &#123;-- &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],-- &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;-- &#125;, -- &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, -- &quot;owner&quot;:&quot;amy&quot;&#125;select get_json_object(jsonArray, &#x27;$.store.fruit[*].weight&#x27;);-- return [8,9] 笔者个人认为，高级之处在于写法极其清爽，按照以前用正则表达式的处理方法，需要多道处理才能得到结果[8,9]，而且其中还有隐性风险，但是现在$.store.fruit[*].weight这种极简语法既避免了风险，又清晰易理解","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"SQL高级语法","slug":"SQL高级语法","permalink":"https://llye-hub.github.io/tags/SQL%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"}]},{"title":"资料汇总","slug":"资料汇总","date":"2022-12-16T03:46:24.000Z","updated":"2023-02-22T02:49:03.975Z","comments":true,"path":"posts/76d5a95a.html","link":"","permalink":"https://llye-hub.github.io/posts/76d5a95a.html","excerpt":"","text":"SparkSQLSpark SQL Limit 介绍及优化 Spark性能优化指南——基础篇 Spark性能优化指南——高级篇 大数据笔记大数据入门指南 数据结构与算法代码随想录 数据治理存储和计算资源都节省 30%，网易云音乐数据治理实践 数据库数据库内核杂谈系列 乱七八糟的2万字揭秘阿里巴巴数据治理平台建设经验","categories":[{"name":"资料","slug":"资料","permalink":"https://llye-hub.github.io/categories/%E8%B5%84%E6%96%99/"}],"tags":[]}],"categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"},{"name":"刷题","slug":"刷题","permalink":"https://llye-hub.github.io/categories/%E5%88%B7%E9%A2%98/"},{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"},{"name":"shell","slug":"shell","permalink":"https://llye-hub.github.io/categories/shell/"},{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"},{"name":"资料","slug":"资料","permalink":"https://llye-hub.github.io/categories/%E8%B5%84%E6%96%99/"}],"tags":[{"name":"窗口函数","slug":"窗口函数","permalink":"https://llye-hub.github.io/tags/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"},{"name":"java","slug":"java","permalink":"https://llye-hub.github.io/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://llye-hub.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Maxcompute","slug":"Maxcompute","permalink":"https://llye-hub.github.io/tags/Maxcompute/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://llye-hub.github.io/tags/LeetCode/"},{"name":"数据倾斜","slug":"数据倾斜","permalink":"https://llye-hub.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"},{"name":"SQL高级语法","slug":"SQL高级语法","permalink":"https://llye-hub.github.io/tags/SQL%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"},{"name":"改分区","slug":"改分区","permalink":"https://llye-hub.github.io/tags/%E6%94%B9%E5%88%86%E5%8C%BA/"},{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"},{"name":"hdfs文件拷贝","slug":"hdfs文件拷贝","permalink":"https://llye-hub.github.io/tags/hdfs%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D/"},{"name":"shell命令","slug":"shell命令","permalink":"https://llye-hub.github.io/tags/shell%E5%91%BD%E4%BB%A4/"},{"name":"动态分区","slug":"动态分区","permalink":"https://llye-hub.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"},{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"}]}