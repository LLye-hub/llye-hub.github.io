{"meta":{"title":"LLye","subtitle":"","description":"scsadad","author":"LLye","url":"https://llye-hub.github.io","root":"/"},"pages":[{"title":"Repositories","date":"2023-01-16T07:15:53.818Z","updated":"2023-01-16T07:15:53.818Z","comments":false,"path":"index.html","permalink":"https://llye-hub.github.io/index.html","excerpt":"","text":""},{"title":"About Me","date":"2023-01-17T08:41:32.555Z","updated":"2023-01-17T08:41:32.555Z","comments":false,"path":"about/index.html","permalink":"https://llye-hub.github.io/about/index.html","excerpt":"","text":"工作经历"},{"title":"友情链接","date":"2023-01-16T07:30:43.399Z","updated":"2023-01-16T07:30:43.399Z","comments":true,"path":"links/index.html","permalink":"https://llye-hub.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-01-16T07:29:50.652Z","updated":"2023-01-16T07:29:50.652Z","comments":false,"path":"tags/index.html","permalink":"https://llye-hub.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-01-16T07:28:48.940Z","updated":"2023-01-16T07:28:48.940Z","comments":false,"path":"categories/index.html","permalink":"https://llye-hub.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"SparkSQL优化之数据倾斜","slug":"SQL/SparkSQL优化之数据倾斜","date":"2023-02-09T02:19:52.000Z","updated":"2023-02-15T06:12:42.997Z","comments":true,"path":"posts/faab1ad7.html","link":"","permalink":"https://llye-hub.github.io/posts/faab1ad7.html","excerpt":"","text":"前言在Spark作业优化场景中，最常见且比较棘手的就是数据倾斜问题。个人认为，具备数据倾斜调优能力对从事数仓开发人员是必备的基本要求。当然，数据倾斜的场景是比较复杂的，针对不同的数据倾斜有不同的处理方案。 如何辨别和定位数据倾斜从Spark作业的执行计划看，若出现某个task任务比其他task任务执行耗时极其久，比如：某个stage有100个task，其中99个task在1min左右就执行成功，但是有1个task却执行了1个小时甚至更久，这种情况显然是出现了数据倾斜。 数据倾斜问题仅出现在shuffle过程，一些会触发shuffle的算子：distinct、groupByKey、reduceByKey、aggregateByKey、countByKey、join、cogroup、repartition等。对应提交的SparkSQL中可能有distinct、count(distinct)、group by、partition by、join等关键词。 常见的数据倾斜场景及解决方案碰到的数据倾斜案例窗口分组数据倾斜倾斜场景业务上有一张消息记录表msg_records，sql要求是取下一次回复消息 12345678910111213141516171819202122232425262728WITH msg_tmp as( select id -- 唯一键，消息id ,from_chat_id -- 消息发送者id ,to_chat_id -- 消息接受者id ,msg_time -- 消息时间 from msg_records)select id ,msg_time ,first_value(if(type = &#x27;reply&#x27;,id,null),true) over(partition by from_chat_id,to_chat_id order by msg_time,id rows between 1 following and unbounded following) as reply_msg_id_n1t -- 取下一次回复消息from( select id ,from_chat_id ,to_chat_id ,msg_time ,&#x27;send&#x27; as type from msg_tmp union all -- 调转，取返回消息 select id ,to_chat_id as from_chat_id ,from_chat_id as to_chat_id ,msg_time ,&#x27;reply&#x27; as type from msg_tmp) t1 sql执行分析有一个task执行耗时1h 数据倾斜分析根据窗口函数的分组from_chat_id + to_chat_id分析，数据量出现严重倾斜，表总数据量1亿多，其中，分组from_chat_id=12 and to_chat_id=81867的数据量有30w，其他分组数据量至多3w。 另外，分组from_chat_id=12 and to_chat_id=81867的数据在业务上可定义为脏数据，且first_value()函数计算出的值全为null。 经过测试验证发现，没有 rows between语句 或是 过滤倾斜数据 时，SQL执行很快 综上分析，再对照spark执行计划基本可以定位倾斜原因为窗口数据倾斜和rows between计算耗时 解决方案结合业务知识，在sql逻辑中过滤from_chat_id=12 and to_chat_id=81867的数据 最终，任务执行耗时从1h优化至10min 参考资料美团技术团队：Spark性能优化指南——高级篇","categories":[{"name":"SQL优化","slug":"SQL优化","permalink":"https://llye-hub.github.io/categories/SQL%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"数据倾斜","slug":"数据倾斜","permalink":"https://llye-hub.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"}]},{"title":"hiveSQL之生成连续数字","slug":"SQL/hiveSQL之生成连续数字","date":"2023-02-08T08:03:36.000Z","updated":"2023-02-08T08:55:24.868Z","comments":true,"path":"posts/3ce3d37f.html","link":"","permalink":"https://llye-hub.github.io/posts/3ce3d37f.html","excerpt":"","text":"sql要求生成100以内的全部整数 涉及udtf函数posexplode(ARRAY&lt;T&gt; a) 官方说明 Return: Returns a row-set with two columns (pos int,val T), one row for each element from the array. Description: posexplode() is similar to explode but instead of just returning the elements of the array it returns the element as well as its position in the original array. 用法示例：有如下一张表myTable (array&lt;int&gt;)myCol [100,200,300] [400,500,600] 执行hive sql 12345678-- 造数据with myTable as ( select array(100,200,300) as myCol union all select array(300,400,500) as myCol)-- 查询sqlSELECT posexplode(myCol) AS (pos, val) FROM myTable 得到结果为： (int)pos (int)val 0 100 1 200 2 300 0 400 1 500 2 600 sql实现借助posexplode返回的pos即可实现 12345select posexplode(split(space(99), &#x27; &#x27;)) as (pos, val)-- 返回的pos字段即为[0,99]区间的100个整数-- 或者下面这种写法select posexplode(split(repeat(&#x27;,&#x27;,99), &#x27;,&#x27;)) as (pos, val) 实例场景数据重复扩容10倍12345678910-- 造数据with myTable as ( select &#x27;张三&#x27; as name union all select &#x27;李四&#x27; as name)-- 将myTable的每行数据重复复制为5行SELECT name ,posexplode(split(space(4), &#x27; &#x27;)) AS (pos, val) FROM myTable 得到结果为： name pos val 张三 0 张三 1 张三 2 张三 3 张三 4 李四 0 李四 1 李四 2 李四 3 李四 4 生成指定范围内的连续日期123456789with subquery as ( select split(space(datediff(&#x27;2023-1-31&#x27;,&#x27;2022-11-30&#x27;)), &#x27; &#x27;) as x) select date_add(&#x27;2022-11-30&#x27;, pos) as new_datefrom subquery t lateral view posexplode(x) pe as pos, val","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"SQL高级语法","slug":"SQL高级语法","permalink":"https://llye-hub.github.io/tags/SQL%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"}]},{"title":"SparkSQL的set参数","slug":"SparkSQL的set参数","date":"2023-02-03T08:15:46.000Z","updated":"2023-02-08T06:46:22.006Z","comments":true,"path":"posts/5e220c44.html","link":"","permalink":"https://llye-hub.github.io/posts/5e220c44.html","excerpt":"","text":"spark.kryoserializer.buffer.max&#x3D;128Mspark.sql.shuffle.partitions&#x3D;1000spark.sql.orc.compression.codec&#x3D;zlibspark.sql.files.maxPartitionBytes&#x3D;65536","categories":[{"name":"Spark","slug":"Spark","permalink":"https://llye-hub.github.io/categories/Spark/"}],"tags":[{"name":"运行参数","slug":"运行参数","permalink":"https://llye-hub.github.io/tags/%E8%BF%90%E8%A1%8C%E5%8F%82%E6%95%B0/"}]},{"title":"hive的set参数","slug":"hive的set参数","date":"2023-02-03T08:14:57.000Z","updated":"2023-02-08T06:45:58.458Z","comments":true,"path":"posts/4547a6e2.html","link":"","permalink":"https://llye-hub.github.io/posts/4547a6e2.html","excerpt":"","text":"hive.merge.mapfilesDefault Value: truemap-only任务结束时合并小文件 hive.merge.mapredfilesDefault Value: truemap-reduce任务结束时合并小文件","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"运行参数","slug":"运行参数","permalink":"https://llye-hub.github.io/tags/%E8%BF%90%E8%A1%8C%E5%8F%82%E6%95%B0/"}]},{"title":"hiveSQL命令之alter partition","slug":"hiveSQL命令之alter-partition","date":"2023-02-02T06:02:26.000Z","updated":"2023-02-03T08:20:50.383Z","comments":true,"path":"posts/8a94c1da.html","link":"","permalink":"https://llye-hub.github.io/posts/8a94c1da.html","excerpt":"","text":"msck repair table https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExchangePartition","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"sql alter","slug":"sql-alter","permalink":"https://llye-hub.github.io/tags/sql-alter/"}]},{"title":"hadoop命令之distcp分布式拷贝","slug":"hadoop/hadoop命令之distcp分布式拷贝","date":"2023-02-01T02:06:03.000Z","updated":"2023-02-15T07:15:10.341Z","comments":true,"path":"posts/bcc5bdf2.html","link":"","permalink":"https://llye-hub.github.io/posts/bcc5bdf2.html","excerpt":"","text":"前言hadoop文件拷贝命令：hadoop fs -cp [src_dir] [dst_dir] 上面这种方式不适合多文件拷贝。如果src_dir是一个目录，目录下有200个文件，由于这种文件拷贝方式是单线程的，200个文件逐个拷贝到指定路径下会十分耗时 distcp用途DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。使用Map&#x2F;Reduce实现文件分发，错误处理和恢复，以及报告生成。DistCp将文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 distcp用法命令行中可以指定多个源目录 12# hadoop distcp source_dir1 [source_dir2 source_dir3……] target_dir 集群内拷贝 12# hadoop distcp [hdfs://nn:8020]/db/table_a/partition=1 [hdfs://nn:8020]/db/table_b/partition=1 不同集群间拷贝，DistCp必须运行在目标端集群上 12# hadoop distcp hdfs://nn1:8020/db/table_a/partition=1 hdfs://nn2:8020/db/table_b/partition=1 常用参数选项-overwrite源文件覆盖同名目标文件 -update拷贝目标目录下不存在而源目录下存在的文件，当文件大小不一致时，源文件覆盖同名目标文件 -delete删除目标目录下存在，但源目录下不存在的文件，需要配合-update或-overwrite使用 -p[rbugpcaxt]控制是否保留源文件的属性，-p默认全部保留，常用的为-pbugp。修改次数不会被保留。并且当指定 -update 时，更新的状态不会 被同步，除非文件大小不同（比如文件被重新创建）。 标识 含义 备注 r replication number 文件副本数 b block size 文件块大小 u user 用户 g group 组 p permission 文件权限 c checksum-type 校验和类型 a acl x xattr t timestamp 时间戳 -m控制拷贝时的map任务最大个数如果没使用-m选项，DistCp会尝试在调度工作时指定map数目&#x3D;min(total_bytes&#x2F;bytes.per.map,20*num_task_trackers)， 其中bytes.per.map默认是256MB。 应用实例表结构一致的两表互相拷贝数据1234567891011121314151617181920212223242526272829303132333435#********************************************************************************# ** 功能描述：通过hdfs文件路径拷贝的方式，实现表结构完全相同的表互相拷贝数据#********************************************************************************# 指定源路径、目标路径source_dir=/db/table_a/partition=1target_dir=/db/table_b/partition=1db_name=db_atarget_tbl_name=db_a.table_b# 判断源路径是否存在，不存在则返回hadoop fs -test -e $source_dirif [ $? -ne 0 ];then echo &quot;源路径$source_dir不存在&quot; exit 1fi# 判断目标路径是否存在，不存在则创建hadoop fs -test -e $target_dirif [ $? -ne 0 ];then hadoop fs -mkdir $target_dir echo &quot;目标路径$target_dir不存在，创建成功&quot;fi# 开始拷贝echo &quot;开始hdfs文件拷贝，source_dir=$source_dir，target_dir=$target_dir&quot;hadoop distcp -overwrite -delete -pbugp $source_dir $target_dirif [ $? -eq 0 ];then echo &quot;hdfs文件拷贝成功&quot;else echo &quot;hdfs文件拷贝失败&quot; exit -1fi# 刷新目标表的metastore信息hive -database $db_name -v -e &quot;msck repair table $target_tbl_name;&quot;if [ $? -eq 0 ];then echo &quot;$target_tbl_name表的metastore信息刷新成功&quot; exit 0fi 参考资料DistCp使用指南Hadoop中文网：DistCp","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"}],"tags":[{"name":"hdfs文件拷贝","slug":"hdfs文件拷贝","permalink":"https://llye-hub.github.io/tags/hdfs%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D/"}]},{"title":"Shell命令之set-e","slug":"Shell命令之set-e","date":"2023-01-31T09:26:54.000Z","updated":"2023-02-02T10:03:10.347Z","comments":true,"path":"posts/ba81765c.html","link":"","permalink":"https://llye-hub.github.io/posts/ba81765c.html","excerpt":"","text":"","categories":[{"name":"shell","slug":"shell","permalink":"https://llye-hub.github.io/categories/shell/"}],"tags":[{"name":"shell命令","slug":"shell命令","permalink":"https://llye-hub.github.io/tags/shell%E5%91%BD%E4%BB%A4/"}]},{"title":"hadoop基本命令","slug":"hadoop基本命令","date":"2023-01-31T09:10:11.000Z","updated":"2023-02-03T08:21:33.709Z","comments":true,"path":"posts/b24f0feb.html","link":"","permalink":"https://llye-hub.github.io/posts/b24f0feb.html","excerpt":"","text":"hadoop fs -cphadoop fs -rm -rhadoop distcp -overwrite -delete -phadoop fs -mkdir -p","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"}]},{"title":"hive动态分区","slug":"hive动态分区","date":"2023-01-30T09:01:49.000Z","updated":"2023-02-02T09:57:13.974Z","comments":true,"path":"posts/44d3528f.html","link":"","permalink":"https://llye-hub.github.io/posts/44d3528f.html","excerpt":"","text":"","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"动态分区","slug":"动态分区","permalink":"https://llye-hub.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"}]},{"title":"SparkSQL无法处理hive表中的空ORC文件","slug":"spark/Spark SQL无法处理hive表中的空ORC文件","date":"2022-12-16T09:56:46.000Z","updated":"2023-02-15T07:15:10.320Z","comments":true,"path":"posts/1f69e18b.html","link":"","permalink":"https://llye-hub.github.io/posts/1f69e18b.html","excerpt":"","text":"碰到了什么问题起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客 问题分析定位问题根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段。 查看HDFS文件查看表存储路径下的文件，发现有1个空文件 为什么会有空文件1、sparkSQL建表2、表写入数据时，sql最后做了distribute by操作，产生了空文件 sparksql读取空文件的时候，因为表是orc格式的，导致sparkSQL解析orc文件出错。但是用hive却可以正常读取。 网上搜罗的解决办法问题原因基本清晰了，就是读取空文件导致的报错，如果非得用SparkSQL执行查询语句，这里提供几种解决方案： 1、修改表存储格式为parquet这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐 2、参数设置：set hive.exec.orc.split.strategy=ETL既然已经定位到是空文件读取的问题，那就从文件读取层面解决。 自建集群Spark源码： 12345678910111213141516171819202122232425262728// org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.javaswitch(context.splitStrategyKind) &#123; case BI: // BI strategy requested through config splitStrategy = new BISplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); break; case ETL: // ETL strategy requested through config splitStrategy = new ETLSplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); break; default: // HYBRID strategy if (avgFileSize &gt; context.maxSize) &#123; splitStrategy = new ETLSplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); &#125; else &#123; splitStrategy = new BISplitStrategy(context, fs, dir, children, isOriginal, deltas, covered); &#125; break;&#125;// ./repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf.classHIVE_ORC_SPLIT_STRATEGY(&quot;hive.exec.orc.split.strategy&quot;, &quot;HYBRID&quot;, new StringSet(new String[]&#123;&quot;HYBRID&quot;, &quot;BI&quot;, &quot;ETL&quot;&#125;), &quot;This is not a user level config. BI strategy is used when the requirement is to spend less time in split generation as opposed to query execution (split generation does not read or cache file footers). ETL strategy is used when spending little more time in split generation is acceptable (split generation reads and caches file footers). HYBRID chooses between the above strategies based on heuristics.&quot;) 也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。 BI策略以文件为粒度进行split划分 ETL策略会将文件进行切分，多个stripe组成一个split HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。 ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。 123456789101112131415161718192021222324252627282930313233// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplitspublic List&lt;OrcSplit&gt; getSplits() throws IOException &#123; List&lt;OrcSplit&gt; splits = Lists.newArrayList(); for (FileStatus fileStatus : fileStatuses) &#123; String[] hosts = SHIMS .getLocationsWithOffset(fs, fileStatus) // 对空文件会返回一个空的TreeMap .firstEntry() // null .getValue() // NPE .getHosts(); OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), 0, fileStatus.getLen(), hosts, null, isOriginal, true, deltas, -1); splits.add(orcSplit); &#125; // add uncovered ACID delta splits splits.addAll(super.getSplits()); return splits;&#125;// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplitspublic List&lt;SplitInfo&gt; getSplits() throws IOException &#123; List&lt;SplitInfo&gt; result = Lists.newArrayList(); for (FileStatus file : files) &#123; FileInfo info = null; if (context.cacheStripeDetails) &#123; info = verifyCachedFileInfo(file); &#125; // ignore files of 0 length（此处对空文件做了过滤） if (file.getLen() &gt; 0) &#123; result.add(new SplitInfo(context, fs, file, info, isOriginal, deltas, true, dir, covered)); &#125; &#125; return result;&#125; 本质上是一个BUG，Spark2.4版本中解决了这个问题。 123456789101112131415// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplitspublic List&lt;OrcSplit&gt; getSplits() throws IOException &#123; List&lt;OrcSplit&gt; splits = Lists.newArrayList(); for (FileStatus fileStatus : fileStatuses) &#123; String[] hosts = SHIMS.getLocationsWithOffset(fs, fileStatus).firstEntry().getValue() .getHosts(); OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), 0, fileStatus.getLen(), hosts, null, isOriginal, true, deltas, -1); splits.add(orcSplit); &#125; // add uncovered ACID delta splits splits.addAll(super.getSplits()); return splits;&#125; 了解了spark读取orc文件策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取 1set hive.exec.orc.split.strategy=ETL 经测试无效。原因分析：1、参数未生效2、hdfs文件有两个，大小为49B和7.45G，文件的平均大小肯定是大于256M的，所以按默认HYBRID策略规则应本就是采取的ETL策略split ORC文件 3、参数设置：spark.sql.hive.convertMetastoreOrc=true关于参数的官方介绍 Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true. 经测试有效。若仍报错，可尝试搭配spark.sql.orc.impl&#x3D;native使用。 补充知识hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。 参考资料SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTIONSparkSQL读取ORC表时遇到空文件","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"}]},{"title":"大数据相关资料","slug":"大数据相关资料","date":"2022-12-16T03:46:24.000Z","updated":"2023-02-15T03:05:16.640Z","comments":true,"path":"posts/76d5a95a.html","link":"","permalink":"https://llye-hub.github.io/posts/76d5a95a.html","excerpt":"","text":"SparkSQLSpark SQL Limit 介绍及优化 大数据笔记大数据入门指南","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"get_json_object在sql中的高级用法","slug":"SQL/get_json_object在sql中的高级用法","date":"2022-12-16T03:46:24.000Z","updated":"2023-02-08T06:27:13.267Z","comments":true,"path":"posts/5f45fcd7.html","link":"","permalink":"https://llye-hub.github.io/posts/5f45fcd7.html","excerpt":"","text":"语法介绍12get_json_object(String json_string, String path)-- return string get_json_object函数是用来根据指定路径提取json字符串中的json对象，并返回json对象的json字符串 现有困惑关于这个函数最常见的用法就是get_json_object(&#39;&#123;&quot;a&quot;:&quot;b&quot;&#125;&#39;, &#39;$.a&#39;)，返回结果b但$.a这种path写法仅适用于简单的多层嵌套json字符串解析，碰到嵌套层有json数组时就难以解析了比如，要提取下面这段json中的所有weight对象的值 123456789&#123; &quot;store&quot;: &#123; &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;], //json数组 &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125; &#125;, &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, &quot;owner&quot;:&quot;amy&quot; &#125; 通过$.store.fruit.weight路径是无法提取的，$.store.fruit[0].weight这种写法仅能获取json数组中第一个json字符串中weight对象的值，也总不能用[0]、[1]、[2]……的方式无穷尽取值吧 到这里思维就限制住了，遇到这种情况时，以前的方式是通过正则表达式处理具体实现如下：首先将item_properties按指定分隔符split为array数组，再利用explode函数将array数组的元素逐行输出，最终得到的item_propertie即为单个json字符串，可根据$.提取指定json对象的值， 12345678910-- item_properties = [&#123;&quot;id&quot;:42,&quot;name&quot;:&quot;包装&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;-- ,&#123;&quot;id&quot;:43,&quot;name&quot;:&quot;种类&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;-- ,&#123;&quot;id&quot;:44,&quot;name&quot;:&quot;规格&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:63,&quot;name&quot;:&quot;保质期(天)&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:100,&quot;name&quot;:&quot;适用年龄&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:101,&quot;name&quot;:&quot;储存条件&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;]select get_json_object(item_propertie,&#x27;$.id&#x27;)from table_alateral view explode(split(regexp_replace(substr(item_properties,2,length(item_properties)-2),&#x27;\\\\&#125;\\\\,\\\\&#123;&#x27;,&#x27;\\\\&#125;\\\\|\\\\|\\\\&#123;&#x27;),&#x27;\\\\|\\\\|&#x27;)) tmp as item_propertie 但上面这种处理方式存在bug，将json数据split为array数组时，必须保证指定分隔符不出现在单个json字符串中，比如上述case中是用&#125;,&#123;替换为&#125;||&#123;，再以||作为分隔符split，如若在单个json字符串中也出现了&#125;,&#123;或是||就会导致解析失败 怎么高级了突然有一天在翻看hive官方文档时发现path支持的通配符* 1234$ : 表示根节点. : 表示子节点[] : [number]表示数组下标，从0开始* : []的通配符，返回整个数组 所以，一开始的问题应该按如下解法： 123456789-- jsonArray = &#123;&quot;store&quot;:-- &#123;-- &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],-- &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;-- &#125;, -- &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, -- &quot;owner&quot;:&quot;amy&quot;&#125;select get_json_object(jsonArray, &#x27;$.store.fruit[*].weight&#x27;);-- return [8,9] 笔者个人认为，高级之处在于写法极其清爽，按照以前用正则表达式的处理方法，需要多道处理才能得到结果[8,9]，而且其中还有隐性风险，但是现在$.store.fruit[*].weight这种极简语法既避免了风险，又清晰易理解","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"SQL高级语法","slug":"SQL高级语法","permalink":"https://llye-hub.github.io/tags/SQL%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"}]}],"categories":[{"name":"SQL优化","slug":"SQL优化","permalink":"https://llye-hub.github.io/categories/SQL%E4%BC%98%E5%8C%96/"},{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"},{"name":"Spark","slug":"Spark","permalink":"https://llye-hub.github.io/categories/Spark/"},{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"},{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"},{"name":"shell","slug":"shell","permalink":"https://llye-hub.github.io/categories/shell/"}],"tags":[{"name":"数据倾斜","slug":"数据倾斜","permalink":"https://llye-hub.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"},{"name":"SQL高级语法","slug":"SQL高级语法","permalink":"https://llye-hub.github.io/tags/SQL%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"},{"name":"运行参数","slug":"运行参数","permalink":"https://llye-hub.github.io/tags/%E8%BF%90%E8%A1%8C%E5%8F%82%E6%95%B0/"},{"name":"sql alter","slug":"sql-alter","permalink":"https://llye-hub.github.io/tags/sql-alter/"},{"name":"hdfs文件拷贝","slug":"hdfs文件拷贝","permalink":"https://llye-hub.github.io/tags/hdfs%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D/"},{"name":"shell命令","slug":"shell命令","permalink":"https://llye-hub.github.io/tags/shell%E5%91%BD%E4%BB%A4/"},{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"},{"name":"动态分区","slug":"动态分区","permalink":"https://llye-hub.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"},{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"},{"name":"大数据","slug":"大数据","permalink":"https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}