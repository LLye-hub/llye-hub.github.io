{"meta":{"title":"LLye","subtitle":"","description":"","author":"yuye","url":"https://llye-hub.github.io","root":"/"},"pages":[],"posts":[{"title":"SparkSQL无法处理hive表中的空ORC文件","slug":"Spark SQL无法处理hive表中的空ORC文件","date":"2022-12-16T09:56:46.000Z","updated":"2022-12-21T08:35:26.765Z","comments":true,"path":"posts/1f69e18b.html","link":"","permalink":"https://llye-hub.github.io/posts/1f69e18b.html","excerpt":"","text":"参考资料SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTIONSparkSQL读取ORC表时遇到空文件 为什么碰到这个问题起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客 问题分析定位问题根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段 查看HDFS文件查看表存储路径下的文件，发现有1个空文件 为什么会有空文件空文件是根据map个数产生的小文件，启动select 查询必然启动MR 那就避免不了Map阶段的产生 解决办法问题原因基本清晰了，就是读取空文件导致的报错，但是计算过程中无法避免空文件产生，如果非得用SparkSQL执行查询语句，这里提供几种解决方案： 1、修改表存储格式为parquet这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐 2、参数设置：set hive.exec.orc.split.strategy=ETL既然已经定位到是空文件读取的问题，那就从文件读取层面解决。 关于参数的官方介绍： hive.exec.orc.split.strategyDefault Value: HYBRIDAdded In: Hive 1.2.0 with HIVE-10114 What strategy ORC should use to create splits for execution. The available options are “BI”, “ETL” and “HYBRID”.The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS blocksize. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS. 相关源码Spark 2.12： 123456789101112switch(context.splitStrategyKind) &#123;case BI: return new OrcInputFormat.BISplitStrategy(context, fs, dir, baseFiles, isOriginal, deltas, covered, allowSyntheticFileIds);case ETL: return combineOrCreateETLStrategy(combinedCtx, context, fs, dir, baseFiles, deltas, covered, readerTypes, isOriginal, ugi, allowSyntheticFileIds);default: if (avgFileSize &lt;= context.maxSize &amp;&amp; totalFiles &gt; context.etlFileThreshold) &#123; return new OrcInputFormat.BISplitStrategy(context, fs, dir, baseFiles, isOriginal, deltas, covered, allowSyntheticFileIds); &#125; else &#123; return combineOrCreateETLStrategy(combinedCtx, context, fs, dir, baseFiles, deltas, covered, readerTypes, isOriginal, ugi, allowSyntheticFileIds); &#125;&#125; 也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。 BI策略以文件为粒度进行split划分 ETL策略会将文件进行切分，多个stripe组成一个split HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。 ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplitspublic List&lt;OrcSplit&gt; getSplits() throws IOException &#123; List&lt;OrcSplit&gt; splits = Lists.newArrayList(); for (FileStatus fileStatus : fileStatuses) &#123; String[] hosts = SHIMS .getLocationsWithOffset(fs, fileStatus) // 对空文件会返回一个空的TreeMap .firstEntry() // null .getValue() // NPE .getHosts(); OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), 0, fileStatus.getLen(), hosts, null, isOriginal, true, deltas, -1); splits.add(orcSplit); &#125; // add uncovered ACID delta splits splits.addAll(super.getSplits()); return splits;&#125;// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplitspublic List&lt;SplitInfo&gt; getSplits() throws IOException &#123; List&lt;SplitInfo&gt; result = new ArrayList&lt;&gt;(files.size()); …… for (int i = 0; i &lt; files.size(); ++i) &#123; …… // Ignore files eliminated by PPD, or of 0 length.（此处对空文件做了过滤） if (ppdResult != FooterCache.NO_SPLIT_AFTER_PPD &amp;&amp; file.getFileStatus().getLen() &gt; 0) &#123; result.add(new SplitInfo(context, dir.fs, file, orcTail, readerTypes, isOriginal, deltas, true, dir.dir, covered, ppdResult)); &#125; &#125; &#125; else &#123; int dirIx = -1, fileInDirIx = -1, filesInDirCount = 0; ETLDir dir = null; for (HdfsFileStatusWithId file : files) &#123; …… // ignore files of 0 length（此处对空文件做了过滤） if (file.getFileStatus().getLen() &gt; 0) &#123; result.add(new Sp litInfo(context, dir.fs, file, null, readerTypes, isOriginal, deltas, true, dir.dir, covered, null)); &#125; &#125; &#125; return result;&#125; 本质上是一个Hive的BUG，Spark2.4版本中解决了这个问题。知道读取数据的策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取 1set hive.exec.orc.split.strategy=ETL 3、参数设置：spark.sql.hive.convertMetastoreOrc=true关于参数的官方介绍 Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true. 最后的最后，以上3种解决方案仅供参考，为笔者对问题剖析之后，再结合网上资料整理的，尚未经过实际检验。笔者对这个问题的解决方法就是不用SparkSQL查就好，hiveSQL不会有此问题。 补充知识hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。","categories":[],"tags":[{"name":"HDFS","slug":"HDFS","permalink":"https://llye-hub.github.io/tags/HDFS/"},{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"}]},{"title":"get_json_object在sql中的高级用法","slug":"getJsonObject","date":"2022-12-16T03:46:24.000Z","updated":"2022-12-20T06:59:07.095Z","comments":true,"path":"posts/5f45fcd7.html","link":"","permalink":"https://llye-hub.github.io/posts/5f45fcd7.html","excerpt":"","text":"","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/tags/SQL/"}]},{"title":"大数据相关资料","slug":"大数据相关资料","date":"2022-12-16T03:46:24.000Z","updated":"2022-12-20T06:59:07.096Z","comments":true,"path":"posts/76d5a95a.html","link":"","permalink":"https://llye-hub.github.io/posts/76d5a95a.html","excerpt":"","text":"","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}],"categories":[],"tags":[{"name":"HDFS","slug":"HDFS","permalink":"https://llye-hub.github.io/tags/HDFS/"},{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"},{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/tags/SQL/"},{"name":"大数据","slug":"大数据","permalink":"https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}