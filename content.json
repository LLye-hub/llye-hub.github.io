{"meta":{"title":"LLye","subtitle":"","description":"scsadad","author":"LLye","url":"https://llye-hub.github.io","root":"/"},"pages":[{"title":"Repositories","date":"2023-01-16T07:15:53.818Z","updated":"2023-01-16T07:15:53.818Z","comments":false,"path":"index.html","permalink":"https://llye-hub.github.io/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-01-16T07:30:43.399Z","updated":"2023-01-16T07:30:43.399Z","comments":true,"path":"links/index.html","permalink":"https://llye-hub.github.io/links/index.html","excerpt":"","text":""},{"title":"About Me","date":"2023-01-17T08:41:32.555Z","updated":"2023-01-17T08:41:32.555Z","comments":false,"path":"about/index.html","permalink":"https://llye-hub.github.io/about/index.html","excerpt":"","text":"工作经历"},{"title":"分类","date":"2023-01-16T07:28:48.940Z","updated":"2023-01-16T07:28:48.940Z","comments":false,"path":"categories/index.html","permalink":"https://llye-hub.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-01-16T07:29:50.652Z","updated":"2023-01-16T07:29:50.652Z","comments":false,"path":"tags/index.html","permalink":"https://llye-hub.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"hive命令之alter partition","slug":"hive命令之alter-partition","date":"2023-02-02T06:02:26.000Z","updated":"2023-02-02T09:57:36.806Z","comments":true,"path":"posts/8a94c1da.html","link":"","permalink":"https://llye-hub.github.io/posts/8a94c1da.html","excerpt":"","text":"","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"hive命令","slug":"hive命令","permalink":"https://llye-hub.github.io/tags/hive%E5%91%BD%E4%BB%A4/"}]},{"title":"hive命令之msck repair table","slug":"hive命令之msck-repair-table","date":"2023-02-02T05:58:48.000Z","updated":"2023-02-02T09:57:31.557Z","comments":true,"path":"posts/940bc38d.html","link":"","permalink":"https://llye-hub.github.io/posts/940bc38d.html","excerpt":"","text":"","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"hive命令","slug":"hive命令","permalink":"https://llye-hub.github.io/tags/hive%E5%91%BD%E4%BB%A4/"}]},{"title":"hadoop命令之distcp分布式拷贝","slug":"hadoop命令之distcp分布式拷贝","date":"2023-02-01T02:06:03.000Z","updated":"2023-02-02T09:56:23.050Z","comments":true,"path":"posts/bcc5bdf2.html","link":"","permalink":"https://llye-hub.github.io/posts/bcc5bdf2.html","excerpt":"","text":"distcp用途DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。使用Map&#x2F;Reduce实现文件分发，错误处理和恢复，以及报告生成。DistCp将文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 distcp用法命令行中可以指定多个源目录 12# hadoop distcp source_dir1 [source_dir2 source_dir3……] target_dir 集群内拷贝 12# hadoop distcp [hdfs://nn:8020]/db/table_a/partition=1 [hdfs://nn:8020]/db/table_b/partition=1 不同集群间拷贝，DistCp必须运行在目标端集群上 12# hadoop distcp hdfs://nn1:8020/db/table_a/partition=1 hdfs://nn2:8020/db/table_b/partition=1 常用参数选项-overwrite源文件覆盖同名目标文件 -update拷贝目标目录下不存在而源目录下存在的文件，当文件大小不一致时，源文件覆盖同名目标文件 -delete删除目标目录下存在，但源目录下不存在的文件，需要配合-update或-overwrite使用 -p[rbugpcaxt]控制是否保留源文件的属性，-p默认全部保留，常用的为-pbugp。修改次数不会被保留。并且当指定 -update 时，更新的状态不会 被同步，除非文件大小不同（比如文件被重新创建）。 标识 含义 备注 r replication number 文件副本数 b block size 文件块大小 u user 用户 g group 组 p permission 文件权限 c checksum-type 校验和类型 a acl x xattr t timestamp 时间戳 -m控制拷贝时的map任务最大个数如果没使用-m选项，DistCp会尝试在调度工作时指定map数目&#x3D;min(total_bytes&#x2F;bytes.per.map,20*num_task_trackers)， 其中bytes.per.map默认是256MB。 应用实例表结构一致的两表互相拷贝数据1234567891011121314151617181920212223242526272829303132333435#********************************************************************************# ** 功能描述：通过hdfs文件路径拷贝的方式，实现表结构完全相同的表互相拷贝数据#********************************************************************************# 指定源路径、目标路径source_dir=/db/table_a/partition=1target_dir=/db/table_b/partition=1db_name=db_atarget_tbl_name=db_a.table_b# 判断源路径是否存在，不存在则返回hadoop fs -test -e $source_dirif [ $? -ne 0 ];then echo &quot;源路径$source_dir不存在&quot; exit 1fi# 判断目标路径是否存在，不存在则创建hadoop fs -test -e $target_dirif [ $? -ne 0 ];then hadoop fs -mkdir $target_dir echo &quot;目标路径$target_dir不存在，创建成功&quot;fi# 开始拷贝echo &quot;开始hdfs文件拷贝，source_dir=$source_dir，target_dir=$target_dir&quot;hadoop distcp -overwrite -delete -pbugp $source_dir $target_dirif [ $? -eq 0 ];then echo &quot;hdfs文件拷贝成功&quot;else echo &quot;hdfs文件拷贝失败&quot; exit -1fi# 刷新目标表的metastore信息hive -database $db_name -v -e &quot;msck repair table $target_tbl_name;&quot;if [ $? -eq 0 ];then echo &quot;$target_tbl_name表的metastore信息刷新成功&quot; exit 0fi 参考资料DistCp使用指南Hadoop中文网：DistCp","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"},{"name":"hdfs文件拷贝","slug":"hdfs文件拷贝","permalink":"https://llye-hub.github.io/tags/hdfs%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D/"}]},{"title":"Shell命令之set-e","slug":"Shell命令之set-e","date":"2023-01-31T09:26:54.000Z","updated":"2023-02-02T09:30:32.876Z","comments":true,"path":"posts/ba81765c.html","link":"","permalink":"https://llye-hub.github.io/posts/ba81765c.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"hadoop基本","slug":"hadoop基本命令","date":"2023-01-31T09:10:11.000Z","updated":"2023-02-02T09:56:35.161Z","comments":true,"path":"posts/b24f0feb.html","link":"","permalink":"https://llye-hub.github.io/posts/b24f0feb.html","excerpt":"","text":"hadoop fs -cphadoop fs -rm -rhadoop distcp -overwrite -delete -phadoop fs -mkdir -p","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"}]},{"title":"hive动态分区","slug":"hive动态分区","date":"2023-01-30T09:01:49.000Z","updated":"2023-02-02T09:57:13.974Z","comments":true,"path":"posts/44d3528f.html","link":"","permalink":"https://llye-hub.github.io/posts/44d3528f.html","excerpt":"","text":"","categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"}],"tags":[{"name":"动态分区","slug":"动态分区","permalink":"https://llye-hub.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"}]},{"title":"SparkSQL无法处理hive表中的空ORC文件","slug":"Spark SQL无法处理hive表中的空ORC文件","date":"2022-12-16T09:56:46.000Z","updated":"2023-01-30T09:31:20.259Z","comments":true,"path":"posts/1f69e18b.html","link":"","permalink":"https://llye-hub.github.io/posts/1f69e18b.html","excerpt":"","text":"为什么碰到这个问题起因是在使用SparkSQL查询表时，遇到报错：java.lang.RuntimeException: serious problem at OrcInputFormat.generateSplitsInfo之后，换了hiveSQL执行成功，但这并不算排查成功，排查应尽可能追根究底，以后才能做到举一反三，所以基于网上资料和个人理解写了这篇博客 问题分析定位问题根据报错的java类名+方法名（OrcInputFormat.generateSplitsInfo），可以判断问题出现在读取orc文件阶段 查看HDFS文件查看表存储路径下的文件，发现有1个空文件 为什么会有空文件空文件是根据map个数产生的小文件，启动select 查询必然启动MR 那就避免不了Map阶段的产生 解决办法问题原因基本清晰了，就是读取空文件导致的报错，但是计算过程中无法避免空文件产生，如果非得用SparkSQL执行查询语句，这里提供几种解决方案： 修改表存储格式为parquet这种方法是网上查询到的，但在实际数仓工作中，对于已在使用中的表来说，删表重建操作是不允许的，所以不推荐 参数设置：set hive.exec.orc.split.strategy=ETL既然已经定位到是空文件读取的问题，那就从文件读取层面解决。 关于参数的官方介绍： hive.exec.orc.split.strategyDefault Value: HYBRIDAdded In: Hive 1.2.0 with HIVE-10114 What strategy ORC should use to create splits for execution. The available options are “BI”, “ETL” and “HYBRID”.The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS blocksize. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS. 相关源码Spark 2.12： 123456789101112switch(context.splitStrategyKind) &#123;case BI: return new OrcInputFormat.BISplitStrategy(context, fs, dir, baseFiles, isOriginal, deltas, covered, allowSyntheticFileIds);case ETL: return combineOrCreateETLStrategy(combinedCtx, context, fs, dir, baseFiles, deltas, covered, readerTypes, isOriginal, ugi, allowSyntheticFileIds);default: if (avgFileSize &lt;= context.maxSize &amp;&amp; totalFiles &gt; context.etlFileThreshold) &#123; return new OrcInputFormat.BISplitStrategy(context, fs, dir, baseFiles, isOriginal, deltas, covered, allowSyntheticFileIds); &#125; else &#123; return combineOrCreateETLStrategy(combinedCtx, context, fs, dir, baseFiles, deltas, covered, readerTypes, isOriginal, ugi, allowSyntheticFileIds); &#125;&#125; 也就是说，默认是HYBRID（混合模式读取，根据平均文件大小和文件个数选择ETL还是BI模式）。 BI策略以文件为粒度进行split划分 ETL策略会将文件进行切分，多个stripe组成一个split HYBRID策略为：当文件的平均大小大于hadoop最大split值（默认256 * 1024 * 1024）时使用ETL策略，否则使用BI策略。 ETLSplitStrategy和BISplitStrategy两种策略在对getSplits方法采用了不同的实现方式，BISplitStrategy在面对空文件时会出现空指针异常，ETLSplitStrategy则帮我们过滤了空文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.BISplitStrategy#getSplitspublic List&lt;OrcSplit&gt; getSplits() throws IOException &#123; List&lt;OrcSplit&gt; splits = Lists.newArrayList(); for (FileStatus fileStatus : fileStatuses) &#123; String[] hosts = SHIMS .getLocationsWithOffset(fs, fileStatus) // 对空文件会返回一个空的TreeMap .firstEntry() // null .getValue() // NPE .getHosts(); OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), 0, fileStatus.getLen(), hosts, null, isOriginal, true, deltas, -1); splits.add(orcSplit); &#125; // add uncovered ACID delta splits splits.addAll(super.getSplits()); return splits;&#125;// org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.ETLSplitStrategy#getSplitspublic List&lt;SplitInfo&gt; getSplits() throws IOException &#123; List&lt;SplitInfo&gt; result = new ArrayList&lt;&gt;(files.size()); …… for (int i = 0; i &lt; files.size(); ++i) &#123; …… // Ignore files eliminated by PPD, or of 0 length.（此处对空文件做了过滤） if (ppdResult != FooterCache.NO_SPLIT_AFTER_PPD &amp;&amp; file.getFileStatus().getLen() &gt; 0) &#123; result.add(new SplitInfo(context, dir.fs, file, orcTail, readerTypes, isOriginal, deltas, true, dir.dir, covered, ppdResult)); &#125; &#125; &#125; else &#123; int dirIx = -1, fileInDirIx = -1, filesInDirCount = 0; ETLDir dir = null; for (HdfsFileStatusWithId file : files) &#123; …… // ignore files of 0 length（此处对空文件做了过滤） if (file.getFileStatus().getLen() &gt; 0) &#123; result.add(new Sp litInfo(context, dir.fs, file, null, readerTypes, isOriginal, deltas, true, dir.dir, covered, null)); &#125; &#125; &#125; return result;&#125; 本质上是一个Hive的BUG，Spark2.4版本中解决了这个问题。知道读取数据的策略，那么就设置避免混合模式使用根据文件大小分割读取，不根据文件来读取 1set hive.exec.orc.split.strategy=ETL 参数设置：spark.sql.hive.convertMetastoreOrc=true关于参数的官方介绍 Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true. 最后的最后，以上3种解决方案仅供参考，为笔者对问题剖析之后，再结合网上资料整理的，尚未经过实际检验。笔者对这个问题的解决方法就是不用SparkSQL查就好，hiveSQL不会有此问题。 补充知识hive.exec.orc.split.strategy参数控制在读取ORC表时生成split的策略。对于一些较大的ORC表，可能其footer较大，ETL策略可能会导致其从hdfs拉取大量的数据来切分split，甚至会导致driver端OOM，因此这类表的读取建议使用BI策略。对于一些较小的尤其有数据倾斜的表（这里的数据倾斜指大量stripe存储于少数文件中），建议使用ETL策略。另外，spark.hadoop.mapreduce.input.fileinputformat.split.minsize参数可以控制在ORC切分时stripe的合并处理。具体逻辑是，当几个stripe的大小小于spark.hadoop.mapreduce.input.fileinputformat.split.minsize时，会合并到一个task中处理。可以适当调小该值，以此增大读ORC表的并发。 参考资料SPARK查ORC格式HIVE数据报错NULLPOINTEREXCEPTIONSparkSQL读取ORC表时遇到空文件","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"HDFS","slug":"HDFS","permalink":"https://llye-hub.github.io/tags/HDFS/"},{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"}]},{"title":"get_json_object在sql中的高级用法","slug":"get_json_object在sql中的高级用法","date":"2022-12-16T03:46:24.000Z","updated":"2023-01-30T09:48:21.057Z","comments":true,"path":"posts/5f45fcd7.html","link":"","permalink":"https://llye-hub.github.io/posts/5f45fcd7.html","excerpt":"","text":"语法介绍12get_json_object(String json_string, String path)-- return string get_json_object函数是用来根据指定路径提取json字符串中的json对象，并返回json对象的json字符串 现有困惑关于这个函数最常见的用法就是get_json_object(&#39;&#123;&quot;a&quot;:&quot;b&quot;&#125;&#39;, &#39;$.a&#39;)，返回结果b但$.a这种path写法仅适用于简单的多层嵌套json字符串解析，碰到嵌套层有json数组时就难以解析了比如，要提取下面这段json中的所有weight对象的值 123456789&#123; &quot;store&quot;: &#123; &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;], //json数组 &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125; &#125;, &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, &quot;owner&quot;:&quot;amy&quot; &#125; 通过$.store.fruit.weight路径是无法提取的，$.store.fruit[0].weight这种写法仅能获取json数组中第一个json字符串中weight对象的值，也总不能用[0]、[1]、[2]……的方式无穷尽取值吧 到这里思维就限制住了，遇到这种情况时，以前的方式是通过正则表达式处理具体实现如下：首先将item_properties按指定分隔符split为array数组，再利用explode函数将array数组的元素逐行输出，最终得到的item_propertie即为单个json字符串，可根据$.提取指定json对象的值， 12345678910-- item_properties = [&#123;&quot;id&quot;:42,&quot;name&quot;:&quot;包装&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;-- ,&#123;&quot;id&quot;:43,&quot;name&quot;:&quot;种类&quot;,&quot;sort&quot;:0,&quot;type&quot;:1&#125;-- ,&#123;&quot;id&quot;:44,&quot;name&quot;:&quot;规格&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:63,&quot;name&quot;:&quot;保质期(天)&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:100,&quot;name&quot;:&quot;适用年龄&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;-- ,&#123;&quot;id&quot;:101,&quot;name&quot;:&quot;储存条件&quot;,&quot;sort&quot;:0,&quot;type&quot;:2&#125;]select get_json_object(item_propertie,&#x27;$.id&#x27;)from table_alateral view explode(split(regexp_replace(substr(item_properties,2,length(item_properties)-2),&#x27;\\\\&#125;\\\\,\\\\&#123;&#x27;,&#x27;\\\\&#125;\\\\|\\\\|\\\\&#123;&#x27;),&#x27;\\\\|\\\\|&#x27;)) tmp as item_propertie 但上面这种处理方式存在bug，将json数据split为array数组时，必须保证指定分隔符不出现在单个json字符串中，比如上述case中是用&#125;,&#123;替换为&#125;||&#123;，再以||作为分隔符split，如若在单个json字符串中也出现了&#125;,&#123;或是||就会导致解析失败 怎么高级了突然有一天在翻看hive官方文档时发现path支持的通配符* 1234$ : 表示根节点. : 表示子节点[] : [number]表示数组下标，从0开始* : []的通配符，返回整个数组 所以，一开始的问题应该按如下解法： 123456789-- jsonArray = &#123;&quot;store&quot;:-- &#123;-- &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],-- &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;-- &#125;, -- &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, -- &quot;owner&quot;:&quot;amy&quot;&#125;select get_json_object(jsonArray, &#x27;$.store.fruit[*].weight&#x27;);-- return [8,9] 笔者个人认为，高级之处在于写法极其清爽，按照以前用正则表达式的处理方法，需要多道处理才能得到结果[8,9]，而且其中还有隐性风险，但是现在$.store.fruit[*].weight这种极简语法既避免了风险，又清晰易理解","categories":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/tags/SQL/"},{"name":"高级语法","slug":"高级语法","permalink":"https://llye-hub.github.io/tags/%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"}]},{"title":"大数据相关资料","slug":"大数据相关资料","date":"2022-12-16T03:46:24.000Z","updated":"2022-12-20T06:59:07.096Z","comments":true,"path":"posts/76d5a95a.html","link":"","permalink":"https://llye-hub.github.io/posts/76d5a95a.html","excerpt":"","text":"","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}],"categories":[{"name":"hive","slug":"hive","permalink":"https://llye-hub.github.io/categories/hive/"},{"name":"hadoop","slug":"hadoop","permalink":"https://llye-hub.github.io/categories/hadoop/"},{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/categories/SQL/"}],"tags":[{"name":"hive命令","slug":"hive命令","permalink":"https://llye-hub.github.io/tags/hive%E5%91%BD%E4%BB%A4/"},{"name":"hadoop命令","slug":"hadoop命令","permalink":"https://llye-hub.github.io/tags/hadoop%E5%91%BD%E4%BB%A4/"},{"name":"hdfs文件拷贝","slug":"hdfs文件拷贝","permalink":"https://llye-hub.github.io/tags/hdfs%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D/"},{"name":"动态分区","slug":"动态分区","permalink":"https://llye-hub.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"},{"name":"HDFS","slug":"HDFS","permalink":"https://llye-hub.github.io/tags/HDFS/"},{"name":"ORC","slug":"ORC","permalink":"https://llye-hub.github.io/tags/ORC/"},{"name":"SQL","slug":"SQL","permalink":"https://llye-hub.github.io/tags/SQL/"},{"name":"高级语法","slug":"高级语法","permalink":"https://llye-hub.github.io/tags/%E9%AB%98%E7%BA%A7%E8%AF%AD%E6%B3%95/"},{"name":"大数据","slug":"大数据","permalink":"https://llye-hub.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}